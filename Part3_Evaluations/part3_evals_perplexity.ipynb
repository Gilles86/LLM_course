{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNgjZqjSCc8/dIHLn9/q57D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 3:</h2>|<h1>Evaluating LLMs<h1>|\n","|<h2>Section:</h2>|<h1>Quantitative evaluations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Perplexity<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"dOEJuhcut4Ia"}},{"cell_type":"code","source":["# run this code, then restart the python session (and then comment it out)\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"J-2H2j41uXlK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6kR6FqjfpeL"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","from datasets import load_dataset\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"CcCcEIMek9X5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import LLM and wikitext data"],"metadata":{"id":"9m1ROaVOubvo"}},{"cell_type":"code","source":["# load SMALL pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# no training! we can use eval mode\n","gpt2.eval()"],"metadata":{"id":"PsrX9u8K3AEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n","tokens = tokenizer.encode('\\n\\n'.join(text['text']), return_tensors='pt')\n","num_tokens = torch.numel(tokens)"],"metadata":{"id":"z0dxdCSiTRI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# example data\n","text['text'][1234]"],"metadata":{"id":"jDyrz-p6uoyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dr4zdQgn33Ch"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# What does perplexity mean?"],"metadata":{"id":"vhRHd-SYyiVs"}},{"cell_type":"code","source":["# each list item is model outputs (logits)\n","situations = [\n","    [ 1,1,1,9 ],\n","    [ 1,1,1,2 ],\n","    [ 1,1,2,2 ],\n","    [ 3,1,1,2 ],\n","    [ 9,1,1,1 ] ]\n","\n","y = 3 # target category index\n","\n","# create a figure\n","plt.figure(figsize=(12,4))\n","xlabls = [] # x-axis tick labels\n","\n","for i,sit in enumerate(situations):\n","\n","  # raw model output (logits)\n","  model_output = torch.tensor([sit],dtype=torch.float64)\n","\n","  # log-softmax\n","  softmax_output = torch.exp(model_output) / torch.sum(torch.exp(model_output))\n","  log_softmax = torch.log(softmax_output)\n","\n","  # negative log-likelihood loss\n","  loss = -log_softmax[0,y]\n","\n","  # perplexity\n","  ppl = torch.exp(loss)\n","\n","  # draw the results\n","  plt.bar(np.array([.7,.9,1.1,1.3])+i,model_output[0].detach(),width=.2,edgecolor='k')\n","  plt.text(1.3+i,model_output[0,-1].detach()+.1,'Targ',font={'size':14},ha='center',va='bottom')\n","\n","  # print the results\n","  print(f'Situation \"{i}\"')\n","  print(f'  Raw logits: {[f\"{o}\" for o in model_output[0]]}')\n","  print(f'  Softmax: {[f\"{o:.4f}\" for o in log_softmax[0]]}')\n","  print(f'  Loss: {loss.item():.4f}')\n","  print(f'  Perplexity: {ppl.item():.4f}\\n')\n","\n","  # x-axis tick label\n","  xlabls.append(f'\"{i}\"\\nppl = {ppl.item():.3f}')\n","\n","\n","\n","plt.gca().set(title='Model outputs (logits) and perplexity',ylabel='Logits',\n","              xticks=range(1,len(situations)+1),xticklabels=xlabls,ylim=[0,10])\n","plt.show()"],"metadata":{"id":"xgqDtnspyiS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"38zIoCewyiE0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Perplexity in a small sample"],"metadata":{"id":"RckOMx9DlSwP"}},{"cell_type":"code","source":["# get a batch of data\n","seq_len = gpt2.config.n_positions # 1024\n","batch_size = 4\n","\n","ix = torch.randint(num_tokens-seq_len,size=(batch_size,))\n","X  = tokens[0][ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","# forward pass to get the loss (internally calculated)\n","outputs = gpt2(X,labels=X)\n","\n","# perplexity is exp(loss)\n","print(f'Perplexity in this random batch is {torch.exp(outputs.loss).item():.3f}')"],"metadata":{"id":"C8xdju0gbkzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nBCLbq4YmJm1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Perplexity across many samples"],"metadata":{"id":"xHceZpyQm-jH"}},{"cell_type":"code","source":["nSamples = 300\n","losses = np.zeros(nSamples)\n","\n","for i in range(nSamples):\n","\n","  # get a batch of data\n","  ix = torch.randint(num_tokens-seq_len,size=(batch_size,))\n","  X  = tokens[0][ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # test it\n","  with torch.no_grad():\n","    outputs = gpt2(X,labels=X)\n","\n","  # calculate and store losses\n","  losses[i] = outputs.loss.item()\n","\n","# perplexity is exp(average)\n","perplexity = np.exp(losses.mean())"],"metadata":{"id":"vjWu3Zdpf4hQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","plt.plot(np.exp(losses),'k.',markersize=6,label='Random segments')\n","plt.axhline(perplexity,color='r',linewidth=2,label='Average')\n","\n","plt.legend()\n","plt.gca().set(xlim=[-2,nSamples+2],xlabel='Sample index (random positions)',ylabel='Perplexity',\n","              title='Perplexity in samples and on average')\n","plt.show()"],"metadata":{"id":"bpDy_EcMLMRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Lw1SBhk6LMOa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Perplexity over the text (no batches)"],"metadata":{"id":"-ow-tpoXLMLW"}},{"cell_type":"code","source":["nSamples = num_tokens//seq_len # how many samples fit into the data\n","perplexities = np.zeros(nSamples)\n","\n","for i in range(nSamples):\n","\n","  # start and end values\n","  start = i*seq_len\n","  end = start + seq_len\n","\n","  # the data\n","  X = tokens[0][start:end].to(device)\n","\n","  # test it\n","  with torch.no_grad():\n","    outputs = gpt2(X,labels=X)\n","\n","  # calculate and store perplexities\n","  perplexities[i] = torch.exp(outputs.loss).item()\n","\n","ave_perplexity = perplexities.mean()"],"metadata":{"id":"vJiBkNjILMFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","plt.plot(perplexities,'ko-',markersize=6,markerfacecolor=[.7,.7,.7],label='Segment')\n","plt.axhline(perplexities.mean(),linestyle='--',color='m',zorder=-4,label='Average')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Sample position (ordered)',ylabel='Perplexity',xlim=[-2,nSamples+2])\n","plt.show()"],"metadata":{"id":"eM0cSJD_LMC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"R7w4ed7wT9sn"},"execution_count":null,"outputs":[]}]}