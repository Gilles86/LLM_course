{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOZ06f1l/WlTSS0qZRuiodi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 3:</h2>|<h1>Evaluating LLMs<h1>|\n","|<h2>Section:</h2>|<h1>Qualitative evaluations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Distributions of model hidden-state activations<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"jc-wzyEfWmw0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6kR6FqjfpeL"},"outputs":[],"source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","# for pairplot\n","import seaborn as sns\n","import pandas as pd\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","# load pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')\n","gpt2.eval()\n","\n","# and the tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"PsrX9u8K3AEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3i386vZJ49_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get model outputs and hidden-layer activations"],"metadata":{"id":"6WOwQSrcLAG0"}},{"cell_type":"code","source":["# the text\n","text = 'The goal of a correlation analysis is to compute a correlation coefficient. This coefficient is indicated using r, and is a number that encodes the normalized strength of the linear relationship between two variables. The normalization imposes boundaries of -1 to +1. Negative, zero, and positive correlation coefficients have distinct interpretations.'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","tokens"],"metadata":{"id":"CcCcEIMek9X5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the outputs of the models\n","with torch.no_grad():\n","  outputs = gpt2(tokens,\n","                 output_hidden_states=True)"],"metadata":{"id":"hyzluVtYefT_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'There are {len(tokens[0])} tokens.')\n","print(f'There are {len(outputs.hidden_states)} \"hidden states.\"')\n","print(f'And each hidden state has size {list(outputs.hidden_states[3].shape)}.')"],"metadata":{"id":"ktaXZrNTXtsp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"80TQkPvXXtol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Scatter plots"],"metadata":{"id":"ox8TP-_VXtmC"}},{"cell_type":"code","source":["# pick one layer to visualize\n","whichLayer = 3\n","\n","y = outputs.hidden_states[whichLayer][0,1:,:].detach()\n","\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","# plot by embedding\n","axs[0].plot(y.T,'o',alpha=.4)\n","axs[0].set(xlabel='Embedding dimension',ylabel='Activation',title='Each color is a token')\n","\n","# plot by token\n","axs[1].plot(y,'o',alpha=.4)\n","axs[1].set(xlabel='Token index',ylabel='Activation',title='Each color is an embedding dimension')\n","\n","plt.suptitle(f'Output activations from attention layer {whichLayer}',fontweight='bold')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"GibW5K31YMI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wyNRWk4KhPvz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Layer covariance and shared variance plots\n"],"metadata":{"id":"CycuA-HjhPs-"}},{"cell_type":"code","source":["# first we gather all the data into one matrix\n","\n","# number of points, excluding the first token\n","npnts = (len(tokens[0])-1) * gpt2.config.n_embd\n","\n","# initialize\n","allDataMat = np.zeros((len(outputs.hidden_states),npnts))\n","\n","# loop over all layers\n","for layeri in range(len(outputs.hidden_states)):\n","\n","  # extract all activations from this layer\n","  vectActs = outputs.hidden_states[layeri][0,1:,:].detach().flatten().numpy()\n","\n","  # put it into a matrix\n","  allDataMat[layeri,:] = vectActs"],"metadata":{"id":"hF42JfIShRLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"atY2Xvc-ssrk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a covariance matrix\n","covmat = np.cov(allDataMat)\n","\n","# and a correlation matrix (squared to get R2 -> shared variance)\n","cormat = np.corrcoef(allDataMat)**2\n","\n","# layer names\n","layerlabels = ['emb' if i==0 else f'h.{i-1}' for i in range(13)]\n","\n","# set the figure\n","_,axs = plt.subplots(1,2,figsize=(10,5))\n","\n","h = axs[0].imshow(covmat,vmin=-10,vmax=10,origin='lower')\n","axs[0].set(xlabel='Layer',ylabel='Layer',title='Covariance matrix',\n","           xticks=range(0,len(layerlabels),2),xticklabels=layerlabels[::2],\n","           yticks=range(1,len(layerlabels),2),yticklabels=layerlabels[1::2])\n","plt.colorbar(h,ax=axs[0],pad=.04,fraction=.046)\n","\n","h = axs[1].imshow(100*cormat,vmin=0,vmax=100,origin='lower')\n","axs[1].set(xlabel='Layer',ylabel='Layer',title=r'R$^2$ matrix (% shared variance)',\n","           xticks=range(0,len(layerlabels),2),xticklabels=layerlabels[::2],\n","           yticks=range(1,len(layerlabels),2),yticklabels=layerlabels[1::2])\n","plt.colorbar(h,ax=axs[1],pad=.04,fraction=.046)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"qSzkS6ivh87B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"L6jGhX5Qdsof"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pairplot in seaborn"],"metadata":{"id":"Y6l0q0o4dsij"}},{"cell_type":"code","source":["# pairplots are great but not really scalable to many variables\n","numpnts = 1000 # reduce the number of data points\n","numlayers = 4 # only show the first 4 layers\n","\n","# convert to pandas dataframe and downsample\n","df = pd.DataFrame(allDataMat[:numlayers,:numpnts].T)\n","\n","# create the pairplot (with internal pandas transform)\n","sns.pairplot(df)\n","plt.show()"],"metadata":{"id":"87W5NT07ji_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nMI1vFSpXtjU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Histograms"],"metadata":{"id":"hC3y28rkXtgf"}},{"cell_type":"code","source":["binbounds = np.linspace(-15,15,301)\n","\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","linecolors = mpl.cm.plasma_r(np.linspace(0,1,len(outputs.hidden_states)))\n","\n","for layeri in range(len(outputs.hidden_states)):\n","\n","  # calculate histogram of activations from this layer\n","  y,_ = np.histogram(allDataMat[layeri,:],bins=binbounds)\n","\n","  axs[0].plot(binbounds[:-1],y,color=linecolors[layeri],label=layerlabels[layeri])\n","  axs[1].plot(binbounds[:-1],np.log(y+1e-14),color=linecolors[layeri],label=layerlabels[layeri])\n","\n","for a in axs:\n","  a.legend(fontsize=8)\n","  a.set(xlim=binbounds[[0,-1]],ylim=[0,None],xlabel='Activation value',ylabel='Count')\n","\n","axs[0].set_title('Linear counts')\n","axs[1].set_title('Log counts')\n","plt.show()"],"metadata":{"id":"Y9-XeUHvXtdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iTLme00iZfqX"},"execution_count":null,"outputs":[]}]}