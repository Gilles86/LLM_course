{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMqc0cfKq3MaCrIs7XeJonW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 3:</h2>|<h1>Evaluating LLMs<h1>|\n","|<h2>Section:</h2>|<h1>Quantitative evaluations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Perplexing perplexities<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"TlposdYOUycC"}},{"cell_type":"code","source":["# run this code, then restart the python session (and then comment it out)\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"G0Pb0AW471z3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6kR6FqjfpeL"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import requests\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","from datasets import load_dataset\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":["# load SMALL pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n","gpt2.eval()\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"PsrX9u8K3AEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-SFoWnpbV1tO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: A function for perplexity"],"metadata":{"id":"mmA9_M2WV4SL"}},{"cell_type":"code","source":["def calc_perplex(tokens,model=gpt2,seq_len=1024):\n","\n","  # number of segments in the total token sequence\n","  nSegments = torch.numel(tokens) //\n","\n","  # initialize losses\n","  sum_losses = 0.\n","\n","  for i in range(nSegments):\n","\n","    # find start and end indices\n","    start =\n","    end =\n","\n","    # get the token sequence (with batch dimension)\n","    X = tokens\n","\n","    # forward pass\n","    with torch.no_grad():\n","      outputs = model(X\n","\n","    # calculate and store this batch's loss\n","    sum_losses +=\n","\n","  # after samples loop, perplexity = exp(average per-token loss)\n","  perplexity =\n","\n","  return perplexity"],"metadata":{"id":"BDyIRKV8WG_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate 50k random tokens (batch=1)\n","randtokens = torch.randint(tokenizer\n","\n","# test the perplexity\n","calc_perplex(randtokens.to(device))"],"metadata":{"id":"3I1MfGxHV4PO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the expected perplexity of random tokens equals the vocabulary size (or does it??)\n","torch.exp(-"],"metadata":{"id":"KVWZirPlUz_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pNWPuM9nvHBt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Perplexities of different texts"],"metadata":{"id":"oaiRlif9Uz8a"}},{"cell_type":"code","source":["# all books have the same url format; they are unique by numerical code\n","baseurl='https://www.gutenberg.org/cache/epub/'\n","\n","bookurls = [\n","    # code       title\n","    ['84',    'Frankenstein'    ],\n","    ['64317', 'GreatGatsby'     ],\n","    ['11',    'AliceWonderland' ],\n","    ['1513',  'RomeoJuliet'     ],\n","    ['76',    'HuckFinn'        ],\n","    ['219',   'HeartDarkness'   ],\n","    ['2591',  'GrimmsTales'     ],\n","    ['2148',  'EdgarAllenPoe'   ],\n","    ['36',    'WarOfTheWorlds'  ],\n","    ['829',   'GulliversTravels']\n","]"],"metadata":{"id":"-lIfoG_EUz5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ppls = np.zeros(len(bookurls))\n","\n","for i,(code,title) in enumerate(bookurls):\n","\n","  # get the text tokens\n","  fullurl = baseurl + code + '/pg' + code + '.txt'\n","  text = requests.get(fullurl).text\n","  tokens = tokenizer.encode(text,return_tensors='pt')\n","\n","  # just the first 50k tokens for speed and direct comparison\n","  tokens = tokens[0][:50000].unsqueeze(0).to(device)\n","\n","  # calculate perplexity\n","  ppls[i] =\n","\n","  print(f'{} has perplexity {}')"],"metadata":{"id":"xq03qbxlUz2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","plt.bar(\n","\n","plt.show()"],"metadata":{"id":"uEiDKNcMYsrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"scoOxMqyUzzq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Perplexities of different models"],"metadata":{"id":"t5x-JkPTUzw0"}},{"cell_type":"code","source":["text = load_dataset('wikitext','wikitext-2-raw-v1',split='test')\n","tokens = tokenizer.encode('\\n'.join(text['text'][:2000]),return_tensors='pt').to(device)\n","torch.numel(tokens)"],"metadata":{"id":"IEPQplsSUzuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","print(f'The wikitext dataset has perplexity {wiki_per:.3f}')"],"metadata":{"id":"BofOHxEOUzrW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary of modelname:identifier\n","model_ids = {\n","    'small':  'gpt2',        # 124M\n","    'medium': 'gpt2-medium', # 355M\n","    'large':  'gpt2-large',  # 774M\n","    'xl':     'gpt2-xl'      # 1.6B\n","}\n","\n","# load all models into a dictionary\n","models = {}\n","for name, id in model_ids.items():\n","  models[name] = AutoModelForCausalLM.from_pretrained(id).to(device)\n","  # switch to eval mode"],"metadata":{"id":"wwvDZ1nFx7Zv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["perp_models = np.zeros\n","\n","for i,(name, model) in enumerate(models.items()):\n","  perp_models[i] =\n","  print(f'\"{name:>8}\" has perplexity {perp_models[i]:.2f}')"],"metadata":{"id":"39FZjKT_x7dX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","plt.bar("],"metadata":{"id":"tIsXEASWx7gX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uZYNMa9rx7qD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Perplexities with different sequence lengths"],"metadata":{"id":"5-KjU-vyUzlm"}},{"cell_type":"code","source":["seq_lengths = 2**np.arange(5,11)\n","\n","perp_by_len = np.zeros(len(seq_lengths))\n","\n","for i in range(len(seq_lengths)):\n","  perp_by_len[i] = calc_perplex(tokens\n","  print(f'Sequence length {seq_lengths[i]:4} has perplexity {perp_by_len[i]:6.2f}')\n"],"metadata":{"id":"xJ9rmXUkUzi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bar plot here :)"],"metadata":{"id":"LGbEq6BgUzgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EyZ7qMhNUzax"},"execution_count":null,"outputs":[]}]}