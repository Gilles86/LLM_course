{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPZzczMw5Z2Zus6koRI+EVt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 3:</h2>|<h1>Evaluating LLMs<h1>|\n","|<h2>Section:</h2>|<h1>Quantitative evaluations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: MAUVE diversity<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"vYostJ-JW2L8"}},{"cell_type":"code","source":["!pip install mauve-text"],"metadata":{"id":"cfiwgWW6W_5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import requests\n","\n","import mauve\n","from tqdm import tqdm\n","\n","import torch\n","from transformers import AutoTokenizer,AutoModelForCausalLM\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"ocJNThBZtJDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"IyoQSJCCtJAA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import GPT2 and disable normalizations\n","gptSmall = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n","gptLarge = AutoModelForCausalLM.from_pretrained('gpt2-large').to(device)\n","gptSmall.eval()\n","gptLarge.eval()\n","\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","tokenizer.set_pad_token_id = tokenizer.eos_token_id"],"metadata":{"id":"cHK58CUctI85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BWMUP9a2vxGb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Get models data"],"metadata":{"id":"1qtZUwc8vxDT"}},{"cell_type":"code","source":["# ~25 mins\n","gptSmall_data = []\n","gptLarge_data = []\n","\n","numReps = 100\n","numTokens = 400\n","\n","# generate token sequences\n","for _ in tqdm(range(numReps),desc='Generating tokens...'):\n","\n","  ### in GPT2-small\n","  out = gptSmall.generate(\n","      torch.tensor([[tokenizer.bos_token_id]]).to(device),\n","      pad_token_id = tokenizer.eos_token_id,\n","      min_length = numTokens,\n","      max_length = numTokens,\n","      do_sample  = True,\n","      top_k      = 50,\n","      top_p      = .95,\n","  )\n","  gptSmall_data.append(out[0][1:])\n","\n","\n","  ### and repeat for GPT2-large\n","  out = gptLarge.generate(\n","      torch.tensor([[tokenizer.bos_token_id]]).to(device),\n","      pad_token_id = tokenizer.eos_token_id,\n","      min_length = numTokens,\n","      max_length = numTokens,\n","      do_sample  = True,\n","      top_k      = 50,\n","      top_p      = .95,\n","  )\n","  gptLarge_data.append(out[0][1:])"],"metadata":{"id":"9XztC7H-X4OU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LEuWqFgphsns"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Test against human texts"],"metadata":{"id":"G9bWuslPw_Ei"}},{"cell_type":"code","source":["# all books have the same url format; they are unique by numerical code\n","baseurl = 'https://www.gutenberg.org/cache/epub/'\n","\n","bookurls = [\n","    # code       title\n","    ['64317', 'GreatGatsby'     ],\n","    ['11',    'AliceWonderland' ],\n","    ['1513',  'RomeoJuliet'     ],\n","    ['76',    'HuckFinn'        ],\n","    ['2148',  'EdgarAllenPoe'   ],\n","    ['829',   'GulliversTravels']\n","]"],"metadata":{"id":"fX_ZgpveSYPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mauves = np.zeros((2,len(bookurls)))\n","booki = 0\n","\n","xticklabels = []\n","\n","# loop through the books\n","for code,title in bookurls:\n","\n","  # get the text\n","  fullurl = baseurl + code + '/pg' + code + '.txt'\n","  txt = requests.get(fullurl).text\n","\n","  xticklabels.append(title[:5])\n","\n","  # tokenize the text\n","  tokens = tokenizer.encode(txt,return_tensors='pt')\n","\n","  # get random contiguous segments\n","  ix = torch.randint(len(tokens[0])-numTokens,size=(numReps,))\n","  human_data = tokens[0][ix[:,None] + torch.arange(numTokens)].to(device)\n","\n","\n","\n","  # --- GPT_SMALL mauve score\n","  mauve_output  = mauve.compute_mauve(\n","      p_tokens  = gptSmall_data,\n","      q_tokens  = human_data,\n","      verbose   = False,\n","      device_id = 0\n","  )\n","  mauves[0,booki] = mauve_output.mauve\n","\n","\n","\n","  # --- GPT_LARGE mauve score\n","  mauve_output  = mauve.compute_mauve(\n","      p_tokens  = gptLarge_data,\n","      q_tokens  = human_data,\n","      verbose   = False,\n","      device_id = 0\n","  )\n","  mauves[1,booki] = mauve_output.mauve\n","\n","  # update book index\n","  booki += 1\n"],"metadata":{"id":"xkS-ngibYEcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","plt.bar(np.arange(len(bookurls))-.2,mauves[0,:],width=.4,facecolor=[.7,.9,.7],edgecolor='k',label='Small')\n","plt.bar(np.arange(len(bookurls))+.2,mauves[1,:],width=.4,facecolor=[.7,.7,.9],edgecolor='k',label='Large')\n","\n","plt.gca().set(xticks=range(6),xticklabels=xticklabels,ylabel='MAUVE score')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"K2tGQ0KAdymp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xZTkQg6rJeEI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: MAUVE after prompting"],"metadata":{"id":"DgeKRVLBN9hi"}},{"cell_type":"code","source":["# ~18 mins\n","gptLarge_dataGT = []\n","nPromptTokens = 100\n","\n","# get large tokens, prompted by human text\n","for b in range(numReps):\n","  out = gptLarge.generate(\n","      human_data[b,:nPromptTokens].unsqueeze(0).to(device),\n","      pad_token_id = tokenizer.eos_token_id,\n","      min_length = nPromptTokens+numTokens,\n","      max_length = nPromptTokens+numTokens,\n","      do_sample  = True,\n","      top_k      = 50,\n","      top_p      = .95,\n","  )\n","  gptLarge_dataGT.append(out[0,nPromptTokens:])\n","\n","  if b%5==0: print(f'Finished {b:2}/{numReps} generations.')"],"metadata":{"id":"GjmErL0NN9Xk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- GPT_LARGE mauve score\n","mauve_output  = mauve.compute_mauve(\n","    p_tokens  = gptLarge_dataGT,\n","    q_tokens  = human_data,\n","    verbose   = False,\n","    device_id = 0\n",")"],"metadata":{"id":"K91iCbPmO9Gd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Naive MAUVE score : {mauves[1,-1]:.3f}')\n","print(f'Prompt MAUVE score: {mauve_output.mauve:.3f}')"],"metadata":{"id":"NhUWjiCJTZho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Uykcl2A3TZWz"},"execution_count":null,"outputs":[]}]}