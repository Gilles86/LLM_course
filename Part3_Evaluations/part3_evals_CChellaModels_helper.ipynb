{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNqLVrFh5IbrArYwPeRJLFI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 3:</h2>|<h1>Evaluating LLMs<h1>|\n","|<h2>Section:</h2>|<h1>Quantitative evaluations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: HellaSwag evals in several models<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"0GM088xUvAc2"}},{"cell_type":"code","source":["# run first to install and then restart\n","# !pip install bitsandbytes\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"17eiuiHHRQFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jJwR4KcRPF4"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","\n","from datasets import load_dataset\n","from tqdm import tqdm # progress bar for for-loops\n","from transformers import AutoTokenizer,AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":[],"metadata":{"id":"bIneion3w4it"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import and inspect the Zephyr model"],"metadata":{"id":"PBy15L8k1NyK"}},{"cell_type":"code","source":["# tokenizer\n","zephyr_tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-alpha')\n","\n","# need a BitsAndBytesConfig object\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit = True,\n","    bnb_4bit_compute_dtype = 'float16', # multiplication at higher precision\n","    bnb_4bit_use_double_quant = True,   # help preserves accuracy\n",")\n","\n","# import the model\n","zephyr_model = AutoModelForCausalLM.from_pretrained('HuggingFaceH4/zephyr-7b-alpha',\n","    quantization_config = quantization_config)"],"metadata":{"id":"A3tTkzq_Zs7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# switch to eval and move to GPU"],"metadata":{"id":"e571MX2pFmtw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6SKHUcW2RJuB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# counting parameters via numel\n","param_total = sum(p.numel() for p in\n","param_trainable = sum( if p.requires_grad)\n","\n","# and manually based on the model description\n","emb   =\n","attn  =\n","mlp   =\n","unemb =\n","man_total = emb + 32*(attn+mlp) + unemb\n","\n","# print the results\n","print(f'Total parameters: {param_total:13,} ({}B)')\n","print(f'Trainable params: {param_trainable:13,}')\n","print(f'Non-trainable   :\n","print(f'Manual counting :"],"metadata":{"id":"h2HvXtuLxwgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# example\n","zephyr_model.model.layers[4].self_attn.q_proj#.weight.shape[0]/(4096*4096)"],"metadata":{"id":"EWQyn02xXsEl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NPOOOW6-1jXH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: A function to test one HellaSwag sample"],"metadata":{"id":"5uObQc2pTKDK"}},{"cell_type":"code","source":["# a function to calculate accuracy on one sample\n","def oneHellaSample(sample,model,tokenizer):\n","\n","  # find context length\n","  context = sample['ctx']\n","  context_len = len( tokenizer.encode(context) )\n","\n","  smSeqs = np.zeros(len(sample['endings']))\n","\n","  # loop over candidate endings, create prompts, get logits, and sum prob scores\n","  for opti in range(len(sample['endings'])):\n","\n","    # prompts and their lengths\n","    prompt = f'{context} {sample[\"endings\"][]}'\n","    prompt_tox = # tokenize\n","    prompt_len = # number of tokens\n","\n","    # forward pass through the model\n","    with torch.no_grad():\n","      logits = model( # just get the logits\n","\n","    # convert to log probabilities\n","    log_probs =\n","\n","    # get the predicting log-probs for each token\n","    smSeq =\n","\n","    # Sum log-probabilities to get the total log-likelihood\n","    smSeqs[opti] =\n","\n","  return smSeqs # also return the index of the correct answer"],"metadata":{"id":"WooU1ZxfACbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import the HellaSwag validation set\n","dataset = load_dataset('hellaswag',split='validation',trust_remote_code=True)\n","dataset"],"metadata":{"id":"FC-r6Gzt1N8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test it with one sample\n","loglikelihoods,answer = oneHellaSample\n","\n","if np.argmax(loglikelihoods)==answer:\n","  print('Model was correct!')\n","else:\n","  print('Model needs more training ;)')"],"metadata":{"id":"DiyeLM4kACYq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XsvM03i21l86"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Evaluate Zephyr and GPT2-small"],"metadata":{"id":"V2z7eFse1l5O"}},{"cell_type":"code","source":["# import GPT2 and disable normalizations\n","gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n","gpt2_model.eval()\n","\n","gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')"],"metadata":{"id":"Ym93MqZ0wAuj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_samples = 500\n","\n","accuracies = np.zeros((2,num_samples))\n","\n","\n","# loop over data samples with progress bar\n","for datai in tqdm(range(num_samples),desc='Evaluating on HellaSwag'):\n","\n","  # extract one sample from the data\n","  example =\n","\n","  # ZEPHYR: calculate the loglikelihoods\n","  loglikelihoods,answer =\n","\n","  if np.argmax(loglikelihoods)==answer:\n","    accuracies[0,datai] = 1\n","  # -------------------------------------\n","\n","\n","  # repeat for GPT2\n","\n"],"metadata":{"id":"395s15djF-Ib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# report the average accuracy\n","print(f'Zepher had {np.mean(accuracies[0,:])*100:.1f}% accuracy.')\n","print(f'  GPT2 had {np.mean(accuracies[1,:])*100:.1f}% accuracy.')"],"metadata":{"id":"p1vqOcLP7K9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","plt.figure(figsize=(12,3))\n","\n","plt.plot...\n","\n","plt.legend()\n","plt.gca().set(ylim=[-.2,.75],xlim=[-2,num_samples+1],xlabel='Swag item (index)',\n","              yticks=[0,.5],yticklabels=['Error','Correct'])\n","plt.show()"],"metadata":{"id":"IgGfnvtcw4Zn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3FVeKJlv2SO_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example that Zephyr got and GPT2 missed"],"metadata":{"id":"Y2jAYX2M2SL4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2oAkhBv59pmT"},"execution_count":null,"outputs":[]}]}