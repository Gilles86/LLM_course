{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMOeFCMyeIIMiYVDbO3ZLx6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 3:</h2>|<h1>Evaluating LLMs<h1>|\n","|<h2>Section:</h2>|<h1>Quantitative evaluations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Import large models using bitsandbytes<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"vgFsfSDKIsFu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztpysSax_Cnd"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM"]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"5bxN3HbQ_6yT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try to import the zephyr model...\n","zephyr_model = AutoModelForCausalLM.from_pretrained('HuggingFaceH4/zephyr-7b-alpha').to(device)"],"metadata":{"id":"U7YYQTog_Gyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GIAzVbqV_Gld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run first to install and then restart\n","!pip install bitsandbytes"],"metadata":{"id":"RfELMWnn_GvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# need a BitsAndBytesConfig object\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit = True,\n","    bnb_4bit_compute_dtype = 'float16', # multiplication at higher precision\n","    bnb_4bit_use_double_quant = True,   # help preserves accuracy\n",")\n","\n","# import the model\n","zephyr_model = AutoModelForCausalLM.from_pretrained('HuggingFaceH4/zephyr-7b-alpha',\n","    quantization_config = quantization_config)\n","\n","zephyr_model"],"metadata":{"id":"74AN1A8rN7iy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# documentation about setting up the config object\n","# https://huggingface.co/docs/transformers/v4.27.1/en/main_classes/quantization#transformers.BitsAndBytesConfig"],"metadata":{"id":"FRql3ZYON7cl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# documentation about this model:\n","# https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha"],"metadata":{"id":"WpkXkDh3N7Ul"},"execution_count":null,"outputs":[]}]}