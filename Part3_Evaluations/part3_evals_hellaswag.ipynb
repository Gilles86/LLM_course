{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOcgZBa/7WkRh5x6yhaiGiK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 3:</h2>|<h1>Evaluating LLMs<h1>|\n","|<h2>Section:</h2>|<h1>Quantitative evaluations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Hellaswag<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"Wwuy5roHLzxN"}},{"cell_type":"code","source":["# run this code, then restart the python session (and then comment it out)\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"NNq1Go9_B1jn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b31f3ui4BxL0"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n","\n","from datasets import load_dataset\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# import the model and tokenizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# import the mdoel and disable normalizations\n","gpt2 = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(device)\n","gpt2.eval()\n","\n","tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"],"metadata":{"id":"89N_ox9DJeJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import the HellaSwag validation set\n","dataset = load_dataset('hellaswag',split='validation',trust_remote_code=True)\n","dataset"],"metadata":{"id":"Oo0fl8pMJlj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]"],"metadata":{"id":"cuhJI2xWKEvi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7pBOm9uqlamJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# One small example to show how the eval works"],"metadata":{"id":"jo1BuhDllajO"}},{"cell_type":"code","source":["# pick a random example\n","exampleNum = 224\n","answer = int(dataset[exampleNum]['label']) # the true answer\n","\n","# context tokens and length\n","context = dataset[exampleNum]['ctx']\n","context_len = len( tokenizer.encode(context) )\n","\n","# prompts and their lengths\n","promptC = f'{context} {dataset[exampleNum][\"endings\"][answer]}'\n","promptC_tox = tokenizer.encode(promptC,return_tensors='pt')\n","promptC_len = len( promptC_tox[0] )\n","\n","promptI = f'{context} {dataset[exampleNum][\"endings\"][3-answer]}'\n","promptI_tox = tokenizer.encode(promptI,return_tensors='pt')\n","promptI_len = len( promptI_tox[0] )\n","\n","# show the prompts\n","print(f'Context:\\n   \"{context}\"')\n","print(f'Correct ending:\\n   \"{promptC}\"')\n","print(f'Incorrect ending:\\n   \"{promptI}\"')"],"metadata":{"id":"gLDukm_pKEsJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass through the model\n","with torch.no_grad():\n","  logitsC = gpt2(promptC_tox.to(device)).logits\n","  logitsI = gpt2(promptI_tox.to(device)).logits\n","\n","# log softmax (more numerically stable than prob values)\n","lsm_logitsC = F.log_softmax(logitsC,dim=-1)\n","lsm_logitsI = F.log_softmax(logitsI,dim=-1)\n","\n","\n","# get the sequence of sm logits for the correct prompt\n","lsmSeqC = np.zeros(promptC_len-1)\n","for i in range(0,promptC_len-1):\n","  lsmSeqC[i] = lsm_logitsC[0,i,promptC_tox[0][i+1]]\n","\n","# repeat for the incorrect prompt\n","lsmSeqI = np.zeros(promptI_len-1)\n","for i in range(0,promptI_len-1):\n","  lsmSeqI[i] = lsm_logitsI[0,i,promptI_tox[0][i+1]]\n","\n","\n","# probabilities of prompts (sum of logs equals product of probabilities)\n","probC = lsmSeqC[context_len-1:].sum()\n","probI = lsmSeqI[context_len-1:].sum()\n","\n","# visualize the logits\n","plt.figure(figsize=(12,4))\n","plt.plot(lsmSeqC,'bo-',markersize=10,markerfacecolor='w',label=f'Correct ending ($\\sum\\ln(p)$={probC:.3f})')\n","plt.plot(lsmSeqI,'rs-',label=f'Incorrect ending ($\\sum\\ln(p)$={probI:.3f})')\n","plt.axvline(context_len-1.5,linestyle='--',color='gray')\n","\n","plt.gca().set(xlabel='Token position (index)',ylabel='Log-softmax probs from distribution',title='Token log-probabilities in HellaSwag evaluation')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"rNjGstXt58cn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cBS3xRt8Cc_6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Incorporating all endings"],"metadata":{"id":"5uObQc2pTKDK"}},{"cell_type":"code","source":["example = dataset[42]\n","\n","\n","# find context length\n","context = example['ctx']\n","context_len = len( tokenizer.encode(context) )\n","answer = int(example['label']) # the true answer\n","\n","loglikelihoods = np.zeros(len(example['endings']))\n","\n","# loop over candidate endings, create prompts, get logits, and sum prob scores\n","for opti in range(len(example['endings'])):\n","\n","  # prompts and their lengths\n","  prompt = f'{context} {example[\"endings\"][opti]}'\n","  prompt_tox = tokenizer.encode(prompt,return_tensors='pt')\n","  prompt_len = len( prompt_tox[0] )\n","\n","  # forward pass through the model\n","  with torch.no_grad():\n","    logits = gpt2(prompt_tox.to(device)).logits\n","\n","  # convert to log probabilities\n","  log_probs = F.log_softmax(logits,dim=-1)\n","\n","  # get the predicting log-probs for each token\n","  smSeq = np.array([ log_probs[0,i,prompt_tox[0][i+1]].item() for i in range(prompt_len-1)])\n","\n","  loglikelihoods[opti] = np.sum(smSeq)\n","\n","\n","# consider accuracy\n","if np.argmax(loglikelihoods)==answer:\n","  print('Model was correct!')\n","else:\n","  print('Model needs more training ;)')"],"metadata":{"id":"WooU1ZxfACbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C0zwr4_30SuC"},"execution_count":null,"outputs":[]}]}