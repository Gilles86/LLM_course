{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNgKs/yogW8Icd3QOiBJg70"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Modifying MLP<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Explorations in subspace removal<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"dVdgcMR4IljT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuKeB769HOkN"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib.gridspec import GridSpec\n","\n","import scipy.stats as stats\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","nlayers = model.config.n_layer\n","model.eval()"],"metadata":{"id":"83A8PoVGRFj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5jWDecMYNTzh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Forward pass and get MLP activations"],"metadata":{"id":"BHcZCWR2I-lF"}},{"cell_type":"code","source":["# https://en.wikipedia.org/wiki/Maurice_Ravel\n","text = \"Joseph Maurice Ravel (7 March 1875 â€“ 28 December 1937) was a French composer, pianist and conductor. He is often associated with Impressionism along with his elder contemporary Claude Debussy, although both composers rejected the term. In the 1920s and 1930s Ravel was internationally regarded as France's greatest living composer.\"\n","tokens = tokenizer.encode(text,return_tensors='pt').to(device)\n","tokens.shape, tokens"],"metadata":{"id":"fYPkVigeOv47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary to store the mlp activations\n","mlp_values = {}\n","\n","def hook(module, input, output):\n","  mlp_values[f'L{whichlayer}'] = output[0].detach().cpu() # detach from the computational graph\n","\n","# surgery ;)\n","whichlayer = 9\n","handle = model.transformer.h[whichlayer].mlp.c_fc.register_forward_hook(hook)"],"metadata":{"id":"pQ6zFv8PY4B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  out_clean = model(tokens,output_hidden_states=True)\n","\n","handle.remove()\n","\n","mlp = mlp_values[f'L{whichlayer}']\n","mlp.shape"],"metadata":{"id":"YAMzPriHL7SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the activations distribution\n","plt.figure(figsize=(8,4))\n","plt.hist(mlp.flatten(),80,edgecolor='k',facecolor='lightgray')\n","\n","plt.gca().set(xlabel='Activation value',ylabel='Count (a.u.)',yticks=[])\n","plt.show()"],"metadata":{"id":"gKzYC8iCL1bH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8vaVMxoSPssb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SVD and reconstruct data without top PC"],"metadata":{"id":"YERQ_arBPsp2"}},{"cell_type":"code","source":["U,s,Vt = torch.linalg.svd(mlp,full_matrices=False)\n","S = torch.diag(s)\n","\n","# remove the top component\n","S[0,0] = 0\n","\n","# reconstruct without the top component\n","proj = U @ S @ Vt\n","\n","proj.shape"],"metadata":{"id":"FopFxLhpVjpJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add back the mean offset\n","proj = proj + mlp.mean(dim=-1,keepdims=True)\n","\n","proj[4,:].mean(), mlp[4,:].mean()"],"metadata":{"id":"balcKCrYUs3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Visualize the impact\n","cov_mlp = torch.cov(mlp)\n","cov_prj = torch.cov(proj)\n","\n","\n","fig,axs = plt.subplots(2,3,figsize=(12,7))\n","\n","# show the data\n","axs[0,0].plot(mlp.flatten()[::14],proj.flatten()[::14],'ko',markersize=3,markerfacecolor=[.7,.9,.7,.3])\n","axs[0,0].set(xlabel='Original data',ylabel='Reconstructed')\n","\n","# and their distributions\n","binedges = np.linspace(-5,4,121)\n","y,_ = np.histogram(mlp.flatten(),binedges)\n","axs[0,1].plot(binedges[:-1],y,label='Original')\n","y,_ = np.histogram(proj.flatten(),binedges)\n","axs[0,1].plot(binedges[:-1],y,label='Recon')\n","axs[0,1].legend()\n","axs[0,1].set(xlim=binedges[[0,-1]],xlabel='Data value',ylabel='Count',yticks=[],title='Activation value distributions')\n","\n","# eigenspectra\n","mlp_evals,_ = np.linalg.eig(cov_mlp)\n","rcn_evals,_ = np.linalg.eig(cov_prj)\n","axs[0,2].plot(100*mlp_evals/mlp_evals.sum(),'ks-',markersize=8,linewidth=.4,markerfacecolor=[.9,.7,.7,.6],label='Orig')\n","axs[0,2].plot(100*rcn_evals/rcn_evals.sum(),'ko-',markersize=8,linewidth=.4,markerfacecolor=[.7,.7,.9,.6],label='Recon')\n","axs[0,2].legend()\n","axs[0,2].set(ylabel='% variance explained',xticks=range(0,len(mlp_evals),2),xlim=[-1,15.5],title='Eigenspectra',xlabel='Component')\n","\n","\n","# show the covariance matrices\n","h = axs[1,0].imshow(cov_mlp,vmin=-.2,vmax=.6,cmap='plasma',aspect='auto')\n","fig.colorbar(h,ax=axs[1,0],pad=.01,fraction=.047)\n","axs[1,0].set(xlabel='Tokens',ylabel='Tokens',title='Original covariance')\n","\n","h = axs[1,1].imshow(cov_prj,vmin=-.2,vmax=.6,cmap='plasma',aspect='auto')\n","fig.colorbar(h,ax=axs[1,1],pad=.01,fraction=.047)\n","axs[1,1].set(xlabel='Tokens',ylabel='Tokens',title='Reconstructed covariance')\n","\n","# histogram of covariance values (note: doubled b/c of laziness :P )\n","binedges = np.linspace(-.3,.8,81)\n","y,_ = np.histogram(cov_mlp.flatten(),binedges)\n","axs[1,2].plot(binedges[:-1],y,label='Orig')\n","y,_ = np.histogram(cov_prj.flatten(),binedges)\n","axs[1,2].plot(binedges[:-1],y,label='Recon')\n","axs[1,2].legend()\n","axs[1,2].set(xlim=binedges[[0,-1]],xlabel='Covariance value',ylabel='Count (a.u.)',yticks=[],title='Covariance distributions')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"nMDbUZr8Q8u2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b-JXEPj5gW0P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Explore subspace removal in one layer"],"metadata":{"id":"w6PDIAbWMatk"}},{"cell_type":"code","source":["def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # SVD and reconstruct without top dimension\n","    U,s,Vt = torch.linalg.svd(output[0],full_matrices=False)\n","    S = torch.diag(s)\n","    S[0,0] = 0\n","    proj = U@S@Vt\n","\n","    # add back the mean offset\n","    proj += output[0].mean(dim=-1,keepdims=True)\n","\n","    # return the data\n","    return proj\n","\n","  return hook\n","\n","\n","# implant to halfway through the model\n","whichlayer = nlayers // 2\n","handle = model.transformer.h[whichlayer].mlp.c_fc.register_forward_hook(implant_hook(whichlayer))"],"metadata":{"id":"dAITTHyiSI4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  out_proj = model(tokens,output_hidden_states=True)\n","\n","handle.remove()"],"metadata":{"id":"yoatPbIkVbKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["norms = np.zeros(nlayers+1)\n","\n","for i in range(nlayers+1):\n","  norms[i] = torch.norm(out_clean.hidden_states[i]-out_proj.hidden_states[i])\n","\n","plt.figure(figsize=(8,3))\n","plt.plot(norms,'ko',markerfacecolor=[.9,.7,.7])\n","plt.axvline(whichlayer+.5,linestyle='--',color='gray')\n","\n","plt.gca().set(xlabel='Transformer block',ylabel='HS difference norm',\n","              title='Impact of subspace removal on hidden states vector lengths')\n","plt.show()"],"metadata":{"id":"Kj7u6-BPWDNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cosine similarities for clean data\n","H = out_clean.hidden_states[whichlayer+1][0,1:,:].cpu()\n","H = H / torch.linalg.vector_norm(H,axis=1,keepdims=True)\n","csM_clean = H @ H.T\n","\n","# and subspace rejection\n","H = out_proj.hidden_states[whichlayer+1][0,1:,:].cpu()\n","H = H / torch.linalg.vector_norm(H,axis=1,keepdims=True)\n","csM_proj = H @ H.T\n","\n","\n","# how the matrices\n","fig,axs = plt.subplots(1,3,figsize=(12,3.5))\n","h = axs[0].imshow(csM_clean,vmin=.3,vmax=.8,cmap='plasma')\n","fig.colorbar(h,ax=axs[0],pad=.01,fraction=.047)\n","axs[0].set(xlabel='Tokens',ylabel='Tokens',xticks=[],yticks=[],\n","           title='Clean model: S$_C$ of token pairs')\n","\n","h = axs[1].imshow(csM_proj,vmin=.3,vmax=.8,cmap='plasma')\n","fig.colorbar(h,ax=axs[1],pad=.01,fraction=.047)\n","axs[1].set(xlabel='Tokens',ylabel='Tokens',xticks=[],yticks=[],\n","           title='Subspace-removed: S$_C$ of token pairs')\n","\n","# and their distributions\n","binedges = np.linspace(0,1.1,101)\n","y,_ = np.histogram(csM_clean.flatten(),binedges)\n","axs[2].plot(binedges[:-1],y,label='Clean')\n","y,_ = np.histogram(csM_proj.flatten(),binedges)\n","axs[2].plot(binedges[:-1],y,label='Projected')\n","axs[2].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"iPQ2cFdCPGf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5u6DzFbVVbHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Scatter plot and text heatmap of impact on logits"],"metadata":{"id":"kCstiYxBMags"}},{"cell_type":"code","source":["# get output logits\n","logitdiffs = np.zeros(len(tokens[0]))\n","\n","plt.figure(figsize=(14,4))\n","\n","for toki in range(len(tokens[0])-1):\n","\n","  # get the logit differences\n","  lsm_clean = F.log_softmax(out_clean.logits[0,toki,:],dim=-1)\n","  lsm_proj  = F.log_softmax(out_proj.logits[0,toki,:],dim=-1)\n","\n","  # logit difference from the current token logit calculated in the previous token position\n","  logitdiffs[toki+1] = lsm_clean[tokens[0,toki+1]] - lsm_proj[tokens[0,toki+1]]\n","\n","\n","\n","# need a normalization to map negative values onto blue\n","norm = mpl.colors.TwoSlopeNorm(vmin=logitdiffs.min(),vcenter=0,vmax=logitdiffs.max())\n","\n","# and visualize\n","for toki in range(1,len(tokens[0])):\n","  plt.plot([toki,toki],[logitdiffs.min(),logitdiffs[toki]],':',color=mpl.cm.coolwarm(norm(logitdiffs[toki])),linewidth=.7)\n","  plt.plot(toki,logitdiffs[toki],'kh',markersize=10,markerfacecolor=mpl.cm.coolwarm(norm(logitdiffs[toki])))\n","\n","plt.axhline(0,color='gray',zorder=-100)\n","\n","xtickskip = 1 # skipping tokens in x-ticks\n","plt.gca().set(ylabel='$\\Delta$ logit from clean',xlim=[-1,len(tokens[0])],\n","              title='Change in token logit from subspace removal',\n","              xticks=range(0,len(tokens[0]),xtickskip),xticklabels=[tokenizer.decode(i) for i in tokens[0,range(0,len(tokens[0]),xtickskip)]])\n","plt.xticks(rotation=90)\n","\n","plt.show()"],"metadata":{"id":"a6fXmSnWlovF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"R5bhjbhrNwNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get width of one letter\n","fig,ax = plt.subplots(figsize=(10,2))\n","temp_text = ax.text(0,0,'n',fontsize=12,fontfamily='monospace')\n","bbox = temp_text.get_window_extent(renderer=fig.canvas.get_renderer())\n","inv = ax.transAxes.inverted()\n","bbox_axes = inv.transform([[bbox.x0,bbox.y0], [bbox.x1,bbox.y1]])\n","en_width = bbox_axes[1,0] - bbox_axes[0,0]\n","plt.close(fig)"],"metadata":{"id":"SNgNdhv5fupO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokCount = 0\n","\n","x_pos = 0  # starting x position (in axis coordinates)\n","y_pos = 1  # vertical center\n","\n","fig, ax = plt.subplots(figsize=(10,2))\n","ax.axis('off')\n","\n","for toki in range(len(tokens[0])):\n","\n","  # text of this token\n","  toktext = tokenizer.decode([tokens[0,toki]])\n","\n","  # width of the token\n","  token_width = en_width*len(toktext)\n","\n","  # text object with background color matching the \"activation\"\n","  ax.text(x_pos+token_width/2, y_pos, toktext, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","          bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.coolwarm(norm(logitdiffs[toki])), edgecolor='none', alpha=.8))\n","\n","  # update the token counter and x_pos\n","  tokCount += 1\n","  x_pos += token_width + .01 # plus a small gap\n","\n","  # end of the line; reset coordinates and counter\n","  if tokCount>=20:\n","    y_pos -= .17\n","    x_pos = 0\n","    tokCount = 0"],"metadata":{"id":"ECipG-4jhftd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MQ7yZWMnSI1L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Repeat over all layers"],"metadata":{"id":"VyPLXhwNSIyr"}},{"cell_type":"code","source":["data = np.zeros((nlayers,2))\n","\n","# loop over layers\n","for layeri in range(nlayers):\n","\n","  # create the hook\n","  def hook(module, input, output):\n","\n","    # SVD and reconstruct without top dimension\n","    U,s,Vt = torch.linalg.svd(output[0],full_matrices=False)\n","    S = torch.diag(s)\n","    data[layeri,1] = 100*s[0]**2 / (s**2).sum() # should be squared; this was a typo in the video\n","    S[0,0] = 0\n","    proj = U@S@Vt\n","\n","    # add back the mean offset\n","    proj += output[0].mean(dim=-1,keepdims=True)\n","\n","    # return the data\n","    return proj\n","  # implant the hook\n","  handle = model.transformer.h[layeri].mlp.c_fc.register_forward_hook(hook)\n","\n","  # forward pass and remove the hook\n","  with torch.no_grad(): out_proj = model(tokens)\n","  handle.remove()\n","\n","  # get output logits\n","  logitdiffs = np.zeros(len(tokens[0]))\n","\n","  for toki in range(len(tokens[0])-1):\n","    lsm_clean = F.log_softmax(out_clean.logits[0,toki,:],dim=-1)\n","    lsm_proj  = F.log_softmax(out_proj.logits[0,toki,:],dim=-1)\n","    logitdiffs[toki+1] = lsm_clean[tokens[0,toki+1]] - lsm_proj[tokens[0,toki+1]]\n","\n","  # and a t-test on all tokens against zero\n","  data[layeri,0] = stats.ttest_1samp(logitdiffs,0).statistic"],"metadata":{"id":"bsqAHwYySIwP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","for i in range(nlayers):\n","  axs[0].plot(i,data[i,0],'ko',markerfacecolor=mpl.cm.plasma(i/nlayers),markersize=10,alpha=.7)\n","  axs[1].plot(i,data[i,1],'ks',markerfacecolor=mpl.cm.plasma(i/nlayers),markersize=10,alpha=.7)\n","  axs[2].plot(data[i,0],data[i,1],'kh',markerfacecolor=mpl.cm.plasma(i/nlayers),markersize=10,alpha=.7)\n","\n","# glow-ups\n","axs[0].axhline(0,linestyle='--',color='gray',linewidth=.8,zorder=-109)\n","axs[0].set(xlabel='Transformer block',ylabel='T-statistic',title='T-test over tokens against zero')\n","axs[1].set(xlabel='Transformer block',ylabel='% variance',title='Variance explained by top component')\n","axs[2].set(xlabel='Logit difference norm',ylabel='% variance explained',title=f'r = {np.corrcoef(data.T)[1,0]:.2f}')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"I0ESORc-SItb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5GzHdUNghf4O"},"execution_count":null,"outputs":[]}]}