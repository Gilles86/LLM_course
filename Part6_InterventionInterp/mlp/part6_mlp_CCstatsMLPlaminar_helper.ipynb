{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyO+xAaEfwRsePQ1SByCXH5i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Modifying MLP<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Laminar profile of MLP t-lesions<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"dVdgcMR4IljT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuKeB769HOkN"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib.gridspec import GridSpec\n","\n","import scipy.stats as stats\n","from statsmodels.stats.multitest import fdrcorrection\n","\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"U7wBe4ucpj4T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: T-values for him vs. her in MLP neurons"],"metadata":{"id":"5wweSFEHpj1U"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForMaskedLM\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# load large BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n","\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"83A8PoVGRFj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some convenience variables\n","nneurons =\n","nlayers =\n","\n","nneurons, model.bert.encoder.layer[4].intermediate"],"metadata":{"id":"jqTjLQ3NrqHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data to use\n","sentences = [\n","    \"I saw him at the market.\",\n","    \"She gave him the book.\",\n","    \"They asked him for advice.\",\n","    \"We invited him to dinner.\",\n","    \"The dog followed him home.\",\n","    \"They asked him to join.\",\n","    \"He saw him at the park yesterday.\",\n","    \"Did you give him your address?\",\n","    \"I haven't seen him in ages.\",\n","    \"I told him the truth.\",\n","    \"They congratulated him on his success.\",\n","    \"She recognized him immediately.\",\n","    \"The teacher praised him for his work.\",\n","    \"I met him last summer.\",\n","    \"The child hugged him tightly.\",\n","    \"They warned him about the danger.\",\n","    \"She drove him to the airport.\",\n","    \"We waited for him for hours.\",\n","    \"The cat scratched him accidentally.\",\n","    \"They surprised him with a gift.\",\n","    \"She called him on the phone.\",\n","    \"The jury found him not guilty.\",\n","    \"I remembered him from school.\",\n","    \"They elected him as president.\",\n","    \"She forgave him for his mistake.\",\n","    \"The police questioned him yesterday.\",\n","    \"I helped him with his homework.\",\n","    \"They spotted him in the crowd.\",\n","    \"She visited him in the hospital.\",\n","    \"The manager promoted him last week.\",\n","    \"I trusted him completely.\",\n","    \"They respected him for his honesty.\",\n","    \"She taught him how to swim.\",\n","    \"The bird attacked him suddenly.\",\n","    \"I greeted him warmly.\",\n","    \"They supported him through difficult times.\",\n","    \"She ignored him at the party.\",\n","    \"The judge sentenced him to community service.\",\n","    \"I photographed him during the event.\",\n","    \"They believed him despite the evidence.\",\n","    \"She surprised him on his birthday.\",\n","    \"The guard stopped him at the entrance.\",\n","    \"I missed him terribly.\",\n","    \"They watched him leave the building.\",\n","    \"She accompanied him to the concert.\",\n","    \"The crowd cheered him enthusiastically.\",\n","    \"I described him to the police.\",\n","    \"They thanked him for his help.\",\n","    \"She admired him for his courage.\",\n","    \"The committee nominated him for the award.\",\n","    \"I married him last spring.\",\n","    \"They informed him about the changes.\",\n","    \"She introduced him to the parents.\",\n","    \"The author based the character on him.\",\n","\n","## same sentences but with \"her\"\n","\n","    \"I saw her at the market.\",\n","    \"She gave her the book.\",\n","    \"They asked her for advice.\",\n","    \"We invited her to dinner.\",\n","    \"The dog followed her home.\",\n","    \"They asked her to join.\",\n","    \"He saw her at the park yesterday.\",\n","    \"Did you give her your address?\",\n","    \"I haven't seen her in ages.\",\n","    \"I told her the truth.\",\n","    \"They congratulated her on his success.\",\n","    \"She recognized her immediately.\",\n","    \"The teacher praised her for his work.\",\n","    \"I met her last summer.\",\n","    \"The child hugged her tightly.\",\n","    \"They warned her about the danger.\",\n","    \"She drove her to the airport.\",\n","    \"We waited for her for hours.\",\n","    \"The cat scratched her accidentally.\",\n","    \"They surprised her with a gift.\",\n","    \"She called her on the phone.\",\n","    \"The jury found her not guilty.\",\n","    \"I remembered her from school.\",\n","    \"They elected her as president.\",\n","    \"She forgave her for his mistake.\",\n","    \"The police questioned her yesterday.\",\n","    \"I helped her with his homework.\",\n","    \"They spotted her in the crowd.\",\n","    \"She visited her in the hospital.\",\n","    \"The manager promoted her last week.\",\n","    \"I trusted her completely.\",\n","    \"They respected her for his honesty.\",\n","    \"She taught her how to swim.\",\n","    \"The bird attacked her suddenly.\",\n","    \"I greeted her warmly.\",\n","    \"They supported her through difficult times.\",\n","    \"She ignored her at the party.\",\n","    \"The judge sentenced her to community service.\",\n","    \"I photographed her during the event.\",\n","    \"They believed her despite the evidence.\",\n","    \"She surprised her on his birthday.\",\n","    \"The guard stopped her at the entrance.\",\n","    \"I missed her terribly.\",\n","    \"They watched her leave the building.\",\n","    \"She accompanied her to the concert.\",\n","    \"The crowd cheered her enthusiastically.\",\n","    \"I described her to the police.\",\n","    \"They thanked her for his help.\",\n","    \"She admired her for his courage.\",\n","    \"The committee nominated her for the award.\",\n","    \"I married her last spring.\",\n","    \"They informed her about the changes.\",\n","    \"She introduced her to the parents.\",\n","    \"The author based the character on her.\"\n","]\n","\n","# indices for him/her sentences\n","him_sentences = np.arange(len(sentences)//2)\n","her_sentences = np.arange(len(sentences)//2,len(sentences))\n","\n","print(f'There are {len(sentences)} sentences.')"],"metadata":{"id":"oqdAP54II-ht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# identify the target token\n","target_token_him =\n","target_token_her =\n","print(f'The target token indices are {target_token_him} and {target_token_her}\\n')\n","\n","# tokenize\n","tokens = tokenizer("],"metadata":{"id":"fYPkVigeOv47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare a vector of target indices per sentence as a torch tensor of integers\n","target_indices =\n","\n","# loop over sentences\n","for senti in range(len(sentences)):\n","  targBool =\n","  target_indices[senti] =\n","\n","target_indices"],"metadata":{"id":"-sR5Ex38IfzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DSPs9Skxp7P9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary to store the mlp t-test results\n","mlpTs = {}\n","\n","# hook function that runs a t-test on MLP activations\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # detach activations\n","    mlpVals =\n","\n","    # matrix of target activation values\n","    acts = np.zeros\n","    for senti in range(len\n","      acts[senti,:] = mlpVals[]\n","\n","    # t-test and find significance via FDR correction\n","    tres = stats.ttest_1samp(-,popmean=0,axis=0)\n","    issig =\n","\n","    # store the results\n","    mlpTs[f'L{layer_number}_him'] =\n","    mlpTs[f'L{layer_number}_her'] =\n","\n","  return hook\n","\n","\n","# implant into all layers\n","handles = []\n","for layeri in range(nlayers):\n","  .register_forward_hook(implant_hook(layeri))\n","  handles.append(h)"],"metadata":{"id":"pQ6zFv8PY4B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E1IwRC6lI-oM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Visualize t-test results"],"metadata":{"id":"3n8ZMz1ok-Uu"}},{"cell_type":"code","source":["# forward pass\n","\n","# remove handles\n"],"metadata":{"id":"YAMzPriHL7SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlpTs.keys(), mlpTs['L4_her'].shape"],"metadata":{"id":"7Zt_6zU9RWDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","\n","# draw the percentage of significant t-tests per layer\n","for i in range(nlayers):\n","  plt.plot(i,mlpTs[f'L{i}_her'] / ,'ko',markerfacecolor=[.9,.9,.7,.7],markersize=12)\n","  plt.plot(\n","\n","\n","# hacky solution to get legend\n","plt.plot(100,100*mlpTs[f'L{i}_her'].sum() / nneurons,'ko',markerfacecolor=[.9,.9,.7,.7],markersize=12,label='her > him')\n","plt.plot(100,100*mlpTs[f'L{i}_him'].sum() / nneurons,'ks',markerfacecolor=[.7,.7,.9,.7],markersize=12,label='him > her')\n","plt.legend()\n","\n","plt.gca().set(xlim=[-.5,nlayers-.5],xlabel='Layers',ylabel='% sig. t-values')\n","plt.show()"],"metadata":{"id":"g-tKTj3WM5iK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YEw3Ou0nWtOH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Confirm BERT’s predictions"],"metadata":{"id":"Fr48LXRAWtLE"}},{"cell_type":"code","source":["texts = [ 'Robert helped Lucy with her project, and she thanked him for his hard work.',\n","          'Robert helped Lucy with [MASK] project, and she thanked him for his hard work.',\n","          'Robert helped Lucy with her project, and she thanked [MASK] for his hard work.' ]\n","\n","# tokenize\n","testtokens = tokenizer(texts,return_tensors='pt').to(device)"],"metadata":{"id":"quAcxaQxhklO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find indices of [MASK]\n","mask_idx_her = torch.where(testtokens['input_ids'][1] ==\n","mask_idx_him =\n","\n","print(f'Masks are at indices {mask_idx_her} and {mask_idx_him}')"],"metadata":{"id":"rXaX6d6RAuft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  out =\n","\n","logits =\n","logits.shape"],"metadata":{"id":"RU38n2SnWtH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_logits = np.zeros((3,2,2))\n","\n","for senti in range(3):\n","  target_logits[senti,0,:] =\n","  target_logits[senti,1,:] =\n","\n","target_logits"],"metadata":{"id":"GZh5E5_tWtBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","for i in range(3):\n","  plt.bar(,width=.2,facecolor=[[.9,.7,.9],[.7,.9,.9]],edgecolor='k')\n","  plt.bar(,width=.2,facecolor=[[.9,.7,.9],[.7,.9,.9]],edgecolor='k')\n","\n","# create the bar labels\n","basetxt = 'her :: him     her :: him\\n----------------+----------------\\nher token   ||   him token'\n","xticklabels = [ basetxt + '\\n\\n|__________$\\\\bf{Clean}$_________|',\n","                basetxt + '\\n\\n|_______$\\\\bf{HER\\; mask}$_______|',\n","                basetxt + '\\n\\n|_______$\\\\bf{HIM\\; mask}$_______|' ]\n","\n","plt.gca().set(xticks=np.arange(.25,3.5,1.5),xticklabels=xticklabels,ylabel='Logits')\n","plt.show()"],"metadata":{"id":"QsCvUrpdpGyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CeYwsFsjWs-B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Laminar nullification of MLP \"him\" and \"her\" neurons"],"metadata":{"id":"Am4TmiarWs6T"}},{"cell_type":"code","source":["# results are (1) magnitude of modulation, (2) \"her\" impact, (3) \"him\" impact\n","results = np.zeros((nlayers,3))\n","\n","\n","# loop over layers\n","for layeri in range():\n","\n","  # patch this layer\n","  def mlp_ablate_hook(module, input, output):\n","    # zero-out the \"her neurons\" on the HER token, and the \"him neurons\" on the HIM token\n","    output[1,mask_idx_her,mlpTs[f'L{layeri}_her']] =\n","    output[2\n","    return output\n","  handle = model.bert.encoder.layer[layeri].intermediate.register_forward_hook(mlp_ablate_hook)\n","\n","  # forward pass to get output logits, and remove hook\n","  with torch.no_grad(): out=model(**testtokens)\n","  logitsZero = out....numpy()\n","  handle.remove()\n","\n","  # get the logits for the target tokens\n","  target_logitsZ = np.zeros((3,2,2))\n","\n","  for senti in range(3):\n","    target_logitsZ[senti,0,:] =\n","    target_logitsZ[senti,1,:] =\n","  deltaLogits = target_logits -  # difference between clean and ablation logits\n","\n","  # measure the total magnitude change\n","  results[layeri,0] =\n","\n","  # specific modulations\n","  results[layeri,1] =\n","  results[layeri,2] =\n"],"metadata":{"id":"9WCXkpgkB20b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","axs[0].plot(,'kh',markerfacecolor=[.9,.7,.7],markersize=12)\n","axs[0].set(xlabel='Transformer block',ylabel='Magnitude change in logits',\n","           ylim=[0,None],title='Overall impact of manipulation on target logits')\n","\n","axs[1].plot(,,'ks',markerfacecolor=[.9,.7,.9,.7],markersize=12,label='HER manipulation')\n","axs[1].plot(,,'ko',markerfacecolor=[.7,.9,.9,.7],markersize=12,label='HIM manipulation')\n","axs[1].set(xlabel='Transformer block',ylabel='Signed change in logits',title='Clean - ablated target $\\Delta$ logits')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"WWnYSA0nB2ve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xBncWPz9Ws0t"},"execution_count":null,"outputs":[]}]}