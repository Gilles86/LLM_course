{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNaTEmEWi+dhOJ4ohQpg8JG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Modifying MLP<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Successive median-replacement of MLP neurons<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"dVdgcMR4IljT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuKeB769HOkN"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2-large').to(device)\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n","\n","model.eval()"],"metadata":{"id":"83A8PoVGRFj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlayers = len(model.transformer.h)\n","nneurons = model.transformer.h[3].mlp.c_fc.weight.shape[-1]\n","nneurons, nlayers"],"metadata":{"id":"UE9iGVUWGRQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5jWDecMYNTzh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Forward pass and get activations"],"metadata":{"id":"BHcZCWR2I-lF"}},{"cell_type":"code","source":["# dictionary to store the mlp activations\n","mlp_values = {}\n","\n","def hook(module, input, output):\n","  mlp_values[f'L{whichlayer}'] = output[0].detach().cpu() # detach from the computational graph\n","\n","# surgery ;)\n","whichlayer = 9\n","handle = model.transformer.h[whichlayer].mlp.c_fc.register_forward_hook(hook)"],"metadata":{"id":"pQ6zFv8PY4B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'It was a dark and stormy'\n","target_idx = tokenizer.encode(' night')[0]\n","\n","# text = 'The cat sat on the'\n","# target_idx = tokenizer.encode(' mat')[0]\n","\n","tokens = tokenizer.encode(text,return_tensors='pt').to(device)\n","\n","tokens.shape, tokens, target_idx"],"metadata":{"id":"fYPkVigeOv47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  out_clean = model(tokens,output_hidden_states=True)\n","\n","# calculate softmax probability in percent\n","logsm_clean = F.log_softmax(out_clean.logits[0,-1,:],dim=-1).detach().cpu().numpy()\n","\n","handle.remove()\n","\n","mlp = mlp_values[f'L{whichlayer}']\n","mlp.shape"],"metadata":{"id":"YAMzPriHL7SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# note that these are pre-gelu\n","plt.figure(figsize=(8,4))\n","plt.hist(mlp.flatten(),np.linspace(-7,7,101),edgecolor='k',facecolor='gray')\n","\n","plt.gca().set(xlabel='Activation value',ylabel='Count (a.u.)',yticks=[],\n","              title=f'MLP activations from layer {whichlayer}')\n","plt.show()"],"metadata":{"id":"gKzYC8iCL1bH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# exploring median-based replacement\n","t = mlp[-1,:]\n","med = torch.median(t)\n","idx = torch.topk(t,int(.1*nneurons)).indices\n","trepl = t.clone()\n","trepl[idx] = med\n","\n","# show the two histograms\n","plt.figure(figsize=(10,4))\n","binedges = np.linspace(-6,3,41)\n","\n","# pre-replace histogram\n","y,x = np.histogram(t,binedges)\n","plt.plot(x[:-1]-.02,y,'ks-',markerfacecolor='w',label='Original')\n","\n","# post-replace histogram\n","y,x = np.histogram(trepl,binedges)\n","plt.plot(x[:-1]+.02,y,'bo-',markerfacecolor='w',label='Replaced')\n","\n","plt.gca().set(xlabel='Data value',ylabel='Count',xlim=binedges[[0,-1]],title='Impact of replacement')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"U3qoh11mFKNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","# all the log-sm values\n","plt.plot(logsm_clean,'k.',markersize=2,alpha=.3)\n","\n","# the target and nontarget values\n","plt.plot(target_idx,logsm_clean[target_idx],'gs',label=tokenizer.decode(target_idx))\n","\n","# make the graph look pretty :D\n","plt.gca().set(xlabel='Vocab elements',ylabel='Log softmax',xlim=[0,model.config.vocab_size])\n","plt.title(f'Predicted next token is \"{tokenizer.decode(np.argmax(logsm_clean))}\"',fontweight='bold')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"kac1tOloFKKd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dbO7kYSDFKHT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Replace up to 90% of MLP neurons per layer"],"metadata":{"id":"MQEiF2fuUfLZ"}},{"cell_type":"code","source":["log_sm = np.zeros((nlayers,9))\n","\n","# loop over layers\n","for layeri in range(nlayers):\n","\n","  for replrate in range(1,10):\n","\n","    # replace this layer\n","    def replace_hook(module, input, output):\n","\n","      # find the median\n","      vals = output[0,-1,:]\n","      med = torch.median(vals)\n","\n","      # replace top p%\n","      idx = torch.topk(vals,int((replrate/10)*nneurons)).indices\n","      output[0,-1,idx] = med\n","      return output\n","\n","    handle = model.transformer.h[layeri].mlp.c_fc.register_forward_hook(replace_hook)\n","\n","    # forward pass to get output logits, and remove hook\n","    with torch.no_grad(): out = model(tokens)\n","    handle.remove()\n","\n","    log_sm[layeri,replrate-1] = F.log_softmax(out.logits[0,-1,:].detach(),dim=-1)[target_idx]"],"metadata":{"id":"BCYIbD9_FKEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","for i in range(nlayers):\n","  plt.plot(np.random.randn(9)/20+i,logsm_clean[target_idx]-log_sm[i,:],'ko',alpha=.4,\n","           markerfacecolor=mpl.cm.plasma(i/nlayers),markersize=10)\n","\n","plt.axhline(0,color='k',linewidth=.5,linestyle='--',zorder=-13)\n","\n","plt.gca().set(xlabel='Transformer block',ylabel='Logit difference from clean',ylim=[-.1,.2])\n","plt.show()"],"metadata":{"id":"jzhS3Zx_FKBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qLYvnQ6IRjJQ"},"execution_count":null,"outputs":[]}]}