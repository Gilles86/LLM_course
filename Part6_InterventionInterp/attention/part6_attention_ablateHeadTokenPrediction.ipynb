{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMYowk6/F4AT6NQqefdoKWr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Interfering with attention <h1>|\n","|<h2>Lecture:</h2>|<h1><b>Head ablation and token prediction<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model.eval()"],"metadata":{"id":"m5QXl5irtjBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some useful variables\n","nheads = model.config.n_head\n","n_emb = model.config.n_embd\n","head_dim = model.config.n_embd // nheads"],"metadata":{"id":"HZ6DyuqKtkyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hook4attn(module,input):\n","\n","  # if out of range, do nothing\n","  if head2ablate in np.arange(nheads):\n","\n","    # reshape so we can index heads\n","    head_tensor = input[0].view(nbatches,ntokens,nheads,head_dim)\n","\n","    # then replace\n","    head_tensor[:,:,head2ablate,:] = 0\n","    print(f'Zeroed out H{head2ablate}')\n","\n","    # reshape back to tensor\n","    head_tensor = head_tensor.view(nbatches,ntokens,n_emb)\n","\n","    # return a tuple matching the original\n","    input = (head_tensor,*input[1:])\n","\n","  return input\n","\n","model.transformer.h[5].attn.c_proj.register_forward_pre_hook(hook4attn)"],"metadata":{"id":"Iy0kVnQztkvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = tokenizer.encode('Berlin is the capital of',return_tensors='pt')\n","nbatches,ntokens = tokens.shape\n","\n","for i in range(ntokens):\n","  print(f'Token position {i:2} is index {tokens[0,i]} and is \"{tokenizer.decode(tokens[0,i])}\"')"],"metadata":{"id":"qvvEHGz9tksP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# target and semantically related nontarget\n","nontarget_idx = tokenizer.encode(' France')[0]\n","target_idx = tokenizer.encode(' Germany')[0]\n","\n","# confirm single-tokens\n","nontarget_idx,target_idx"],"metadata":{"id":"yBcAXDnWEKpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"czPntE-oES19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize to invalid index\n","head2ablate = 100000\n","\n","with torch.no_grad():\n","  out = model(tokens)\n","\n","# calculate softmax probability in percent\n","logsm_clean = F.log_softmax(out.logits[0,-1,:],dim=-1).detach().numpy()"],"metadata":{"id":"aktYlZw3wrp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","# all the log-sm values\n","plt.plot(logsm_clean,'k.',markersize=2,alpha=.3)\n","\n","# the target and nontarget values\n","plt.plot(target_idx,logsm_clean[target_idx],'gs',label='Germany')\n","plt.plot(nontarget_idx,logsm_clean[nontarget_idx],'ro',label='France')\n","\n","# make the graph look pretty :D\n","plt.gca().set(xlabel='Vocab elements',ylabel='Log softmax',xlim=[0,model.config.vocab_size])\n","plt.title(f'Predicted next token is \"{tokenizer.decode(np.argmax(logsm_clean))}\"',fontweight='bold')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"vlIIvcC7tkpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5uGtLMFmtklw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Zero-out each attention head"],"metadata":{"id":"t2RPBEtECytS"}},{"cell_type":"code","source":["resultsZero = np.zeros((nheads,3))\n","\n","# loop over heads\n","for head2ablate in range(nheads):\n","\n","  # forward pass\n","  with torch.no_grad():\n","    out = model(tokens)\n","\n","  # softmax\n","  logsm = F.log_softmax(out.logits[0,-1,:],dim=-1).detach().numpy()\n","\n","  # sm logits for target and nontarget\n","  resultsZero[head2ablate,0] = logsm[target_idx]\n","  resultsZero[head2ablate,1] = logsm[nontarget_idx]\n","\n","  # and the predicted next token\n","  resultsZero[head2ablate,2] = np.argmax(logsm)"],"metadata":{"id":"l07sNt7htkip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","axs[0].bar(range(model.config.n_layer),resultsZero[:,0] - logsm_clean[target_idx],color=[.7,.7,.9],edgecolor='k')\n","axs[0].axhline(0,color='gray')\n","axs[0].set(xlabel='Head',ylabel='$\\Delta$ logit from clean',ylim=[-.4,.4],\n","           xticks=range(nheads),title='$\\Delta$ in log-prob. for target word')\n","\n","axs[1].bar(range(model.config.n_layer),resultsZero[:,1] - logsm_clean[nontarget_idx],color=[.9,.7,.7],edgecolor='k')\n","axs[1].axhline(0,color='gray')\n","axs[1].set(xlabel='Head',ylabel='$\\Delta$ logit from clean',ylim=[-.4,.4],\n","           xticks=range(nheads),title='$\\Delta$ in log-prob. for non-target word')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"ojMdlVejOGQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the token selection\n","for i in range(nheads):\n","  print(f'When ablating head {i:2}, the predicted new token was \"{tokenizer.decode(int(resultsZero[i,2]))}\"')"],"metadata":{"id":"voQIYDffI76s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"llSLlyGw1EAx"},"execution_count":null,"outputs":[]}]}