{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMX3jKnlcnoGgmIUHgBURxP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Interfering with attention <h1>|\n","|<h2>Lecture:</h2>|<h1><b>Impact of head-silencing on cosine similarity<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import scipy.stats as stats\n","\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model.eval()"],"metadata":{"id":"m5QXl5irtjBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some useful variables\n","nheads = model.config.n_head\n","nlayers = model.config.n_layer\n","n_emb = model.config.n_embd\n","head_dim = model.config.n_embd // nheads"],"metadata":{"id":"HZ6DyuqKtkyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def implant_hook(layer_number):\n","  def hook4attn(module,input):\n","\n","    # modify the activation only for this layer\n","    if layer_number==layer2silence:\n","\n","      # reshape so we can index heads\n","      head_tensor = input[0].view(nbatches,ntokens,nheads,head_dim)\n","\n","      # ablate\n","      head_tensor[:,:,head2silence,:] = 0\n","\n","      # reshape back to tensor\n","      head_tensor = head_tensor.view(nbatches,ntokens,n_emb)\n","\n","      # return a tuple matching the original\n","      input = (head_tensor,*input[1:])\n","\n","    return input\n","  return hook4attn\n","\n","\n","handles = []\n","for layeri in range(nlayers):\n","  h = model.transformer.h[layeri].attn.c_proj.register_forward_pre_hook(implant_hook(layeri))\n","  handles.append(h)"],"metadata":{"id":"Iy0kVnQztkvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NC_Ro-WZuWln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokens (my apologies to the Italians :P )"],"metadata":{"id":"BwB6ltw-uWic"}},{"cell_type":"code","source":["tokens = tokenizer.encode('Peanut butter and pineapple taste great on pizza',return_tensors='pt')\n","nbatches,ntokens = tokens.shape\n","\n","for i in range(ntokens):\n","  print(f'Token position {i} is index {tokens[0,i]:6} and is \"{tokenizer.decode(tokens[0,i])}\"')"],"metadata":{"id":"qvvEHGz9tksP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yBcAXDnWEKpF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cosine similarities in an unmodulated model"],"metadata":{"id":"kT3vXPD2uazW"}},{"cell_type":"code","source":["layer2silence = 1000\n","head2silence = 1000\n","\n","with torch.no_grad():\n","  out_clean = model(tokens,output_hidden_states=True)\n","\n","# check hidden states sizes\n","hs_clean = out_clean.hidden_states\n","hs_clean[4].shape"],"metadata":{"id":"aktYlZw3wrp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vU2sc-FfutEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get cossim from all layers\n","\n","# how many unique tokens are in the CS matrix\n","uniqueCScount = int((ntokens-1)*(ntokens-2)/2)\n","\n","cs_clean = np.zeros((nlayers,uniqueCScount))\n","\n","for layeri in range(nlayers):\n","\n","  # cosine similarities\n","  H = out_clean.hidden_states[layeri+1][0,1:,:]\n","  H = H / torch.linalg.vector_norm(H,axis=1,keepdims=True)\n","  csMat = H @ H.T\n","\n","  # and extract the unique elements\n","  cs = torch.triu(csMat,1).flatten()\n","  cs_clean[layeri,:] = cs[cs!=0].numpy()\n"],"metadata":{"id":"Xj2-jTNaqnlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Pqq-OtirqniW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Disrupt one head and measure impact"],"metadata":{"id":"RhMENs_MhZP_"}},{"cell_type":"code","source":["layer2silence = 5\n","head2silence = 1\n","\n","with torch.no_grad():\n","  out_silence = model(tokens,output_hidden_states=True)"],"metadata":{"id":"0WAdStEthZLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cosine similarities\n","H = out_silence.hidden_states[layer2silence+1][0,1:,:]\n","H = H / torch.linalg.vector_norm(H,axis=1,keepdims=True)\n","csMat = H @ H.T\n","\n","# and extract the unique elements\n","cs_silence = torch.triu(csMat,1).flatten()\n","cs_silence = cs_silence[cs_silence!=0]\n","cs_silence"],"metadata":{"id":"Y3fkjIRFbRjy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lAKtt1EtwgHA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Manipulate and measure from the same layer"],"metadata":{"id":"3JRvSqaMnhLz"}},{"cell_type":"code","source":["cs_manip = np.zeros((nlayers,uniqueCScount))\n","\n","for layer2silence in range(nlayers):\n","\n","  with torch.no_grad():\n","    out_manip = model(tokens,output_hidden_states=True)\n","\n","  # cosine similarities\n","  H = out_manip.hidden_states[layer2silence+1][0,1:,:]\n","  H = H / torch.linalg.vector_norm(H,axis=1,keepdims=True)\n","  csMat = H @ H.T\n","\n","  # and extract the unique elements\n","  cs = torch.triu(csMat,1).flatten()\n","  cs_manip[layer2silence,:] = cs[cs!=0].numpy()\n"],"metadata":{"id":"R27pNWJebRg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","for i in range(nlayers):\n","\n","  # plot the raw cossim values\n","  axs[0].plot(np.zeros(uniqueCScount)+i,cs_clean[i,:],'ks',markersize=4,alpha=.5,markerfacecolor=mpl.cm.plasma(i/nlayers))\n","  axs[1].plot(np.zeros(uniqueCScount)+i,cs_manip[i,:],'ko',markersize=4,alpha=.5,markerfacecolor=mpl.cm.plasma(i/nlayers))\n","\n","  # test for a statistical difference\n","  tres = stats.ttest_rel(cs_manip[i,:],cs_clean[i,:])\n","\n","  # plot marker depends on significance\n","  if tres.pvalue<.05:\n","    axs[2].plot(np.zeros(uniqueCScount)+i,cs_manip[i,:]-cs_clean[i,:],'k^',markersize=4,alpha=.5,markerfacecolor=mpl.cm.plasma(i/nlayers))\n","  else:\n","    axs[2].plot(np.zeros(uniqueCScount)+i,cs_manip[i,:]-cs_clean[i,:],'rx',markersize=4,alpha=.5)\n","\n","\n","# some embellishments etc\n","axs[0].set(xlabel='Transformer block',ylabel='Cosine similarity',title='Clean model')\n","axs[1].set(xlabel='Transformer block',ylabel='Cosine similarity',title='Layer-specific silencing')\n","axs[2].set(xlabel='Transformer block',ylabel='$\\Delta S_c$',title='Silenced - clean difference')\n","axs[2].axhline(0,color='k',zorder=-30,linewidth=.5)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"blKAMVApbRdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Edsgn_3tbRax"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Manipulate one layer and measure downstream impact"],"metadata":{"id":"q2quL_0RnkdH"}},{"cell_type":"code","source":["layer2silence = 3\n","\n","# forward pass\n","with torch.no_grad():\n","  out_silence = model(tokens,output_hidden_states=True)\n","\n","# no more forward passes or silencing; just measure cs\n","cs_manip2 = np.zeros((nlayers,uniqueCScount))\n","for layeri in range(nlayers):\n","\n","  # cosine similarities\n","  H = out_silence.hidden_states[layeri+1][0,1:,:]\n","  H = H / torch.linalg.vector_norm(H,axis=1,keepdims=True)\n","  csMat = H @ H.T\n","\n","  # and extract the unique elements\n","  cs_silence = torch.triu(csMat,1).flatten()\n","  cs_manip2[layeri,:] = cs_silence[cs_silence!=0].numpy()\n"],"metadata":{"id":"5nrJVkpHnkaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","for i in range(nlayers):\n","\n","  # plot the raw cossim values\n","  axs[0].plot(np.zeros(uniqueCScount)+i,cs_clean[i,:],'ks',markersize=4,alpha=.5,markerfacecolor=mpl.cm.plasma(i/nlayers))\n","  axs[1].plot(np.zeros(uniqueCScount)+i,cs_manip2[i,:],'ko',markersize=4,alpha=.5,markerfacecolor=mpl.cm.plasma(i/nlayers))\n","\n","  # test for a statistical difference\n","  tres = stats.ttest_1samp(cs_manip2[i,:]-cs_clean[i,:],0)\n","\n","  # plot marker depends on significance\n","  if tres.pvalue<.05:\n","    axs[2].plot(np.zeros(uniqueCScount)+i,cs_manip2[i,:]-cs_clean[i,:],'k^',markersize=4,alpha=.5,markerfacecolor=mpl.cm.plasma(i/nlayers))\n","  else:\n","    axs[2].plot(np.zeros(uniqueCScount)+i,cs_manip2[i,:]-cs_clean[i,:],'rx',markersize=4)\n","\n","\n","# some embellishments etc\n","axs[0].set(xlabel='Transformer block',ylabel='Cosine similarity',title='Clean model')\n","axs[1].set(xlabel='Transformer block',ylabel='Cosine similarity',title=f'Silenced layer {layer2silence}')\n","axs[2].set(xlabel='Transformer block',ylabel='$\\Delta S_c$',title='Silenced - clean difference')\n","axs[2].axhline(0,color='k',zorder=-30,linewidth=.5)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"26nS_FCTnkXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jnY3w_UsnkSo"},"execution_count":null,"outputs":[]}]}