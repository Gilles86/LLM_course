{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMjpGLU2tKGjqvLWpx390oN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Interfering with attention <h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Token prediction after head ablations<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib.gridspec import GridSpec\n","\n","from tqdm import tqdm\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"PPLDxIujCnTK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Model, hook, tokens"],"metadata":{"id":"S_JBf3BvCnQS"}},{"cell_type":"code","source":["model = GPT2LMHeadModel.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"m5QXl5irtjBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some useful variables\n","nheads =\n","n_emb =\n","head_dim ="],"metadata":{"id":"HZ6DyuqKtkyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def implant_hook(layer_number):\n","  def hook4attn(module,input):\n","\n","    # print some useful information\n","    # print(len(input),type(input),input[0].shape)\n","\n","    # modify the activation only for this layer\n","    if\n","\n","      # reshape so we can index heads\n","      head_tensor = input\n","\n","      # specify the value to replace\n","      if\n","        value2replace = 0\n","      else:\n","\n","\n","      # then replace\n","      head_tensor\n","\n","      # print confirmation\n","      # print(f'Zeroed out L{layer_number}, H{head2ablate}')\n","\n","      # reshape back to tensor\n","      head_tensor =\n","\n","      # return a tuple matching the original\n","      input =\n","\n","    return input\n","  return hook4attn\n","\n","\n","handles = []\n","for layeri in range(model.config.n_layer):\n","  register_forward_pre_hook(implant_hook(layeri))\n","  .append(h)"],"metadata":{"id":"Iy0kVnQztkvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = tokenizer.encode('Berlin is the capital of',return_tensors='pt')\n","nbatches,ntokens =\n","\n","\n","for i in range(ntokens):\n","  print(f'Token position {i:2} is index {} and is \"{}\"')"],"metadata":{"id":"qvvEHGz9tksP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# target and semantically related nontarget\n","nontarget_idx =\n","target_idx =\n","\n","# confirm single-tokens\n","nontarget_idx,target_idx"],"metadata":{"id":"yBcAXDnWEKpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"czPntE-oES19"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Confirm accuracy and get clean logits"],"metadata":{"id":"vO-xoXMqESzZ"}},{"cell_type":"code","source":["layer2ablate =\n","head2ablate =\n","\n","# forward pass\n","\n","# calculate softmax probability in percent\n","sm_clean ="],"metadata":{"id":"aktYlZw3wrp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","# all the log-sm values\n","plt.plot(,'k.',markersize=2,alpha=.3)\n","\n","# the target and nontarget values\n","plt.plot(target_idx,,'gs',label='Germany')\n","plt.plot(nontarget_idx,,'ro',label='France')\n","\n","# make the graph look pretty :D\n","plt.gca().set(xlabel='Vocab elements',ylabel='Log softmax',xlim=[0,model.config.vocab_size])\n","plt.title(f'Predicted next token is \"{}\"')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"vlIIvcC7tkpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5uGtLMFmtklw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Zero-out attention heads for all token indices"],"metadata":{"id":"t2RPBEtECytS"}},{"cell_type":"code","source":["replaceWithZeros = True"],"metadata":{"id":"PmBRWsW-HTbx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resultsZero = np.zeros\n","\n","# loop over layers and heads\n","for layer2ablate in tqdm\n","  for head2ablate in\n","\n","    # forward pass\n","\n","\n","    # softmax\n","    sm =\n","\n","    # sm logits for target and nontarget\n","    resultsZero[layer2ablate,head2ablate,0] = sm\n","    resultsZero[layer2ablate,head2ablate,1] = sm\n","\n","    # and the predicted next token\n","    resultsZero[layer2ablate,head2ablate,2] ="],"metadata":{"id":"l07sNt7htkip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","clim = 5\n","\n","h = axs[0].imshow(,vmin=-clim,vmax=clim,cmap=mpl.cm.plasma,aspect='auto')\n","axs[0].set(xlabel='Layer',ylabel='Head',yticks=range(0,nheads,2),title='%$\\Delta$ in prob. for target word')\n","fig.colorbar(h,ax=axs[0],pad=.01)\n","\n","axs[1].set(xlabel='Layer',ylabel='Head',yticks=range(0,nheads,2),title='%$\\Delta$ in prob. for non-target word')\n","fig.colorbar(h,ax=axs[1],pad=.01)\n","\n","plt.suptitle('Change in token selection probability from clean model',fontweight='bold')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"ojMdlVejOGQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["u,c = np.unique(resultsZero[:,:,2],)\n"],"metadata":{"id":"voQIYDffI76s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"llSLlyGw1EAx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Repeat with head mean imputation"],"metadata":{"id":"Zka0lkRdIm67"}},{"cell_type":"code","source":[],"metadata":{"id":"hEFRzP9wIxTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resultsMean = np.zeros((model.config.n_layer,nheads,4))\n","\n","# loop over layers and heads\n","for layer2ablate in tqdm(range(model.config.n_layer),desc='Layers...'):\n","  for head2ablate in range(nheads):\n","\n","    # forward pass\n","    with torch.no_grad():\n","      out = model(tokens)\n","\n","    # log-softmax\n","    sm =\n","\n","    # log-sm logits for target and nontarget\n","\n","\n","    # the empirical mean value that was imputed\n","\n","\n","    # and the predicted next token\n","\n"],"metadata":{"id":"pLeOW9bDItDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the figure"],"metadata":{"id":"1HB5WVsvItDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the unique values and their counts"],"metadata":{"id":"hosww27_1D73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# head-averaged activations\n","fig,axs = plt.subplots(1,2,figsize=(12,5))\n","\n","axs[0].plot(,'ko',markerfacecolor=[.9,.7,.7,.6])\n","axs[0].set(xlabel='Heads $\\\\times$ layer (index)',ylabel='Head mean',title='As scatter plot')\n","\n","h = axs[1].imshow(,vmin=-.05,vmax=.05,cmap=mpl.cm.plasma,aspect='auto')\n","axs[1].set(xlabel='Layer',ylabel='Head',yticks=range(0,nheads,2),title='As image')\n","fig.colorbar(h,ax=axs[1],pad=.02,fraction=.05)\n","\n","plt.suptitle('Head activation averages',fontweight='bold')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"q_-6y5jS1Dq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X2IV-v341D47"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Comparisons"],"metadata":{"id":"73cmCZ3oPocv"}},{"cell_type":"code","source":["# setup the figure\n","fig = plt.figure(figsize=(13,4))\n","gs  = GridSpec(1,3,figure=fig)\n","axs = [ fig.add_subplot(gs[:2]) , fig.add_subplot(gs[-1]) ]\n","\n","\n","### histograms\n","nbins = 20\n","\n","y,x = np.histogram(resultsZero - sm_clean\n","axs[0].plot(x[:-1],y,'.-',linewidth=2,markersize=10,label='Zero target')\n","\n","y,x = np.histogram(\n","axs[0].plot(x[:-1],y,'.-',linewidth=2,markersize=10,label='Mean target')\n","\n","y,x = np.histogram(\n","axs[0].plot(x[:-1],y,linewidth=2,label='Zero nontarget')\n","\n","y,x = np.histogram(\n","axs[0].plot(x[:-1],y,linewidth=2,label='Mean nontarget')\n","\n","axs[0].set(xlabel='Token probability ($\\Delta$ from clean model)',ylabel='Count',ylim=[-1,None],\n","           title='Histograms of $\\Delta$ softmax')\n","axs[0].legend(fontsize=15)\n","\n","\n","# difference heat map\n","h = axs[1].imshow( ,vmin=-1,vmax=1,cmap=mpl.cm.plasma,aspect='auto')\n","axs[1].set(xlabel='Layer',ylabel='Head',yticks=range(0,nheads,2),title='$\\Delta$ target: (mean - zero)')\n","fig.colorbar(h,ax=axs[1],pad=.02,fraction=.05)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"5nHg26111D2U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3J-RMtP-yXzE"},"execution_count":null,"outputs":[]}]}