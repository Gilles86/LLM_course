{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMd0hkRP/rWoYD0ZdjXr2+H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Interfering with attention <h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Does GPT2 like pineapple pizza?<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import scipy.stats as stats\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"UWuNrl2obTi9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Model, hooks, tokens"],"metadata":{"id":"wlspGaaIbTgV"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"PPLDxIujCnTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"m5QXl5irtjBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some useful variables\n","nheads\n","nlayers\n","n_emb\n","head_dim"],"metadata":{"id":"HZ6DyuqKtkyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hook_silencer(layer_number):\n","  def hook2zero(module,input):\n","\n","    # modify the activation only for this layer\n","    if\n","\n","      # reshape so we can index heads\n","      head_tensor =\n","\n","      # ablate\n","      head_tensor\n","\n","      # reshape back to tensor\n","      head_tensor =\n","\n","      # return a tuple matching the original\n","      input = (head_tensor,*input[1:])\n","\n","    return\n","  return hook2zero\n","\n","\n"],"metadata":{"id":"Iy0kVnQztkvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NC_Ro-WZuWln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokens (my apologies to the Italians :P )"],"metadata":{"id":"BwB6ltw-uWic"}},{"cell_type":"code","source":["tokens = tokenizer.encode('Peanut butter and pineapple taste great on pizza',return_tensors='pt').to(device)\n","nbatches,ntokens = tokens.shape\n","\n","for i in range(ntokens):\n","  print"],"metadata":{"id":"qvvEHGz9tksP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["butter_idx =\n","pineap_idx =\n","pizza_idx  =\n","\n","butter_idx,pineap_idx,pizza_idx"],"metadata":{"id":"xoyhs7yqJNYi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yBcAXDnWEKpF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Does GPT2 prefer peanut butter or pineapple on pizza?"],"metadata":{"id":"kT3vXPD2uazW"}},{"cell_type":"code","source":["layer2silence =\n","head2silence =\n","\n","with torch.no_grad():\n","\n","\n","# check hidden states sizes\n"],"metadata":{"id":"aktYlZw3wrp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get cossim from all layers\n","cs_clean = np.zeros((nlayers,2))\n","\n","# loop over layers\n","for layeri in range(nlayers):\n","\n","  # cosine similarities\n","  cs_clean[layeri,0] = F.cosine_similarity(\n","  cs_clean[layeri,1] = F.cosine_similarity("],"metadata":{"id":"Xj2-jTNaqnlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,5))\n","\n","plt.plot(\n","plt.plot(\n","\n","plt.gca().set(xlabel='Transformer block',ylabel='Cosine similarity',\n","              title='Cosine similarity to \" pizza\"')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Pqq-OtirqniW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"L8ItpunmPHBy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Can we disrupt GPT's pizza preference?"],"metadata":{"id":"lAKtt1EtwgHA"}},{"cell_type":"code","source":["token2silence =\n","head2silence =\n","\n","cs_manip = np.zeros((nlayers,2))\n","\n","for layer2silence\n","\n","  with torch.no_grad():\n","    out_manip =\n","\n","  # cosine similarities\n","  cs_manip[layer2silence,0] = F.cosine_similarity\n","  cs_manip[layer2silence,1] = F.cosine_similarity"],"metadata":{"id":"JqmbLrXNPEKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","axs[0].plot(np.arange(nlayers)-.1,cs_manip[:,0],'ko',markerfacecolor=[.9,.7,.7,.6],markersize=10,label='Peanut butter')\n","\n","axs[1].plot(np.arange(nlayers)-.1,cs_manip[:,0]-cs_clean[:,0],'ko',markerfacecolor=[.9,.7,.7,.6],markersize=10,label='Peanut butter')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Rt_GGz0uPEG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aAMTY0BrPEEZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Layer-specific silencing"],"metadata":{"id":"cYl81MUPbcYc"}},{"cell_type":"code","source":["# redefined\n","token2silence =\n","\n","# initializes results matrix\n","cs_manip = np.zeros\n","\n","# loop over layers and heads\n","for layer2silence in tqdm(:\n","  for head2silence in\n","\n","    # forward pass\n","\n","\n","    # cosine similarities\n","    cs_manip[layer2silence,head2silence,0] = F.cosine_similarity\n","    cs_manip[layer2silence,head2silence,1] = F.cosine_similarity"],"metadata":{"id":"Jpq_J22lPD_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","for i in range(nlayers):\n","  axs[0].plot(\n","  axs[0].plot(\n","\n","  # run a ttest\n","  tres = stats.ttest_1samp\n","\n","\n","\n","axs[0].set(xlabel='Transformer block',ylabel='Cosine similarity',title='Cosine similarity in clean and silenced models')\n","\n","axs[1].set(xlabel='Transformer block',ylabel='$\\Delta$ cosine similarity',title='Silenced - clean')\n","axs[1].axhline(0,color='k',linewidth=.8,zorder=-10)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"FnJM6R7EUJK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C2XvYIUNUJIL"},"execution_count":null,"outputs":[]}]}