{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyP978PV+6ao0dEssladAevg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Editing hidden states<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Activation patching with indirect object identification<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"vEQLNzgMunaD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"hTWsybWmZJe-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import the model and create tokens"],"metadata":{"id":"ny-nH3gniySB"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","n_layers = model.config.n_layer\n","model.eval()"],"metadata":{"id":"pFyxDs3UaDiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_ME = 'When Mike and Emma went to the cafe, Mike gave a coffee to'\n","text_EM = 'When Mike and Emma went to the cafe, Emma gave a coffee to'\n","\n","target_M = tokenizer.encode(' Mike')[0]\n","target_E = tokenizer.encode(' Emma')[0]\n","\n","tokensME = tokenizer.encode(text_ME,return_tensors='pt').to(device)\n","tokensEM = tokenizer.encode(text_EM,return_tensors='pt').to(device)"],"metadata":{"id":"iKaNCGyPunU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NqZsbWmnyR5v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get \"clean\" data on texts (no patching)"],"metadata":{"id":"NNhsbhTxjVoz"}},{"cell_type":"code","source":["with torch.no_grad():\n","  outME = model(tokensME,output_hidden_states=True)\n","  outEM = model(tokensEM,output_hidden_states=True)\n","\n","hs_ME = outME.hidden_states\n","outME.keys(), outME.hidden_states[3].shape"],"metadata":{"id":"vgUVkKNounSL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logitDiff_ME = outME.logits[0,-1,target_M] - outME.logits[0,-1,target_E]\n","logitDiff_EM = outEM.logits[0,-1,target_M] - outEM.logits[0,-1,target_E]\n","\n","print(f'Logit difference for text \"ME\": {logitDiff_ME:6.3f}')\n","print(f'Logit difference for text \"EM\": {logitDiff_EM:6.3f}')"],"metadata":{"id":"8EAYrp4okirZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eT34F4ZEa1l1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run the IOI experiment over layers"],"metadata":{"id":"5eMS9_iAjcje"}},{"cell_type":"code","source":["# initializations\n","confirmManipulation = np.zeros((n_layers,2))\n","logitDiffs = np.zeros(n_layers)\n","\n","# loop over layers\n","for layeri in range(n_layers):\n","\n","  # patch this layer\n","  def hookfun(module, input, output):\n","    hs = output[0].clone()\n","    hs[0,-1,:] = outME.hidden_states[layeri+1][0,-1,:]\n","    output = (hs,*output[1:])\n","    return output\n","\n","  # implant the hook\n","  handle = model.transformer.h[layeri].register_forward_hook(hookfun)\n","\n","  # forward pass with hook\n","  with torch.no_grad():\n","    outEM = model(tokensEM,output_hidden_states=True)\n","  hs_EM = outEM.hidden_states\n","\n","  # remove the hook\n","  handle.remove()\n","\n","  # confirmation: first element should be zero, second non-zero\n","  confirmManipulation[layeri,0] = hs_EM[layeri+1][0,-1,10] - hs_ME[layeri+1][0,-1,10]\n","  confirmManipulation[layeri,1] = hs_EM[layeri+1][0,-2,10] - hs_ME[layeri+1][0,-2,10]\n","\n","  # now for the logit-difference test\n","  logitDiffs[layeri] = outEM.logits[0,-1,target_M] - outEM.logits[0,-1,target_E]\n"],"metadata":{"id":"giaEdthGum8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sanity check :)\n","confirmManipulation"],"metadata":{"id":"IiJowgurc5lm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualization\n","plt.figure(figsize=(11,4))\n","\n","# plot the logit differences for the \"clean\" runs (no patching)\n","plt.axhline(logitDiff_EM.cpu(),color='b',label='Clean \"EM\"')\n","plt.axhline(logitDiff_ME.cpu(),color='r',label='Clean \"ME\"')\n","\n","# then for the experiment results\n","plt.plot(logitDiffs,'ko',markerfacecolor=[.9,.7,.9],markersize=10,label='A patched to B')\n","\n","# the dividing line\n","plt.axhline(0,linestyle='--',color='gray',linewidth=.5)\n","plt.text(0,.1,'Prefer \"Mike\"',fontsize=12,va='bottom')\n","plt.text(0,-.1,'Prefer \"Emma\"',fontsize=12,va='top')\n","\n","plt.gca().set(xlabel='Transformer block',ylabel='Logit difference',title='Reversing logit bias towards target-Mike')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"vjZdxt4sa2jf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YiaKyevha2eB"},"execution_count":null,"outputs":[]}]}