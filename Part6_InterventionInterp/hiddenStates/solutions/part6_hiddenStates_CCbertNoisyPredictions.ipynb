{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMhXNksuQ8EFLsKD40SVPuP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Editing hidden states<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Noisy and shuffled BERT predictions<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"uwGYahQ2vHxw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Hook the BERT model"],"metadata":{"id":"4UR67eKtvHu0"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForMaskedLM\n","\n","# Load BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"ON8xghnbejvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# indices (redefined later)\n","layer2shuffle = 40000 # no shuffling...\n","maskTarget_idx = 1\n","\n","\n","# hooking functions\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # only change this layer\n","    if layer_number == layer2shuffle:\n","\n","      # unpack tuple\n","      hidden, *rest = output\n","\n","      # randomly shuffle\n","      acts = hidden[0,maskTarget_idx,:]\n","      shuffleidx = torch.randperm(model.config.hidden_size)\n","      hidden[0,maskTarget_idx,:] = acts[shuffleidx]\n","\n","      # reconstruct output\n","      output = tuple([hidden]+rest)\n","\n","      print(f'Shuffled layer {layer_number}')\n","\n","    return output\n","  return hook\n","\n","\n","# loop over layers and do surgery\n","handles = []\n","for layeri in range(model.config.num_hidden_layers):\n","  h = model.bert.encoder.layer[layeri].register_forward_hook(implant_hook(layeri))\n","  handles.append(h)"],"metadata":{"id":"Unfqn9uLejsf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oNvhUHM2vfx5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Tokens and masked predictions"],"metadata":{"id":"_UveiRJIvfu_"}},{"cell_type":"code","source":["# the text\n","tokens = tokenizer(f'Pay no attention to that man {tokenizer.mask_token} the curtain!',return_tensors='pt')\n","\n","# the mask index\n","maskTarget_idx = torch.where(tokens['input_ids'][0] == tokenizer.mask_token_id)[0].item()\n","\n","# correct response\n","correct_target = tokenizer.encode('behind',add_special_tokens=False)[0]\n","\n","# print out the tokens\n","for t in tokens['input_ids'][0]:\n","  print(f'{t:5}: \"{tokenizer.decode(t)}\"')\n","\n","print(f'\\nThe mask is in token index {maskTarget_idx}')"],"metadata":{"id":"5PDoNbldejp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# redefine as outside the range, in case you want to rerun this code later\n","layer2shuffle = 40000\n","\n","with torch.no_grad():\n","  out_pure = model(**tokens.to(device),output_hidden_states=True)\n","\n","print(f'There are {len(out_pure.hidden_states)} hidden states,\\neach of size {list(out_pure.hidden_states[2].shape)}')"],"metadata":{"id":"3zfr49vckLvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predicted tokens\n","for i in range(len(tokens['input_ids'][0])):\n","\n","  # the word in the original text\n","  actual = tokenizer.decode(tokens['input_ids'][0,i])\n","\n","  # the predicted word\n","  predicted = tokenizer.decode(torch.argmax(out_pure.logits[0,i,:].detach().cpu(),dim=-1))\n","\n","  # print!\n","  print(f'{actual:>12} predicted as \"{predicted}\"')"],"metadata":{"id":"CHFiEK-gejnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TowS3dtqtM4n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Masked predictions after random shuffling"],"metadata":{"id":"BBV_57Y3uZbp"}},{"cell_type":"code","source":["mask_prediction = np.zeros((model.config.num_hidden_layers,2))\n","\n","for layeri in range(model.config.num_hidden_layers):\n","\n","  # layer to shuffle\n","  layer2shuffle = layeri\n","\n","  # forward pass\n","  with torch.no_grad(): out=model(**tokens.to(device))\n","\n","  # log-softmax the logits and store the correct target's lsm\n","  logsm = F.log_softmax(out.logits[0,maskTarget_idx,:].detach().cpu(),dim=-1)\n","  mask_prediction[layeri,0] = logsm[correct_target]\n","\n","  # predicted masked target\n","  maxidx = torch.argmax(logsm,dim=-1)\n","  mask_prediction[layeri,1] = maxidx\n"],"metadata":{"id":"wHq33CQWuZZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for layeri in range(model.config.num_hidden_layers):\n","  print(f'Shuffling layer {layeri:2} led to prediction: \"{tokenizer.decode(int(mask_prediction[layeri,1]))}\"')"],"metadata":{"id":"mA-9it-ZuZV8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# log-softmax the unshuffled data\n","logsm = F.log_softmax(out_pure.logits[0,maskTarget_idx,:].detach().cpu(),dim=-1)\n","pure_max_logit = logsm[correct_target]\n","\n","# visualize\n","plt.figure(figsize=(8,3))\n","plt.plot(mask_prediction[:,0],'ks',markerfacecolor=[.9,.7,.7],markersize=10,label='Shuffled')\n","plt.axhline(pure_max_logit,color='b',zorder=-4,label='Unshuffled')\n","\n","plt.gca().set(xlabel='Hidden layer that was shuffled',ylabel='Correct token logit (log-sm)')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"9IrJsgmBuZTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1cjQ_mZvuZQF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Add noise instead"],"metadata":{"id":"TxmAO3rRzu2M"}},{"cell_type":"code","source":["for h in handles:\n","  h.remove()"],"metadata":{"id":"kQWixY3H0GY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hooking functions\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # only change one layer\n","    if layer_number == layer2shuffle:\n","\n","      # unpack tuple\n","      hidden, *rest = output\n","\n","      # noisify\n","      acts = hidden[0,maskTarget_idx,:]\n","      noise = torch.randn_like(acts) * acts.std()*2 # same size and slightly bigger std\n","      hidden[0,maskTarget_idx,:] = acts + noise\n","\n","      # reconstruct output\n","      output = tuple([hidden]+rest)\n","\n","    return output\n","  return hook\n","\n","\n","# loop over layers and do surgery\n","handles = []\n","for layeri in range(model.config.num_hidden_layers):\n","  h = model.bert.encoder.layer[layeri].register_forward_hook(implant_hook(layeri))\n","  handles.append(h)"],"metadata":{"id":"VBgzPu1vzzQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mask_prediction = np.zeros((model.config.num_hidden_layers,2))\n","\n","for layeri in range(model.config.num_hidden_layers):\n","\n","  # layer to shuffle\n","  layer2shuffle = layeri\n","\n","  # forward pass\n","  with torch.no_grad(): out=model(**tokens.to(device))\n","\n","  # log-softmax the logits and store the correct target's lsm\n","  logsm = F.log_softmax(out.logits[0,maskTarget_idx,:].detach().cpu(),dim=-1)\n","  mask_prediction[layeri,0] = logsm[correct_target]\n","\n","  # predicted masked target\n","  maxidx = torch.argmax(logsm,dim=-1)\n","  mask_prediction[layeri,1] = maxidx\n","\n","\n","for layeri in range(model.config.num_hidden_layers):\n","  print(f'Shuffling layer {layeri:2} led to prediction: \"{tokenizer.decode(int(mask_prediction[layeri,1]))}\"')"],"metadata":{"id":"jLLNZ3zAzuy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,3))\n","plt.plot(mask_prediction[:,0],'ks',markerfacecolor=[.9,.7,.7],markersize=10,label='Shuffled')\n","plt.axhline(pure_max_logit,color='b',label='Unshuffled')\n","\n","plt.gca().set(xlabel='Hidden layer that was shuffled',ylabel='MASK location max logit')\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"i3dfQ8TizuwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9OQsWR8_zulA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Distributions of activations and noise"],"metadata":{"id":"tp5YPBJHwg7q"}},{"cell_type":"code","source":["# # another model run with no shuffling, and with hidden states\n","# layer2shuffle = 98765\n","# with torch.no_grad():\n","#   out = model(**tokens.to(device),output_hidden_states=True)\n","\n","# out_pure is from exercise 2\n","hs = out_pure.hidden_states"],"metadata":{"id":"SR-DAH1YtzHq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(10,4))\n","\n","# bins for the histogram\n","xvals4hist = np.linspace(-4,4,101)\n","\n","# loop over layers\n","for i in range(1,len(hs)):\n","\n","  # get the vectorized data\n","  vdat = hs[i].detach().cpu().numpy().flatten()\n","\n","  # calculate and draw the histogram\n","  y,x = np.histogram(vdat,bins=xvals4hist)\n","  plt.plot(x[:-1],y,color=mpl.cm.plasma(i/(len(hs)+1)))\n","\n","# and again for gaussian noise\n","noise = np.random.randn(len(vdat)) * vdat.std()*2\n","y,x = np.histogram(noise,bins=xvals4hist)\n","plt.plot(x[:-1],y,'k',linewidth=3,label='Gaussian noise')\n","\n","# create a colorbar for the lines\n","norm = mpl.colors.Normalize(vmin=0,vmax=len(hs))\n","sm = mpl.cm.ScalarMappable(cmap=mpl.cm.plasma,norm=norm)\n","cbar = fig.colorbar(sm,ax=plt.gca(),pad=.01)\n","cbar.set_label(r'Transformer block')\n","\n","# final touches\n","plt.legend()\n","plt.gca().set(xlabel='Activation values',ylabel='Count',xlim=xvals4hist[[0,-1]],\n","              ylim=[-10,None],title='Histograms of activation values')\n","plt.show()"],"metadata":{"id":"WPBxecbKtzFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IfGWhBAHvIC_"},"execution_count":null,"outputs":[]}]}