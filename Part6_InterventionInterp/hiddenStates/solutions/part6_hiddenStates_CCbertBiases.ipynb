{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOeCOKFaqzRbov5usdmAV83"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Editing hidden states<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Measure and correct BERT's bias<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"uwGYahQ2vHxw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Hook the BERT model"],"metadata":{"id":"4UR67eKtvHu0"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForMaskedLM\n","\n","# Load BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"ON8xghnbejvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# indices (redefined later)\n","layer2replace = 40000 # no replacement...\n","hs_vector2replace = torch.zeros(model.config.hidden_size)\n","\n","\n","mixture = [.1,.9]\n","\n","# hooking functions\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # only change this layer if there's a matching variable value\n","    if layer_number == layer2replace:\n","\n","      # unpack tuple\n","      hidden, *rest = output\n","\n","      # mix the old and the new\n","      hidden[0,maskTarget_idx,:] = mixture[0]*hidden[0,maskTarget_idx,:] + mixture[1]*hs_vector2replace\n","\n","\n","      # reconstruct output\n","      output = tuple([hidden]+rest)\n","\n","      print(f'Replaced layer {layer_number:2}')\n","\n","    return output\n","  return hook\n","\n","\n","# loop over layers and do surgery\n","handles = []\n","for layeri in range(model.config.num_hidden_layers):\n","  h = model.bert.encoder.layer[layeri].register_forward_hook(implant_hook(layeri))\n","  handles.append(h)"],"metadata":{"id":"yxxIZNri99kM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mZ87E0MC9bou"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Test for a gender bias in BERT"],"metadata":{"id":"_UveiRJIvfu_"}},{"cell_type":"code","source":["# list of target words\n","target_words = [ 'he','she','they' ]\n","\n","# tokenize sentences\n","tokens_he   = tokenizer(f'The engineer informed the client that he would need more time.',return_tensors='pt')\n","tokens_she  = tokenizer(f'The engineer informed the client that she would need more time.',return_tensors='pt')\n","tokens_they = tokenizer(f'The engineer informed the client that they would need more time.',return_tensors='pt')\n","\n","# tokenize the masked sentence\n","tokens_mask = tokenizer(f'The engineer informed the client that {tokenizer.mask_token} would need more time.',return_tensors='pt')"],"metadata":{"id":"bDCuHH2a-GCn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the mask index\n","maskTarget_idx = torch.where(tokens_mask['input_ids'][0] == tokenizer.mask_token_id)[0].item()\n","\n","# token indices of target words\n","targets_idx = [tokenizer.encode(t)[1] for t in target_words]\n","\n","# print out the tokens\n","for t in tokens_mask['input_ids'][0]:\n","  print(f'{t:5}: \"{tokenizer.decode(t)}\"')\n","\n","print(f'\\nThe mask is in token index {maskTarget_idx}\\n')\n","for t in targets_idx:\n","  print(f'Target \"{tokenizer.decode(t)}\" is index {t}')"],"metadata":{"id":"5PDoNbldejp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# redefine as outside the range, in case you want to rerun this code later\n","layer2replace = 40000\n","\n","# forward-pass the four versions\n","with torch.no_grad():\n","  out_he = model(**tokens_he.to(device),output_hidden_states=True)\n","  out_she = model(**tokens_she.to(device),output_hidden_states=True)\n","  out_they = model(**tokens_they.to(device),output_hidden_states=True)\n","  out_mask = model(**tokens_mask.to(device),output_hidden_states=True)"],"metadata":{"id":"3zfr49vckLvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# grab and visualize the log-softmax\n","\n","fig,axs = plt.subplots(2,3,figsize=(10,5))\n","\n","# for \"he\"\n","logsm = F.log_softmax(out_he.logits[0,maskTarget_idx,:],dim=-1).detach().cpu()\n","axs[0,0].bar(range(3),logsm[targets_idx])\n","axs[1,0].bar(range(3),torch.exp(logsm[targets_idx]))\n","axs[0,0].set(xticks=range(3),xticklabels=target_words,ylabel='Log-softmax',title='Probs. in \"he\" sentence')\n","axs[1,0].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob')\n","\n","\n","# for \"she\"\n","logsm = F.log_softmax(out_she.logits[0,maskTarget_idx,:],dim=-1).detach().cpu()\n","axs[0,1].bar(range(3),logsm[targets_idx])\n","axs[1,1].bar(range(3),torch.exp(logsm[targets_idx]))\n","axs[0,1].set(xticks=range(3),xticklabels=target_words,ylabel='Log-softmax',title='Probs. in \"she\" sentence')\n","axs[1,1].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob')\n","\n","# for \"they\"\n","logsm = F.log_softmax(out_they.logits[0,maskTarget_idx,:],dim=-1).detach().cpu()\n","axs[0,2].bar(range(3),logsm[targets_idx])\n","axs[1,2].bar(range(3),torch.exp(logsm[targets_idx]))\n","axs[0,2].set(xticks=range(3),xticklabels=target_words,ylabel='Log-softmax',title='Probs. in \"they\" sentence')\n","axs[1,2].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"B1DkIHj6-98o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# grab and visualize the log-softmax\n","logsm = F.log_softmax(out_mask.logits[0,maskTarget_idx,:],dim=-1).detach().cpu()\n","\n","fig,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","axs[0].bar(range(3),logsm[targets_idx])\n","axs[1].bar(range(3),torch.exp(logsm[targets_idx]))\n","\n","axs[0].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Log-softmax',title='Log-softmax for masked word')\n","axs[1].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob.',title='Softmax probability for masked word')\n","\n","fig.suptitle(tokenizer.decode(tokens_mask['input_ids'][0,1:-1]),fontweight='bold')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"yTYI-88B--AD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UE8-uojW-95n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Edit in an anti-bias?"],"metadata":{"id":"CHFiEK-gejnH"}},{"cell_type":"code","source":["# get s/he/they activation from one hidden state\n","\n","layer2replace = 10\n","hs_vector2replace = out_she.hidden_states[layer2replace+1][0,maskTarget_idx,:]\n","\n","with torch.no_grad():\n","  out_mask_replace = model(**tokens_mask.to(device),output_hidden_states=True)"],"metadata":{"id":"6LSLB9BFEY1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# grab and visualize the log-softmax\n","logsm_orig = F.log_softmax(out_mask.logits[0,maskTarget_idx,:],dim=-1).detach().cpu()\n","logsm_repl = F.log_softmax(out_mask_replace.logits[0,maskTarget_idx,:],dim=-1).detach().cpu()\n","\n","fig,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","axs[0].bar(np.arange(3)-.2,logsm_orig[targets_idx],width=.5,label='Original')\n","axs[0].bar(np.arange(3)+.2,logsm_repl[targets_idx],width=.5,label='Modified')\n","axs[0].legend()\n","axs[0].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Log-softmax',title='Log-softmax for masked word')\n","\n","axs[1].bar(np.arange(3)-.2,torch.exp(logsm_orig[targets_idx]),width=.5,label='Original')\n","axs[1].bar(np.arange(3)+.2,torch.exp(logsm_repl[targets_idx]),width=.5,label='Modified')\n","axs[1].legend()\n","axs[1].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob.',title='Softmax probability for masked word')\n","\n","fig.suptitle(tokenizer.decode(tokens_mask['input_ids'][0,1:-1]),fontweight='bold')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"y2w4hqpF9bl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bias_orig = logsm_orig[targets_idx[0]] - logsm_orig[targets_idx[1]]\n","bias_repl = logsm_repl[targets_idx[0]] - logsm_repl[targets_idx[1]]\n","\n","print(f'Bias (he-she) in original model: {bias_orig:.3f}')\n","print(f'Bias (he-she) in modified model: {bias_repl:.3f}')"],"metadata":{"id":"dI09CGu_9bjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zMla7k2T9bgT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Laminar profile of anti-bias impact"],"metadata":{"id":"_okADJHm9bdD"}},{"cell_type":"code","source":["mixture = [.5,.5]"],"metadata":{"id":"0mqlJJr-Rr6j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bias_scores = torch.zeros(model.config.num_hidden_layers)\n","\n","for layer2replace in range(model.config.num_hidden_layers):\n","\n","  # vector to replace (from \"she\" sentence)\n","  hs_vector2replace = out_she.hidden_states[layer2replace+1][0,maskTarget_idx,:]\n","\n","  # forward-pass with hook to replace\n","  with torch.no_grad():\n","    out_mask_replace = model(**tokens_mask.to(device),output_hidden_states=True)\n","\n","  # calculate the log-sm probabilities\n","  logsm_repl = F.log_softmax(out_mask_replace.logits[0,maskTarget_idx,:],dim=-1).detach().cpu()\n","\n","  # calculate the bias towards \"he\"\n","  bias_scores[layer2replace] = logsm_repl[targets_idx[0]] - logsm_repl[targets_idx[1]]"],"metadata":{"id":"r9Njv3pg9baV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,3))\n","plt.plot(bias_scores,'ko-',markerfacecolor=[.7,.9,.7],markersize=10,linewidth=.5)\n","plt.axhline(0,linestyle='--',zorder=-3,color='gray')\n","plt.gca().set(xlabel='Layer of replacement',ylabel='Bias score')\n","plt.show()"],"metadata":{"id":"SXXd1g249bXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for h in handles:\n","  h.remove()"],"metadata":{"id":"scLwMAQYMUZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XyjdpgWU9bUD"},"execution_count":null,"outputs":[]}]}