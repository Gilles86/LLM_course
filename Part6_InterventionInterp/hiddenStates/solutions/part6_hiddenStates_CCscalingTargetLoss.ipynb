{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyObG/nu8rtiZHgy2xR4eDE0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Editing hidden states<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Hidden-state scaling and token loss<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"hTWsybWmZJe-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Model, hooks, tokens"],"metadata":{"id":"RwWvB7OEbId9"}},{"cell_type":"code","source":["model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n","model.eval()"],"metadata":{"id":"pFyxDs3UaDiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary of scaling factors\n","scaling_fact_dict = {}\n","\n","# hooking functions\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # only change this layer if there's a matching key in the dictionary\n","    if layer_number in scaling_fact_dict.keys():\n","      hidden, *rest = output\n","      hs = hidden.clone()\n","      hs.mul_(scaling_fact_dict[layer_number])\n","      output = tuple([hs]+rest)\n","\n","    return output\n","  return hook\n","\n","for layeri in range(model.config.n_layer):\n","  model.transformer.h[layeri].register_forward_hook(implant_hook(layeri))"],"metadata":{"id":"IEerqWdRZJXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# quote from Einstein (missing final word is \"curious\")\n","text = 'I have no special talent. I am only passionately'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","\n","target_token = tokenizer.encode(' curious',return_tensors='pt')[0].item() # isolate the index from the tensor\n","target_token"],"metadata":{"id":"NwDH3NKkaDf4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  out = model(tokens)\n","\n","pure_logits = out.logits.detach()"],"metadata":{"id":"I6BnmkRqb7TU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"U441XxlvZJVG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Scale one layer"],"metadata":{"id":"g2LpmQv9ZJSW"}},{"cell_type":"code","source":["# redefine dictionary\n","scaling_fact_dict = {2:.6}\n","\n","with torch.no_grad():\n","  out = model(tokens)\n","\n","scal_logits = out.logits.detach()"],"metadata":{"id":"er9aXrX_ZJPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show the logits\n","plt.figure(figsize=(8,3))\n","plt.plot(pure_logits[0,-1,:],'b.',markersize=2,alpha=.1,label='Unscaled')\n","plt.plot(scal_logits[0,-1,:],'r.',markersize=2,alpha=.1,label='Scaled')\n","\n","# adjust legend markers\n","h = plt.legend()\n","for hi in h.legend_handles:\n","  hi.set(markersize=5,alpha=1)\n","\n","plt.gca().set(xlabel='Vocab elements',xlim=[0,model.config.vocab_size],ylabel='Final logits')\n","\n","plt.show()"],"metadata":{"id":"jokQ4l1KZJM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show the logits\n","plt.figure(figsize=(5,4))\n","plt.plot(pure_logits[0,-1,:],scal_logits[0,-1,:],'k.',alpha=.3,markersize=2)\n","\n","# their correlation\n","r = np.corrcoef(pure_logits[0,-1,:],scal_logits[0,-1,:])\n","\n","plt.gca().set(xlabel='Unscaled logits',ylabel='Scaled logits',\n","              title=f'Impact of scaling layer {list(scaling_fact_dict.keys())[0]} (r = {r[0,1]:.6f})')\n","plt.show()"],"metadata":{"id":"O7PtOVQuZJKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7U19mENfKrbR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Sort and print predicted tokens"],"metadata":{"id":"c9gNITFsWkt-"}},{"cell_type":"code","source":["# sort the final logits\n","pure_sorted = torch.argsort(pure_logits[0,-1,:],descending=True)\n","scal_sorted = torch.argsort(scal_logits[0,-1,:],descending=True)\n","\n","print(' '*(len(text)+3),'   Unscaled   |    Scaled')\n","print(' '*(len(text)+3),'-'*30)\n","for i in range(10):\n","  print(f'{tokenizer.decode(tokens[0])}... {tokenizer.decode(pure_sorted[i]):>12}  | {tokenizer.decode(scal_sorted[i])}')"],"metadata":{"id":"eCjoIk4hZJHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate loss for target item on final logit\n","pure_target_loss = -F.log_softmax(pure_logits[0,-1,:],dim=-1)[target_token]\n","scal_target_loss = -F.log_softmax(scal_logits[0,-1,:],dim=-1)[target_token]\n","\n","dictItems = list(scaling_fact_dict.items())[0]\n","print(f'When scaling layer {dictItems[0]} by {dictItems[1]}:','\\n')\n","print(f'Target loss in unscaled model: {pure_target_loss:.3f}')\n","print(f'Target loss in scaled model  : {scal_target_loss:.3f}')"],"metadata":{"id":"XR4wDb4Cf7F7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cDCrlE2DZJEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Target token loss by layer scaling"],"metadata":{"id":"hDD4j_JOZJBg"}},{"cell_type":"code","source":["# initialize\n","targetLosses = np.zeros(model.config.n_layer)\n","targetCorrs = np.zeros(model.config.n_layer)\n","\n","# log-softmax for the unscaled logits\n","pure_logsm = F.log_softmax(pure_logits[0,-1,:],dim=-1)\n","\n","# loop over layers\n","for layeri in range(model.config.n_layer):\n","\n","  # recreate the scaling dictionary from scratch\n","  scaling_fact_dict = {layeri:.6}\n","\n","  # forward pass\n","  with torch.no_grad(): out=model(tokens)\n","\n","  # get the target log-softmax and loss\n","  scal_logsm = F.log_softmax(out.logits[0,-1,:],dim=-1)\n","  targetLosses[layeri] = -scal_logsm[target_token]\n","\n","  # and their correlation\n","  catX = torch.concatenate((pure_logsm.unsqueeze(0),scal_logsm.unsqueeze(0)),dim=0)\n","  targetCorrs[layeri] = torch.corrcoef(catX)[0,1]"],"metadata":{"id":"kerYAMYphCBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","axs[0].plot(targetLosses,'ks-',markerfacecolor=[.7,.9,.7],markersize=10,linewidth=.5,label='Scaled')\n","axs[0].axhline(-pure_logsm[target_token],color=[.9,.7,.7],linewidth=2,zorder=-3,label='Unscaled')\n","\n","axs[0].set(xlabel='Transformer block with hidden-state scaling',\n","              ylabel='Target token loss $\\\\left(-\\ln(\\hat{y})\\\\right)$',title='Target losses')\n","axs[0].legend()\n","\n","axs[1].plot(targetCorrs,'ks-',markerfacecolor=[.7,.7,.9],markersize=10,linewidth=.5)\n","axs[1].set(xlabel='Transformer block with hidden-state scaling',\n","              ylabel='Logit correlation',title='Correlation between scaled and unscaled log-sm')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"JEpzulA5hB9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzDbMzB9Y2U_"},"outputs":[],"source":[]}]}