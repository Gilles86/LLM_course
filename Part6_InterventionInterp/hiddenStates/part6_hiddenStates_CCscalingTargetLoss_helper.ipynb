{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO5J7Hpgji5K0tdCl1cEw6v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Editing hidden states<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Hidden-state scaling and token loss<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"hTWsybWmZJe-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Model, hooks, tokens"],"metadata":{"id":"RwWvB7OEbId9"}},{"cell_type":"code","source":["model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n","\n","# in eval mode"],"metadata":{"id":"pFyxDs3UaDiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary of scaling factors\n","scaling_fact_dict = {}\n","\n","# hooking functions\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # only change this layer if there's a matching key in the dictionary\n","    if layer_number in scaling_fact_dict.keys():\n","      hidden, *rest =\n","      hs =\n","\n","      output =\n","\n","    return output\n","  return hook\n","\n","for layeri in range(model.config.n_layer):\n","  .register_forward_hook(implant_hook(layeri))"],"metadata":{"id":"IEerqWdRZJXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# quote from Einstein (missing final word is \"curious\")\n","text = 'I have no special talent. I am only passionately'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","\n","target_token =\n","target_token"],"metadata":{"id":"NwDH3NKkaDf4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  out = model(tokens)\n","\n","pure_logits ="],"metadata":{"id":"I6BnmkRqb7TU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"U441XxlvZJVG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Scale one layer"],"metadata":{"id":"g2LpmQv9ZJSW"}},{"cell_type":"code","source":["# redefine dictionary\n","scaling_fact_dict =\n","\n","with torch.no_grad():\n","  out = model(tokens)\n","\n","scal_logits ="],"metadata":{"id":"er9aXrX_ZJPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show the logits\n","plt.figure(figsize=(8,3))\n","plt.plot(pure_logits\n","plt.plot(scal_logits\n","\n"],"metadata":{"id":"jokQ4l1KZJM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show the logits\n","plt.figure(figsize=(5,4))\n","plt.plot(pure_logits[0,-1\n","\n","# their correlation\n","r = np.corrcoef(\n","\n","plt.gca().set(xlabel='Unscaled logits',ylabel='Scaled logits',)\n","plt.show()"],"metadata":{"id":"O7PtOVQuZJKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7U19mENfKrbR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Sort and print predicted tokens"],"metadata":{"id":"c9gNITFsWkt-"}},{"cell_type":"code","source":["# sort the final logits\n","pure_sorted = torch.argsort(\n","scal_sorted =\n","\n","print(' '*(len(text)+3),'   Unscaled   |    Scaled')\n","print(' '*(len(text)+3),'-'*30)\n","for i in range(10):\n","  print"],"metadata":{"id":"eCjoIk4hZJHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate loss for target item on final logit\n","pure_target_loss =\n","scal_target_loss =\n","\n","dictItems =\n","print(f'When scaling layer {} by {}:','\\n')\n","print(f'Target loss in unscaled model:\n","print(f'Target loss in scaled model  :"],"metadata":{"id":"XR4wDb4Cf7F7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cDCrlE2DZJEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Target token loss by layer scaling"],"metadata":{"id":"hDD4j_JOZJBg"}},{"cell_type":"code","source":["# initialize\n","targetLosses =\n","targetCorrs = np.zeros\n","\n","# log-softmax for the unscaled logits\n","pure_logsm =\n","\n","# loop over layers\n","for layeri in\n","\n","  # recreate the scaling dictionary from scratch\n","  scaling_fact_dict =\n","\n","  # forward pass\n","  with torch.no_grad():\n","\n","  # get the target log-softmax and loss\n","  scal_logsm =\n","  targetLosses[layeri] =\n","\n","  # and their correlation\n","  catX = torch.concatenate(,dim=)\n","  targetCorrs[layeri] = torch.corrcoef"],"metadata":{"id":"kerYAMYphCBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","axs[0].plot\n","axs[0].axhline\n","\n","\n","axs[1].plot\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"JEpzulA5hB9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzDbMzB9Y2U_"},"outputs":[],"source":[]}]}