{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPO9HhV6/O7U6ZnobYREl1x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Editing hidden states<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Measure and correct BERT's bias<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"uwGYahQ2vHxw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Hook the BERT model"],"metadata":{"id":"4UR67eKtvHu0"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForMaskedLM\n","\n","# Load BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# push the model to the GPU and switch to eval mode"],"metadata":{"id":"ON8xghnbejvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# indices (redefined later)\n","layer2replace = 40000 # no replacement...\n","hs_vector2replace = torch.zeros(\n","\n","\n","mixture = [.1,.9]\n","\n","# hooking functions\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # only change this layer if there's a matching variable value\n","    if layer_number == layer2replace:\n","\n","      # unpack tuple\n","      hidden,\n","\n","      # mix the old and the new\n","      hidden[0,maskTarget_idx,:] =  +\n","\n","\n","      # reconstruct output\n","      output =\n","\n","      print(f'Replaced layer {layer_number:2}')\n","\n","    return output\n","  return hook\n","\n","\n","# loop over layers and do surgery\n","handles = []\n","for layeri in range(model.config.num_hidden_layers):\n","  h = model.bert.encoder.layer[layeri].\n","  handles.append"],"metadata":{"id":"yxxIZNri99kM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mZ87E0MC9bou"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Test for a gender bias in BERT"],"metadata":{"id":"_UveiRJIvfu_"}},{"cell_type":"code","source":["# list of target words\n","target_words = [ 'he','she','they' ]\n","\n","# tokenize sentences\n","tokens_he   = tokenizer(f'The engineer informed the client that he would need more time.',return_tensors='pt')\n","tokens_she  =\n","tokens_they =\n","\n","# tokenize the masked sentence\n","tokens_mask ="],"metadata":{"id":"bDCuHH2a-GCn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the mask index\n","maskTarget_idx =\n","\n","# token indices of target words\n","targets_idx =\n","\n","# print out the tokens\n","for t in tokens_mask['input_ids'][0]:\n","  print(f\n","\n","print(f'\\nThe mask is in token index {maskTarget_idx}\\n')\n","for t in targets_idx:\n","  print(f"],"metadata":{"id":"5PDoNbldejp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# redefine as outside the range, in case you want to rerun this code later\n","layer2replace = 40000\n","\n","# forward-pass the four versions\n","with torch.no_grad():\n","  out_he = model(**tokens_he,)\n","  out_she =\n","  out_they =\n","  out_mask ="],"metadata":{"id":"3zfr49vckLvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# grab and visualize the log-softmax\n","\n","fig,axs = plt.subplots(2,3,figsize=(10,5))\n","\n","# for \"he\"\n","logsm = F.log_softmax(out_he.logits\n","axs[0,0].bar(range(3),logsm[])\n","axs[1,0].bar(range(3),torch.exp(logsm[]))\n","axs[0,0].set(xticks=range(3),xticklabels=target_words,ylabel='Log-softmax',title='Probs. in \"he\" sentence')\n","axs[1,0].set(xticks=range(3),xticklabels=target_words,xlabel='Target words',ylabel='Softmax prob')\n","\n","\n","# for \"she\"\n","\n","\n","# for \"they\"\n","\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"B1DkIHj6-98o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# grab and visualize the log-softmax\n","logsm = F.log_softmax(out_mask.l\n","\n","fig,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","axs[0].bar(range(3),\n","axs[1].bar(range(3),\n","\n","\n","\n","fig.suptitle(tokenizer.decode(tokens_mask['input_ids'][0,1:-1]),fontweight='bold')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"yTYI-88B--AD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UE8-uojW-95n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Edit in an anti-bias?"],"metadata":{"id":"CHFiEK-gejnH"}},{"cell_type":"code","source":["# get s/he/they activation from one hidden state\n","\n","layer2replace =\n","hs_vector2replace =\n","\n","with torch.no_grad():\n","  out_mask_replace = model(**tokens_mask.to(device),output_hidden_states=True)"],"metadata":{"id":"6LSLB9BFEY1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# grab and visualize the log-softmax\n","logsm_orig =\n","logsm_repl =\n","\n","fig,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","axs[0].bar(label='Original')\n","axs[0].bar(,label='Modified')\n","axs[0].legend()\n","\n","\n","axs[1].\n","\n","fig.suptitle(tokenizer.decode(tokens_mask['input_ids'][0,1:-1]),fontweight='bold')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"y2w4hqpF9bl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bias_orig =\n","bias_repl =\n","\n","print(f'Bias (he-she) in original model: {bias_orig:.3f}')\n","print(f'Bias (he-she) in modified model: {bias_repl:.3f}')"],"metadata":{"id":"dI09CGu_9bjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zMla7k2T9bgT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Laminar profile of anti-bias impact"],"metadata":{"id":"_okADJHm9bdD"}},{"cell_type":"code","source":["mixture = [.5,.5]"],"metadata":{"id":"0mqlJJr-Rr6j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bias_scores = torch.zeros(model.config.num_hidden_layers)\n","\n","for layer2replace in range(\n","\n","  # vector to replace (from \"she\" sentence)\n","  hs_vector2replace = out_she.hidden_stat\n","\n","  # forward-pass with hook to replace\n","  with torch.no_grad():\n","    out_mask_replace = model(,=True)\n","\n","  # calculate the log-sm probabilities\n","  logsm_repl =\n","\n","  # calculate the bias towards \"he\"\n","  bias_scores[layer2replace] ="],"metadata":{"id":"r9Njv3pg9baV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,3))\n","plt.plot(bias_scores,'ko-',markerfacecolor=[.7,.9,.7],markersize=10,linewidth=.5)\n"],"metadata":{"id":"SXXd1g249bXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for h in handles:\n","  h.remove()"],"metadata":{"id":"scLwMAQYMUZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XyjdpgWU9bUD"},"execution_count":null,"outputs":[]}]}