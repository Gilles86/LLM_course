{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNsQ524xdycJo+Ybr88nSTE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>How to modify activations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: replacing attention, MLP, and hidden states<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model.eval()"]},{"cell_type":"code","source":[],"metadata":{"id":"ojMdlVejOGQB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Zero-out the third attention head in K"],"metadata":{"id":"qMS9TnAsOGLq"}},{"cell_type":"code","source":["tokens = tokenizer.encode('I wonder how many tokens are in pomegranate.',return_tensors='pt')\n","\n","for t in tokens[0]:\n","  print(f'Token index   is \"\"')"],"metadata":{"id":"-Uui0C85R3Lf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some useful variables\n","nheads =\n","n_emb =\n","head_dim =\n","\n","# find the start and end index of the 3rd head\n","whichHead_idx = 2\n","h3_start =\n","h3_end =\n","\n","print(f'Attention head {} starts at \\nindex {} and ends at index {}.')"],"metadata":{"id":"z_Had9RjTTOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize activations dictionary\n","activations = {}\n","\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # split the output into QKV (each is [B,S,H])\n","    q,k,v = output.split\n","\n","    # make an editable copy of k vectors\n","    k_copy = k.\n","\n","    # zero-out the data only from the specified head\n","    k_copy[:,:,:] = 0\n","\n","    # recombine q with modified k and v\n","    QKV = torch.cat()\n","\n","    # store the activations\n","    activations['qkv'] = QKV.().()\n","\n","    # output the QKV matrix so it replaces the original\n","    return QKV\n","\n","  return hook\n","\n","\n","layer2modify = 3\n","hookHandle = .register_forward_hook(implant_hook(layer2modify))"],"metadata":{"id":"TXLPspZpOGIg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm\n","model(tokens)\n","\n","plt.figure(figsize=(12,4))\n","plt.plot(range(n_emb),activations['qkv'][0,5,:n_emb],'ks',markerfacecolor=[.7,.7,.9,.5],label='Q')\n","plt.plot(\n","plt.plot(\n","\n","plt.legend()\n","plt.gca().set(xlim=[-5,n_emb*3+4],xlabel='Index into QKV matrix',ylabel='Activation value',\n","              title=f'Activations to the token \"{tokenizer.decode(tokens[0,5])}\" in layer {layer2modify}')\n","plt.show()"],"metadata":{"id":"7xKZGxn9OfBr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove the hook"],"metadata":{"id":"jKDHMgzrOe-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KzMFRZr6Oe7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Replace even-indexed MLP neurons with noise"],"metadata":{"id":"dRfq8s4JOGBh"}},{"cell_type":"code","source":["# initialize activations dictionary\n","activations = {}\n","\n","def hook(module, input, output):\n","\n","  # create random noise of the same size\n","  noise =\n","\n","  # Note: Because the modification is done directly on the tensor and not on a view of it,\n","  #       you can edit it in-place as shown below. Making a copy (as in the video) is also fine :D\n","\n","  # replace\n","  output\n","\n","  # store the activations\n","  activations['mlp'] =\n","\n","  # and return the modified version\n","  return output\n","\n","\n","hookHandle = .register_forward_hook(hook)"],"metadata":{"id":"ZRvuN0aaOF-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm\n","model(tokens)\n","\n","plt.figure(figsize=(12,4))\n"],"metadata":{"id":"6RUjo9keOfa-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hookHandle.remove()"],"metadata":{"id":"eeI28AOIOfX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELMdKsLDN9Ds"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Exercise 3: Scale the hidden-state activations"],"metadata":{"id":"8YGK2WUaOWAe"}},{"cell_type":"code","source":["# scaling factor\n","scaling_factor =\n","\n","def hook(module, input, output):\n","\n","  # extract the hidden states\n","  hs =\n","\n","  # scaling via matrix-scalar multiplication\n","\n","\n","  # reconstruct and output\n","  return (hs,*output[1:])\n","\n","hookHandle = model.transformer.h[8].register_forward_hook(hook)"],"metadata":{"id":"tu2mnCz4OV93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm\n","out = model(tokens,output_hidden_states=True)\n","\n","hs = out.hidden_states\n","print(f'There are    hidden_states.')\n","print(f'Each hidden state is of size"],"metadata":{"id":"WlaHvncoOV6-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","for i in range(len(hs)):\n","\n","  # data from this transformer block for one token\n","  thisBlock = .numpy()\n","\n","  # plot all the data\n","  axs[0].plot(\n","\n","  # plot the norm\n","  axs[1].plot(\n","\n","axs[0].set(xlabel='Hidden state layer',ylabel='Activation value',title='Hidden state activations for token #4')\n","axs[1].set(xlabel='Hidden state layer',ylabel='Matrix norm',title='Hidden state norms from token #4')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Sv2d4OZ0eltj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t6whsx1B2LF3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Scale up"],"metadata":{"id":"lwRQj5zp2LCy"}},{"cell_type":"code","source":["# now scale by 10x\n","scaling_factor = 10\n","out = model(tokens\n","hs = out.hidden_\n","\n","\n","_,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","for i in range(len(hs)):\n","\n","  # data from this transformer block for one token\n","  thisBlock = hs\n","\n","  # plot all the data\n","  axs[0].plot(np.ones(n_emb\n","\n","  # plot the norm\n","  axs[1].plot(i,\n","\n","axs[0].set(xlabel='Hidden state layer',ylabel='Activation value',title='Hidden state activations')\n","axs[1].set(xlabel='Hidden state layer',ylabel='Vector norm',title='Hidden state norms')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"3dXKF7ZgnCZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hookHandle.remove()"],"metadata":{"id":"gwhhvE8XOV1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZpRoyj9qC04p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bonus! Example of output variable with more than just transformer outputs"],"metadata":{"id":"V5XXitzCC02B"}},{"cell_type":"code","source":["# just a hook to print\n","\n","def hook(module, input, output):\n","\n","  # print info about the output variable\n","  print(f'output is type {type(output)} and has {len(output)} element(s).')\n","\n","  # info about each element of output\n","  for i in range(len(output)):\n","    print(f'Element {i} has size {list(output[i].shape)}')\n","\n","hookHandle = model.transformer.h[8].register_forward_hook(hook)"],"metadata":{"id":"rUOJuMlPC0zX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = [ 'Here is the first sentence', 'Here is another one of a different length.', 'Shall we go for three?' ]\n","tokenizer.pad_token = tokenizer.eos_token\n","tokens = tokenizer(text,padding=True,return_tensors='pt')\n","tokens"],"metadata":{"id":"RUScdgLUC0wk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model(**tokens);"],"metadata":{"id":"Gqkz5J7uC0tz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.config._attn_implementation = 'eager'\n","model.config.output_attentions = True\n","model.config.output_hidden_states = True\n","model(**tokens);"],"metadata":{"id":"WaDk5oSkC0q-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sRlT4P-2GJix"},"execution_count":null,"outputs":[]}]}