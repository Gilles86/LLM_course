{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPeKeJWjJRzhaDkIy8RSzDc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 6:</h2>|<h1>Intervention (causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>How to modify activations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: replacing attention, MLP, and hidden states<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"_gPy1MwYgrhi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPsr5B0nv52"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model.eval()"]},{"cell_type":"code","source":[],"metadata":{"id":"ojMdlVejOGQB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Zero-out the third attention head in K"],"metadata":{"id":"qMS9TnAsOGLq"}},{"cell_type":"code","source":["tokens = tokenizer.encode('I wonder how many tokens are in pomegranate.',return_tensors='pt')\n","\n","for t in tokens[0]:\n","  print(f'Token index {t:5} is \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"-Uui0C85R3Lf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some useful variables\n","nheads = model.config.n_head\n","n_emb = model.config.n_embd\n","head_dim = model.config.n_embd // nheads\n","\n","# find the start and end index of the 3rd head\n","whichHead_idx = 2\n","h3_start = whichHead_idx*head_dim\n","h3_end = h3_start + head_dim\n","\n","print(f'Attention head {whichHead_idx+1} starts at \\nindex {h3_start} and ends at index {h3_end-1}.')"],"metadata":{"id":"z_Had9RjTTOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize activations dictionary\n","activations = {}\n","\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # split the output into QKV (each is [B,S,H])\n","    q,k,v = output.split(n_emb,dim=2)\n","\n","    # make an editable copy of k vectors\n","    k_copy = k.clone()\n","\n","    # zero-out the data\n","    k_copy[:,:,h3_start:h3_end] = 0\n","\n","    # recombine q with modified k and v\n","    QKV = torch.cat([q,k_copy,v],dim=2)\n","\n","    # store the activations\n","    activations['qkv'] = QKV.detach().numpy()\n","\n","    # output the QKV matrix so it replaces the original\n","    return QKV\n","\n","  return hook\n","\n","\n","layer2modify = 3\n","hookHandle = model.transformer.h[layer2modify].attn.c_attn.register_forward_hook(implant_hook(layer2modify))"],"metadata":{"id":"TXLPspZpOGIg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm\n","model(tokens)\n","\n","plt.figure(figsize=(12,4))\n","plt.plot(range(n_emb),activations['qkv'][0,5,:n_emb],'ks',markerfacecolor=[.7,.7,.9,.5],label='Q')\n","plt.plot(range(n_emb,2*n_emb),activations['qkv'][0,5,n_emb:n_emb*2],'ks',markerfacecolor=[.7,.9,.7,.5],label='K')\n","plt.plot(range(2*n_emb,3*n_emb),activations['qkv'][0,5,n_emb*2:],'ks',markerfacecolor=[.9,.7,.7,.5],label='V')\n","\n","plt.legend()\n","plt.gca().set(xlim=[-5,n_emb*3+4],xlabel='Index into QKV matrix',ylabel='Activation value',\n","              title=f'Activations to the token \"{tokenizer.decode(tokens[0,5])}\" in layer {layer2modify}')\n","plt.show()"],"metadata":{"id":"7xKZGxn9OfBr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hookHandle.remove()"],"metadata":{"id":"jKDHMgzrOe-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KzMFRZr6Oe7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Replace even-indexed MLP neurons with noise"],"metadata":{"id":"dRfq8s4JOGBh"}},{"cell_type":"code","source":["# initialize activations dictionary\n","activations = {}\n","\n","def hook(module, input, output):\n","\n","  # create random noise of the same size\n","  noise = torch.randn_like(output[:,4,::2]) + 10\n","\n","  # Note: Because the modification is done directly on the tensor and not on a view of it,\n","  #       you can edit it in-place as shown below. Making a copy (as in the video) is also fine :D\n","\n","  # replace\n","  output[:,4,::2] = noise\n","\n","  # store the activations\n","  activations['mlp'] = output.detach().numpy()\n","\n","  # and return the modified version\n","  return output\n","\n","\n","hookHandle = model.transformer.h[5].mlp.c_fc.register_forward_hook(hook)"],"metadata":{"id":"ZRvuN0aaOF-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm\n","model(tokens)\n","\n","plt.figure(figsize=(12,4))\n","plt.plot(activations['mlp'][0,4,:],'ks',markerfacecolor=[.7,.7,.9,.5])\n","\n","plt.gca().set(xlim=[-5,activations['mlp'].shape[-1]+4],xlabel='Index into MLP expansion',ylabel='Activation value',\n","              title=f'Activations to the token \"{tokenizer.decode(tokens[0,4])}\" in layer 5')\n","plt.show()"],"metadata":{"id":"6RUjo9keOfa-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hookHandle.remove()"],"metadata":{"id":"eeI28AOIOfX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELMdKsLDN9Ds"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Exercise 3: Scale the hidden-state activations"],"metadata":{"id":"8YGK2WUaOWAe"}},{"cell_type":"code","source":["# scaling factor\n","scaling_factor = .1\n","\n","def hook(module, input, output):\n","\n","  # extract the hidden states\n","  hs = output[0]\n","\n","  # scaling via matrix-scalar multiplication\n","  hs.mul_(scaling_factor)\n","\n","  # reconstruct and output\n","  return (hs,*output[1:])\n","\n","# note: it's not necessary to create a separate variable hs; you could also use:\n","# > output[0].mul_(scaling_factor)\n","# > return output\n","\n","hookHandle = model.transformer.h[8].register_forward_hook(hook)"],"metadata":{"id":"tu2mnCz4OV93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm\n","out = model(tokens,output_hidden_states=True)\n","\n","hs = out.hidden_states\n","print(f'There are {len(hs)} hidden_states.')\n","print(f'Each hidden state is of size {list(hs[3].shape)}')"],"metadata":{"id":"WlaHvncoOV6-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","for i in range(len(hs)):\n","\n","  # data from this transformer block for one token\n","  thisBlock = hs[i][0,4,:].detach().numpy()\n","\n","  # plot all the data\n","  axs[0].plot(np.ones(n_emb)*i,thisBlock,'ks',markerfacecolor=mpl.cm.plasma(i/len(hs)))\n","\n","  # plot the norm\n","  axs[1].plot(i,np.linalg.norm(thisBlock),'ks',markerfacecolor=mpl.cm.plasma(i/len(hs)))\n","\n","axs[0].set(xlabel='Hidden state layer',ylabel='Activation value',title='Hidden state activations for token #4')\n","axs[1].set(xlabel='Hidden state layer',ylabel='Matrix norm',title='Hidden state norms from token #4')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Sv2d4OZ0eltj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t6whsx1B2LF3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Scale up"],"metadata":{"id":"lwRQj5zp2LCy"}},{"cell_type":"code","source":["# now scale by 10x\n","scaling_factor = 10\n","out = model(tokens,output_hidden_states=True)\n","hs = out.hidden_states\n","\n","\n","_,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","for i in range(len(hs)):\n","\n","  # data from this transformer block for one token\n","  thisBlock = hs[i][0,4,:].detach().numpy()\n","\n","  # plot all the data\n","  axs[0].plot(np.ones(n_emb)*i,thisBlock,'ks',markerfacecolor=mpl.cm.plasma(i/len(hs)))\n","\n","  # plot the norm\n","  axs[1].plot(i,np.linalg.norm(thisBlock),'ks',markerfacecolor=mpl.cm.plasma(i/len(hs)))\n","\n","axs[0].set(xlabel='Hidden state layer',ylabel='Activation value',title='Hidden state activations')\n","axs[1].set(xlabel='Hidden state layer',ylabel='Vector norm',title='Hidden state norms')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"3dXKF7ZgnCZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hookHandle.remove()"],"metadata":{"id":"gwhhvE8XOV1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZpRoyj9qC04p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bonus! Example of output variable with more than just transformer outputs"],"metadata":{"id":"V5XXitzCC02B"}},{"cell_type":"code","source":["# just a hook to print\n","\n","def hook(module, input, output):\n","\n","  # print info about the output variable\n","  print(f'output is type {type(output)} and has {len(output)} element(s).')\n","\n","  # info about each element of output\n","  for i in range(len(output)):\n","    print(f'Element {i} has size {list(output[i].shape)}')\n","\n","hookHandle = model.transformer.h[8].register_forward_hook(hook)"],"metadata":{"id":"rUOJuMlPC0zX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = [ 'Here is the first sentence', 'Here is another one of a different length.', 'Shall we go for three?' ]\n","tokenizer.pad_token = tokenizer.eos_token\n","tokens = tokenizer(text,padding=True,return_tensors='pt')\n","tokens"],"metadata":{"id":"RUScdgLUC0wk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model(**tokens);"],"metadata":{"id":"Gqkz5J7uC0tz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.config._attn_implementation = 'eager'\n","model.config.output_attentions = True\n","model.config.output_hidden_states = True\n","model(**tokens);"],"metadata":{"id":"WaDk5oSkC0q-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sRlT4P-2GJix"},"execution_count":null,"outputs":[]}]}