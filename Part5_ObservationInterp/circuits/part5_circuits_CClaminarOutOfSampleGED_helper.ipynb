{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOBfW8f7CM6ON+fv2qqeRex"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifying latent factors<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: GED for category isolation across layers<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"dVdgcMR4IljT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuKeB769HOkN"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.linalg\n","import scipy.stats as stats\n","from sklearn.decomposition import PCA\n","\n","from tqdm import tqdm # for progress bar\n","\n","import torch\n","from transformers import GPT2Model, GPT2Tokenizer\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["model = GPT2Model.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"CDk2YLwfsz1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jqTjLQ3NrqHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Tokenize the training and test texts"],"metadata":{"id":"YI_EkZGRHaXr"}},{"cell_type":"code","source":["# generated by Claude.ai\n","sentences = [\n","    \"I saw him at the market.\",\n","    \"She gave him the book.\",\n","    \"They asked him for advice.\",\n","    \"We invited him to dinner.\",\n","    \"The dog followed him home.\",\n","    \"They asked him to join.\",\n","    \"He saw him at the park yesterday.\",\n","    \"Did you give him your address?\",\n","    \"I haven't seen him in ages.\",\n","    \"I told him the truth.\",\n","    \"They congratulated him on his success.\",\n","    \"She recognized him immediately.\",\n","    \"The teacher praised him for his work.\",\n","    \"I met him last summer.\",\n","    \"The child hugged him tightly.\",\n","    \"They warned him about the danger.\",\n","    \"She drove him to the airport.\",\n","    \"We waited for him for hours.\",\n","    \"The cat scratched him accidentally.\",\n","    \"They surprised him with a gift.\",\n","    \"She called him on the phone.\",\n","    \"The jury found him not guilty.\",\n","    \"I remembered him from school.\",\n","    \"They elected him as president.\",\n","    \"She forgave him for his mistake.\",\n","    \"The police questioned him yesterday.\",\n","    \"I helped him with his homework.\",\n","    \"They spotted him in the crowd.\",\n","    \"She visited him in the hospital.\",\n","    \"The manager promoted him last week.\",\n","    \"I trusted him completely.\",\n","    \"They respected him for his honesty.\",\n","    \"She taught him how to swim.\",\n","    \"The bird attacked him suddenly.\",\n","    \"I greeted him warmly.\",\n","    \"They supported him through difficult times.\",\n","    \"She ignored him at the party.\",\n","    \"The judge sentenced him to community service.\",\n","    \"I photographed him during the event.\",\n","    \"They believed him despite the evidence.\",\n","    \"She surprised him on his birthday.\",\n","    \"The guard stopped him at the entrance.\",\n","    \"I missed him terribly.\",\n","    \"They watched him leave the building.\",\n","    \"She accompanied him to the concert.\",\n","    \"The crowd cheered him enthusiastically.\",\n","    \"I described him to the police.\",\n","    \"They thanked him for his help.\",\n","    \"She admired him for his courage.\",\n","    \"The committee nominated him for the award.\",\n","    \"I married him last spring.\",\n","    \"They informed him about the changes.\",\n","    \"She introduced him to the parents.\",\n","    \"The author based the character on him.\",\n","\n","## same sentences but with \"her\"\n","\n","    \"I saw her at the market.\",\n","    \"She gave her the book.\",\n","    \"They asked her for advice.\",\n","    \"We invited her to dinner.\",\n","    \"The dog followed her home.\",\n","    \"They asked her to join.\",\n","    \"He saw her at the park yesterday.\",\n","    \"Did you give her your address?\",\n","    \"I haven't seen her in ages.\",\n","    \"I told her the truth.\",\n","    \"They congratulated her on his success.\",\n","    \"She recognized her immediately.\",\n","    \"The teacher praised her for his work.\",\n","    \"I met her last summer.\",\n","    \"The child hugged her tightly.\",\n","    \"They warned her about the danger.\",\n","    \"She drove her to the airport.\",\n","    \"We waited for her for hours.\",\n","    \"The cat scratched her accidentally.\",\n","    \"They surprised her with a gift.\",\n","    \"She called her on the phone.\",\n","    \"The jury found her not guilty.\",\n","    \"I remembered her from school.\",\n","    \"They elected her as president.\",\n","    \"She forgave her for his mistake.\",\n","    \"The police questioned her yesterday.\",\n","    \"I helped her with his homework.\",\n","    \"They spotted her in the crowd.\",\n","    \"She visited her in the hospital.\",\n","    \"The manager promoted her last week.\",\n","    \"I trusted her completely.\",\n","    \"They respected her for his honesty.\",\n","    \"She taught her how to swim.\",\n","    \"The bird attacked her suddenly.\",\n","    \"I greeted her warmly.\",\n","    \"They supported her through difficult times.\",\n","    \"She ignored her at the party.\",\n","    \"The judge sentenced her to community service.\",\n","    \"I photographed her during the event.\",\n","    \"They believed her despite the evidence.\",\n","    \"She surprised her on his birthday.\",\n","    \"The guard stopped her at the entrance.\",\n","    \"I missed her terribly.\",\n","    \"They watched her leave the building.\",\n","    \"She accompanied her to the concert.\",\n","    \"The crowd cheered her enthusiastically.\",\n","    \"I described her to the police.\",\n","    \"They thanked her for his help.\",\n","    \"She admired her for his courage.\",\n","    \"The committee nominated her for the award.\",\n","    \"I married her last spring.\",\n","    \"They informed her about the changes.\",\n","    \"She introduced her to the parents.\",\n","    \"The author based the character on her.\"\n","]\n","\n","# indices of him/her sentences\n","him_sentences = np.arange(len(sentences)//2)\n","her_sentences = np.arange(len(sentences)//2,len(sentences))\n","\n","print(f'There are {len(sentences)} sentences.')"],"metadata":{"id":"oqdAP54II-ht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# identify the target token\n","target_token_him =\n","target_token_her =\n","print(f'The target token indices are {target_token_him} and {target_token_her}\\n')\n","\n","# need to specify a padding token\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# tokenize\n","train_tokens ="],"metadata":{"id":"fYPkVigeOv47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5satY3LpzTc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggclU75JnzrI"},"outputs":[],"source":["# import fineweb, look through tokens, find 50 \"him\" and 50 \"her\", take tokens before and after for context\n","!pip install datatrove\n","from datatrove.pipeline.readers import ParquetReader\n","\n","numDocs = 2000 # how many documents to retrive; each doc has ~750 tokens\n","data_reader = ParquetReader('hf://datasets/HuggingFaceFW/fineweb/data',limit=numDocs)\n","\n","# join all texts into one token vector\n","tokens = np.array([],dtype=int)\n","for t in data_reader():\n","  tokens = np.append(tokens,tokenizer.encode(t.text))"]},{"cell_type":"code","source":["# find all \"him\" and \"her\" tokens\n","him_tokens = np.where(\n","her_tokens = np.where(\n","\n","# let's see a few, in context\n","for i in him_tokens[:10]:\n","  print(tokenizer.deco"],"metadata":{"id":"1xaJG5P9l8zq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a batch\n","batch = np.zeros((,),dtype=int)\n","for i in range(50):\n","  batch[i,:] = tokens\n","\n","  # \"her\" tokens have source index offset to avoid risk of using the same sentence\n","  batch[50+i,:] = tokens\n","\n","\n","# create a tokens dictionary\n","test_tokens = {}\n","test_tokens['input_ids'] =\n","test_tokens['attention_mask'] =\n","\n","test_tokens['input_ids'].shape"],"metadata":{"id":"c7GK2Kk9zTVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and show a few examples\n","for i in [16,34,60,90]:\n","  print"],"metadata":{"id":"3bVj1FA2Cdk1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TOqCVnGDm7Kc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Get model activations for train and test"],"metadata":{"id":"y-Qvc-PDmPU3"}},{"cell_type":"code","source":["# hook function to store MLP attention vectors\n","activations = {}\n","def implant_hook_mlp(layer_number):\n","\n","# hook it!\n"],"metadata":{"id":"3kRaud4VHY6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TRAIN tokens to define GED vectors\n","with torch.no_grad(): model(**train_tokens)\n","train_activations = # make a copy of the activations\n","\n","# TEST tokens to evaluate out-of-sample performance\n"],"metadata":{"id":"YAMzPriHL7SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm dataset sizes\n","print('Train activations are size:',train_activations['mlp_5'].shape)\n","print('Test activations are size:',test_activations['mlp_5'].shape)"],"metadata":{"id":"7oyHrp_QwIa8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vJQpq9v626e5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: A function for PCA"],"metadata":{"id":"cSyzdEUxvS1j"}},{"cell_type":"code","source":["def dimred_PCA(layerNum):\n","\n","  # --- get target activations\n","  trainacts = np.zeros((len(sentences),train_activations[f'mlp_{layerNum}'].shape[-1]))\n","\n","  for senti in range(len(sentences)):\n","\n","    # find the index of either of the target tokens\n","    targBool =\n","    targidx = np.where(targBool)[0]\n","\n","    # then get the activation\n","    trainacts[senti,:] =\n","\n","\n","  # FYI, manual approach, which is considerably slower than sklearn.\n","  # # --- PCA via eigendecomposition of covariance matrix\n","  # d,PCA_eigvecs = scipy.linalg.eigh( np.cov(trainacts.T) )\n","\n","  # # sort the values and vectors\n","  # idx = d.argsort()[::-1]\n","  # d = np.real(d[idx])\n","  # PCA_eigvecs = np.real(PCA_eigvecs[:,idx]) # sort the columns, not the rows!\n","\n","  # # transform the eigenvalues to cumulative % variance explained\n","  # varExplained = d*100/np.sum(d)\n","  # cumVarExplained = np.cumsum(varExplained)\n","\n","\n","  # --- PCA via sklearn's PCA()\n","  pca = PCA().fit(trainacts)\n","  cumVarExplained = # hint: use pca.explained_variance_ratio_\n","  PCA_eigvecs = pca.components_.T # pca.components_ have eigenvectors in the rows\n","\n","\n","\n","\n","  # how many components to explain 99% of the variability?\n","  numComps2keep =\n","\n","  # --- output the activations, eigenvectors, and number of components to keep\n","  return trainacts,PCA_eigvecs,numComps2keep"],"metadata":{"id":"XdNJjcUrrgwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UiIY5vhDwPiN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: A function for GED"],"metadata":{"id":"OjGVQSvwwPfe"}},{"cell_type":"code","source":["def sourcesep_GED_train(layerNum):\n","\n","  # --- create the regularized covariance matrices\n","\n","  # compress down to numComps2keep dimensions\n","  lowD_acts = trainacts @ PCA_eigvecs # only the first 'numComps2keep' column vectors\n","\n","  # covariance matrices\n","  himProjcov = np.cov(lowD_acts # only for the 'him' sentences\n","  herProjcov = np.cov(lowD_acts # only for the 'her' sentences. check the size of the covariance matrices!\n","\n","  # regularization\n","  regu_gam = .01\n","  himProjcovS = (1-regu_gam)*himProjcov +\n","  herProjcovS =\n","\n","\n","  # --- generalized eigendecomposition\n","  #     (note: using regularized matrices only for \"R\" (second input))\n","\n","  # HIM > HER: eig and sort\n","  evalsHim,evecsHim = scipy.linalg.eigh(,)\n","  idx = evalsHim.argsort()[::-1]\n","  evalsHim = np.real(evalsHim[idx])\n","  evecsHim = np.real() # sort the columns, not the rows!\n","\n","  # HER > HIM\n","\n","\n","  # project the data onto the top GED vectors\n","  ged_proj_him = lowD_acts @ # the first (sorted) 'him' eigenvector\n","  ged_proj_her = lowD_acts @\n","\n","  # find the pattern in the covariance matrix (through the PC vectors to return to MLP space)\n","  mlpPattern_him = PCA_eigvecs[:,:numComps2keep] @ himProjcov @ evecsHim[:,0]\n","  mlpPattern_her =\n","\n","  r = stats.pearsonr(,)\n","\n","  # --- output the projections, vectors, and correlations\n","  return ged_proj_him,ged_proj_her,evecsHim,evecsHer,r,evalsHim[0],evalsHer[0]"],"metadata":{"id":"0vtwlKfPwOBg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BfxOsO0AMDRz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Loop over layers"],"metadata":{"id":"kHSLvAqvwIYI"}},{"cell_type":"code","source":["# initialize a results matrix\n","ged_results = np.zeros((numLayers,7,2))\n","\n","\n","# loop over layers\n","for layeri in tqdm(range(numLayers),desc='Transformer blocks'):\n","\n","\n","  ### --- training data to get GED vectors\n","  # PCA\n","  trainacts,PCA_eigvecs,numComps2keep =\n","\n","  # GED\n","  ged_proj_him,ged_proj_her,evecsHim,evecsHer,r,evHim,evHer =\n","\n","  # statistical results and their p-values\n","  tres = stats.ttest_ind(ged_proj_him[him_sentences],ged_proj_him[her_sentences])\n","  ged_results[layeri,0,0] = # |t|\n","  ged_results[layeri,0,1] = # p-value\n","\n","  tres = stats.ttest_ind( # same as above but for 'her'\n","  ged_results[layeri,1,0] =\n","  ged_results[layeri,1,1] =\n","\n","  ged_results[layeri,2,0] = # |r|\n","  ged_results[layeri,2,1] = # p-value\n","\n","  # top eigenvalues\n","  ged_results[layeri,5,0] =\n","  ged_results[layeri,5,1] =\n","\n","  # number of components to reach 99% variance explained\n","  ged_results[layeri,6,0] =\n","\n","\n","  ### --- out-of-sample evaluation\n","  # get target activations (target token is always in index 5)\n","  testacts =\n","\n","  # project down to numComps2keep dimensions\n","  actsProj = testacts @\n","\n","  # then project onto the top GED vectors\n","  ged_proj_him_test = actsProj @\n","  ged_proj_her_test =\n","\n","  # ttests\n","  tres = stats.ttest_ind(ged_proj_him_test[asdf],ged_proj_him_test[asdf])\n","  ged_results[layeri,3,0] = # |t|\n","  ged_results[layeri,3,1] = # p-value\n","\n","  tres = stats.ttest_ind( # same as above but for 'her'\n","  ged_results[layeri,4,0] =\n","  ged_results[layeri,4,1] =\n"],"metadata":{"id":"Ak_1dhTlzlNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","plt.plot(,'kh-',markerfacecolor=[.9,.7,.9],linewidth=.5,markersize=12)\n","\n","plt.gca().set(xlabel='Transformer layer',ylabel='Count',xlim=[-.5,numLayers-.5],\n","              title='Number of components to explain 99% variability')\n","\n","plt.show()"],"metadata":{"id":"mE8GQwFqkRDm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kQT1ysiYmR0m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 6: Visualize GED results"],"metadata":{"id":"iwL178KXcTpm"}},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","xtix = np.arange(1,numLayers+1)\n","pvalthresh = # Bonferroni-corrected for n layers\n","\n","# TRAIN him > her\n","p = ged_results[:,0,1]\n","axs[0].plot(xtix,,color=[.7,.7,.9])\n","axs[0].plot(xtix,,'ko',markerfacecolor=[.7,.7,.9],linewidth=.5,label='TRAIN him>her')\n","axs[0].plot(xtix,,'rx')\n","\n","# TRAIN her > him\n","p = ged_results[:,1,1]\n","\n","\n","# TEST him > her\n","p = ged_results[:,3,1]\n","\n","\n","# TEST her > him\n","p = ged_results[:,4,1]\n","\n","axs[0].legend(ncols=2)\n","axs[0].set(ylim=[0,50],xlabel='Layer',ylabel='|t| value',title='Magnitude of t-tests')\n","\n","# correlation\n","p = ged_results[:,2,1]\n","axs[1].plot(xtix,ged_results[:,2,0],color=[.9,.9,.9])\n","\n","axs[1].set(xlabel='Layer',ylabel='|r| value',title='Correlation between \"him\" and \"her\" patterns')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Zv7jSKRpWvU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","# top eigenvalues for him > her\n","plt.plot(,label='him > her')\n","\n","# top eigenvalues for her > him\n","plt.plot(,label='her > him')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Transformer layer',ylabel='Eigenvalue magnitude',xlim=[-.5,numLayers-.5],\n","              title='Largest eigenvalue from GEDs')\n","\n","plt.show()"],"metadata":{"id":"e_o4g-quwIVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"R-pIz-rqEasp"},"execution_count":null,"outputs":[]}]}