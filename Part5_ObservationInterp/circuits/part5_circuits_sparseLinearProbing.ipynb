{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM0/Jy+KXAdRHHMdBifugvF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifying circuits and components<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Sparse linear probing: theory and code<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"vlCAhKXwOThX"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","# functions for implementing and evaluating the logistic regression\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"U-oRuiQi7TR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import gpt and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model     = AutoModelForCausalLM.from_pretrained('gpt2')\n","model.eval()"],"metadata":{"id":"Kx2tBTUW90Z-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook the post-GELU MLP activations\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    activations[f'mlp_{layer_number}'] = output.detach().numpy()\n","  return hook\n","\n","# hook the MLP post-gelu activations\n","layer2hook = 3\n","model.transformer.h[layer2hook].mlp.act.register_forward_hook(implant_hook(layer2hook))"],"metadata":{"id":"PasFDGjDPwFw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uDRjlO419-lP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The data"],"metadata":{"id":"igImUWg29-e3"}},{"cell_type":"code","source":["# import fineweb\n","!pip install datatrove\n","from datatrove.pipeline.readers import ParquetReader\n","\n","# get some data\n","numDocs = 500 # how many documents to retrive; each doc has ~750 tokens\n","data_reader = ParquetReader('hf://datasets/HuggingFaceFW/fineweb/data',limit=numDocs)\n","\n","# join all texts into one token vector\n","tokens = np.array([],dtype=int)\n","for t in data_reader():\n","  tokens = np.append(tokens,tokenizer.encode(t.text))"],"metadata":{"id":"0bjt-l1u_1lE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find all the \"the\" and \"an\" token indices\n","the_tokens = np.where(tokens==tokenizer.encode(' the'))[0]\n","an_tokens = np.where(tokens==tokenizer.encode(' an'))[0]\n","\n","len(the_tokens),len(an_tokens)"],"metadata":{"id":"UZrAWYkn_614"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0OeX7PkGVTiy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create batches\n","samplesize = 100\n","\n","context_pre = 14\n","context_pst =  3\n","\n","the_batch = np.zeros((samplesize,context_pre+context_pst),dtype=int)\n","an_batch  = np.zeros((samplesize,context_pre+context_pst),dtype=int)\n","\n","# loop through \"the\" tokens and make sure they are whole words\n","i = 0\n","for thei in the_tokens:\n","  # it's a whole word if the next token starts with a space\n","  if (tokenizer.decode(tokens[thei+1])[0]==' ') & (i<samplesize):\n","    the_batch[i,:] = tokens[thei-context_pre:thei+context_pst]\n","    i += 1\n","\n","# repeat for \"an\"\n","i = 0\n","for ani in an_tokens:\n","  if (tokenizer.decode(tokens[ani+1])[0]==' ') & (i<samplesize):\n","    an_batch[i,:] = tokens[ani-context_pre:ani+context_pst]\n","    i += 1"],"metadata":{"id":"7adS3EQF_6y4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some examples\n","print('THE sequences')\n","for i in np.random.randint(0,samplesize,7):\n","  print('::',tokenizer.decode(the_batch[i]))\n","\n","print('\\n\\nAN sequences')\n","for i in np.random.randint(0,samplesize,7):\n","  print('::',tokenizer.decode(an_batch[i]))"],"metadata":{"id":"eNlpsRQcQQ2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yzXDCBacfixf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FYI, more about preceeding spaces"],"metadata":{"id":"a-ZrGbOjOyQT"}},{"cell_type":"code","source":["t1 = tokenizer.encode('the',add_prefix_space=True)\n","print(\"('the',add_prefix_space=True):\")\n","print(f'  token {t1[0]} -> \"{tokenizer.decode(t1)}\"')\n","\n","t2 = tokenizer.encode('the',add_prefix_space=False)\n","print(\"\\n('the',add_prefix_space=False):\")\n","print(f'  token {t2[0]} -> \"{tokenizer.decode(t2)}\"')\n","\n","t3 = tokenizer.encode(' the',add_prefix_space=True)\n","print(\"\\n(' the',add_prefix_space=True):\")\n","print(f'  token {t3[0]} -> \"{tokenizer.decode(t3)}\"')\n","\n","t4 = tokenizer.encode(' the',add_prefix_space=False)\n","print(\"\\n(' the',add_prefix_space=False):\")\n","print(f'  token {t4[0]} -> \"{tokenizer.decode(t4)}\"')"],"metadata":{"id":"H_cVMuF-I9D9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GxeGaBS-O82c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the dataset for the logistic regression (linear probe)"],"metadata":{"id":"fQaasoalYTtK"}},{"cell_type":"code","source":["# forward passes\n","with torch.no_grad(): model(torch.tensor(the_batch))\n","the_activations = activations[f'mlp_{layer2hook}'][:,context_pre,:]\n","\n","with torch.no_grad(): model(torch.tensor(an_batch))\n","an_activations = activations[f'mlp_{layer2hook}'][:,context_pre,:]\n","\n","# number of neurons\n","nneurons = the_activations.shape[-1]\n","\n","# examine the sizes\n","print('THE activations have size',the_activations.shape)\n","print('AN activations have size',an_activations.shape)"],"metadata":{"id":"PDrZcMid96aF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# histograms of all activations for all neurons\n","yThe,xThe = np.histogram(the_activations.flatten(),100)\n","yAn, xAn  = np.histogram(an_activations.flatten(),100)\n","\n","plt.figure(figsize=(10,4))\n","plt.plot(xThe[:-1],yThe,linewidth=2,label='The')\n","plt.plot(xAn[:-1], yAn,linewidth=2,label='An')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Activation',ylabel='Count',yscale='log')\n","plt.show()"],"metadata":{"id":"sBfvI3bdRaHU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ov-Z6yM-RaEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data and labels\n","alldata = np.concatenate((the_activations,an_activations),axis=0)\n","labels = np.array([0]*samplesize + [1]*samplesize)\n","\n","# split into train/test (70/30%)\n","X_train, X_test, y_train, y_test = train_test_split(alldata, labels, test_size=.3, stratify=labels)\n","\n","# print sizes\n","print('Size of train data:',X_train.shape)\n","print('Size of test data:', X_test.shape)"],"metadata":{"id":"xkS5X80ZTxiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Pr0GvhiFW5Jk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The logistic regression"],"metadata":{"id":"BYRb7wW6W5Gw"}},{"cell_type":"code","source":["# fit the model (C is 1/lambda)\n","logreg = LogisticRegression(penalty='l1', max_iter=1000, solver='saga', C=10)\n","logreg.fit(X_train,y_train)\n","\n","# generate predictions\n","y_pred = logreg.predict(X_test)\n","print(classification_report(y_test, y_pred, target_names=['the','an']))"],"metadata":{"id":"t77LmUNt-Mtk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reminder:\n","#   sparsity: N(zero coeffs) / N(coeffs)\n","#   density:  N(nonzero coeffs) / N(coeffs)"],"metadata":{"id":"fJ9Wpx7Tn6T8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract the coefficients and find the zeros\n","coeffs = logreg.coef_.squeeze()\n","zeroCoeffs = np.where(coeffs==0)[0]\n","nonzeroCoeffs = np.where(coeffs!=0)[0]\n","\n","\n","# some visualizations\n","fig = plt.figure(figsize=(12,4))\n","gs  = GridSpec(1,3,figure=fig)\n","ax1 = fig.add_subplot(gs[:2])\n","ax2 = fig.add_subplot(gs[2])\n","\n","\n","# plot the non-zero coefficients\n","ax1.plot(nonzeroCoeffs,coeffs[nonzeroCoeffs],'ko',markerfacecolor=[.7,.9,.7,.7],label='Non-zero')\n","ax1.plot(zeroCoeffs,coeffs[zeroCoeffs],'rx',zorder=-3,label='Exact zero')\n","ax1.legend()\n","ax1.set(xlabel='MLP neuron',ylabel='Coefficient',xlim=[-5,nneurons+4],\n","        title=f'L1 probe selected {len(nonzeroCoeffs)}/{nneurons} coefficients ({100*len(nonzeroCoeffs)/nneurons:.2f}% dense)')\n","\n","# and the distribution of non-zero coeffs\n","ax2.hist(coeffs[nonzeroCoeffs],bins=30,color=[.7,.7,.9],edgecolor='k')\n","ax2.set(xlabel='Coefficient value',ylabel='Count',title='Distribution of non-zero coefficients')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"HqcO1DKKYfq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","\n","for i,nidx in enumerate(nonzeroCoeffs):\n","  y = alldata[labels==0,nidx]\n","  plt.plot(np.random.randn(len(y))/50+i-.15,y,'ko',markerfacecolor=[.9,.7,.7,.4])\n","\n","  y = alldata[labels==1,nidx]\n","  plt.plot(np.random.randn(len(y))/50+i+.15,y,'ks',markerfacecolor=[.7,.9,.7,.4])\n","\n","\n","# hacky legend solution\n","plt.plot(i*3,1,'ko',markerfacecolor=[.9,.7,.7],label='\"The\"')\n","plt.plot(i*3,1,'ks',markerfacecolor=[.7,.9,.7],label='\"An\"')\n","plt.legend()\n","\n","plt.gca().set(xlabel='Neurons with non-zero coefficients (index)',ylabel='Activations',\n","              xticks=range(i+1),xlim=[-1,i+1])\n","plt.show()"],"metadata":{"id":"O_CFf3-ugNdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"44nGzgsFd6bI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Relation between sparsity and lambda (1/C)"],"metadata":{"id":"BxGYd3jfDmUK"}},{"cell_type":"code","source":["seas = np.linspace(1,100,17)\n","densities = np.zeros(len(seas))\n","\n","for idx,c in enumerate(seas):\n","\n","  logreg = LogisticRegression(penalty='l1', max_iter=1000, solver='saga', C=c)\n","  logreg.fit(X_train,y_train)\n","\n","  coeffs = logreg.coef_.squeeze()\n","  densities[idx] = 100 * (coeffs!=0).sum()/nneurons"],"metadata":{"id":"fZVc-QLRDmRa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3))\n","axs[0].plot(1/seas,densities,'ko-',markerfacecolor=[.7,.7,.9],markersize=10)\n","axs[0].set(xscale='log',xlabel=r'$\\lambda = 1/C$',ylabel='Density (% non-zero neurons)',\n","           title='Thinking about $\\lambda$')\n","\n","axs[1].plot(seas,densities,'ko-',markerfacecolor=[.7,.7,.9],markersize=10)\n","axs[1].set(xlabel=r'$C = 1/\\lambda$',ylabel='Density (% non-zero neurons)',title='Thinking about $C$')\n","\n","plt.show()"],"metadata":{"id":"nMi2O3hNDmOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g5oTlbUxDmJr"},"execution_count":null,"outputs":[]}]}