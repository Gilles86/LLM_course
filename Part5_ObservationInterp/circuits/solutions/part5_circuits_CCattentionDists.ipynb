{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNnRa6aYZ305gqUIJzkA9Gb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifying circuits and components<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Laminar profile of attention head weights<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"dVdgcMR4IljT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuKeB769HOkN"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import scipy.stats as stats\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"WDrsqWdnSBA1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Model, hooks, tokens, activations"],"metadata":{"id":"_6Eq0W5qSA-X"}},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-2.8b')\n","\n","# and their pythia model\n","model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8b\")\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","model.eval()"],"metadata":{"id":"83A8PoVGRFj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.config"],"metadata":{"id":"jqTjLQ3NrqHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some helpful variables\n","nheads = model.config.num_attention_heads\n","head_dim = model.config.hidden_size // nheads\n","sqrtD = torch.sqrt(torch.tensor(head_dim)) # used for attention equation\n","\n","print(f'There are {nheads} heads, each with {head_dim} dimensions.')"],"metadata":{"id":"h-IgxbdHA57U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iAu_-SuigluP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook the query vectors\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    activations[f'attn_{layer_number}'] = output.detach().cpu()\n","  return hook\n","\n","# implant the hooks\n","for layeri in range(model.config.num_hidden_layers):\n","  model.gpt_neox.layers[layeri].attention.query_key_value.register_forward_hook(implant_hook(layeri))"],"metadata":{"id":"pQ6zFv8PY4B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E1IwRC6lI-oM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://en.wikipedia.org/wiki/Fiji\n","txt = \"The majority of Fiji's islands were formed by volcanic activity starting around 150 million years ago. Some geothermal activity still occurs today on the islands of Vanua Levu and Taveuni.\"\n","\n","# tokenize\n","tokens = tokenizer.encode(txt,return_tensors='pt')\n","ntokens = len(tokens[0])\n","\n","# run through the model\n","with torch.no_grad():\n","  model(tokens.to(device))"],"metadata":{"id":"fYPkVigeOv47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking sizes\n","print(activations.keys(),'\\n')\n","print(activations['attn_4'].shape)"],"metadata":{"id":"BD0Wb8YdMBqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oDDh2AEr9Hss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the code for splitting into heads\n","\n","# first, separate the Q,K,V matrices\n","Q,K,V = torch.split(activations['attn_13'][0,:,:],model.config.hidden_size,dim=1)\n","\n","# now split into heads\n","Q_h = torch.split(Q,head_dim,dim=1)\n","\n","print(f'There are {len(Q_h)} heads')\n","print(f'Each head has size {Q_h[2].shape}')"],"metadata":{"id":"JGs7C-adPhDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gT3SKDwDnhfl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Laminar profile of attention weight distributions"],"metadata":{"id":"-2EluHa3Bn-K"}},{"cell_type":"code","source":["# initializations\n","smx = np.linspace(0,1,300)\n","head_distributions = np.zeros((model.config.num_hidden_layers,len(smx),2))\n","\n","\n","# loop over layers\n","for layeri in range(model.config.num_hidden_layers):\n","\n","  # separate Q and K, and split into heads\n","  Q,K,V = torch.split(activations[f'attn_{layeri}'][0,:,:],model.config.hidden_size,dim=1)\n","  Q_h = torch.split(Q,head_dim,dim=1)\n","  K_h = torch.split(K,head_dim,dim=1)\n","\n","\n","  # initialize empty arrays\n","  final2prev = np.array([])\n","  selfAttend = np.array([])\n","\n","\n","  # loop over heads\n","  for qi in range(nheads):\n","\n","    # raw attention scores with mask\n","    attn_scores = (Q_h[qi] @ K_h[qi].t()) / sqrtD\n","    pastmask = torch.tril(torch.ones(ntokens,ntokens))\n","    attn_scores[pastmask==0] = -torch.inf\n","\n","    # softmax\n","    attn_sm = F.softmax( attn_scores ,dim=-1)\n","\n","    # the final token with all previous tokens (including the first but excluding self-attn)\n","    final_with_prev = attn_sm[-1,:-1]\n","\n","    # matching tokens are self-attention\n","    matching_toks = torch.diag(attn_sm[1:,1:]) # exclude the first token in the sequence\n","\n","    # add to dataset\n","    final2prev = np.concatenate((final2prev,final_with_prev))\n","    selfAttend = np.concatenate((selfAttend,matching_toks))\n","\n","\n","  ### head loop is complete; get kde's\n","  y = stats.gaussian_kde(final2prev)(smx)\n","  head_distributions[layeri,:,0] = y / y.max()\n","\n","  y = stats.gaussian_kde(selfAttend)(smx)\n","  head_distributions[layeri,:,1] = y / y.max()\n"],"metadata":{"id":"Fv3k52SW6k2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## visualize one layer\n","plt.figure(figsize=(10,4))\n","\n","plt.plot(np.random.randn(len(final2prev))/70 - .1,final2prev,'ko',markerfacecolor=[.7,.9,.7,.7],markersize=8)\n","plt.plot(np.random.randn(len(selfAttend))/70 + .1,selfAttend,'ks',markerfacecolor=[.9,.7,.7,.7],markersize=8)\n","\n","plt.gca().set(xticks=[-.1,.1],xlim=[-.3,.3],xticklabels=['Final to\\nprev','Self-\\nattention',],\n","              ylabel='Softmax attention weight',title='Softmax attention weights from final layer')\n","\n","plt.show()"],"metadata":{"id":"qct5s1aSLVP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cpWhDx7F1RS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show all lines in one plot\n","plt.figure(figsize=(10,4))\n","\n","# plot all the lines\n","for i in range(model.config.num_hidden_layers):\n","\n","  # special case for final layer to get the legend\n","  if i==model.config.num_hidden_layers-1:\n","    plt.plot(smx,head_distributions[i,:,0],color=mpl.cm.Reds(i/32),label='Final to previous')\n","    plt.plot(smx,head_distributions[i,:,1],color=mpl.cm.Blues(i/32),label='Self-attention')\n","  else:\n","    plt.plot(smx,head_distributions[i,:,0],color=mpl.cm.Reds(i/32))\n","    plt.plot(smx,head_distributions[i,:,1],color=mpl.cm.Blues(i/32))\n","\n","\n","plt.legend()\n","plt.gca().set(xlim=[0,1],xlabel='Softmax attention weight',ylabel='Proportion (norm.)',\n","              title='Probability density estimates (each line is a layer)')\n","plt.show()"],"metadata":{"id":"lj8CBxGDM2vE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# heatmaps of layers and probabilities\n","fig,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","# final2prev\n","h = axs[0].imshow(head_distributions[:,:,0].T,aspect='auto',origin='lower',vmin=0,vmax=.008,cmap='hot',\n","                  extent=[0,model.config.num_hidden_layers,smx[0],smx[-1]])\n","fig.colorbar(h,ax=axs[0],pad=.01)\n","axs[0].set(ylabel='Softmax probability',xlabel='Layer',title='Attention weights for final to previous')\n","\n","# self-attention\n","h = axs[1].imshow(head_distributions[:,:,1].T,aspect='auto',origin='lower',vmin=0,vmax=.08,cmap='hot',\n","                  extent=[0,model.config.num_hidden_layers,smx[0],smx[-1]])\n","fig.colorbar(h,ax=axs[1],pad=.01)\n","axs[1].set(ylabel='Softmax probability',xlabel='Layer',title='Attention weights for self')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"MarQpsE8M2sk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WxunQIZHx0aR"},"execution_count":null,"outputs":[]}]}