{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMbyyygrCQG2TiZSpslMFT4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><b><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></b></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifing latent factors<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Laminar profile of autoencoder sparsity<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"mv5YXPmTJfJl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kIpBsXzV4US"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer"]},{"cell_type":"code","source":[],"metadata":{"id":"fo-hzu4uPdsc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import text, tokenize, get MLP activations"],"metadata":{"id":"m0bTR3J9nYFQ"}},{"cell_type":"code","source":["# model & tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model     = AutoModelForCausalLM.from_pretrained('gpt2')\n","\n","# push to GPU in eval mode\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model.eval().to(device)"],"metadata":{"id":"7yWovRbDcLeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a function to hook the activations\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    activations[f'mlp_{layer_number}'] = output.detach()\n","  return hook\n","\n","# put hooks in all layers\n","for layer2hook in range(model.config.n_layer):\n","  model.transformer.h[layer2hook].mlp.c_fc.register_forward_hook(implant_hook(layer2hook))"],"metadata":{"id":"VUolDrVBV66Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X58L8HPmBW-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","text = requests.get('https://en.wikipedia.org/wiki/Light-emitting_diode').text\n","print(f'Full website contains {len(text):,} characters')\n","\n","text = text[text.find('mw-body-content'):]\n","text = text[:text.find('id=\"References\"')]\n","print(f'Selected text contains {len(text):,} characters')"],"metadata":{"id":"IxdZnQhvWAta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.pad_token = tokenizer.eos_token\n","\n","# tokenize the text\n","tokens = tokenizer.encode(text)\n","print(f'There are {len(tokens):,} tokens')"],"metadata":{"id":"nFWb-EsHTO4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["u,c = np.unique(tokens,return_counts=True)\n","sidx = np.argsort(-c)\n","u = u[sidx]\n","c = c[sidx]\n","\n","print('Top 30 most common tokens:')\n","for i in range(30):\n","  print(f'  {c[i]:4} apperances of \"{tokenizer.decode(u[i])}\"')"],"metadata":{"id":"GPKi4WaHTTXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create 10 batches of 1024 tokens\n","batches = torch.tensor(tokens[:10*1024]).reshape(10,1024).to(device)\n","batches.shape"],"metadata":{"id":"okob1EfHXeNG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Forward-pass the tokens to get the activations\n","with torch.no_grad():\n","  model(batches)"],"metadata":{"id":"sqVP0eFyumBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Z-bxx4i1TTT8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Create an autoencoder class in a function"],"metadata":{"id":"T_TPzU3_naiU"}},{"cell_type":"code","source":["def createTheSAE(num_latent):\n","\n","  # create the class\n","  class SparseAE(nn.Module):\n","    def __init__(self, input_dim, latent_dim, k=None, sparsity_weight=1, decor_weight=.0005):\n","      super().__init__()\n","      self.encoder = nn.Linear(input_dim, latent_dim, bias=False)\n","      # self.decoder = nn.Linear(latent_dim, input_dim, bias=False)\n","      # note: decoder is tied to encoder in forward()\n","\n","      self.sparsity_weight = sparsity_weight\n","      self.decor_weight = decor_weight\n","\n","      # k-sparse parameter defaults to 50% of input\n","      if k==None:\n","        self.k = input_dim//2\n","      else:\n","        self.k = k\n","\n","    def forward(self, x):\n","\n","      # forward pass to the latent layer\n","      latent = F.relu(self.encoder(x))\n","\n","      # \"k-sparsify\": force sparsity by zeroing out small activations\n","      topk_vals = torch.topk(latent,self.k,dim=1)[0]\n","      thresh = topk_vals[:,-1].unsqueeze(1) # kth-largest value is the smallest of the sorted top-k\n","      mask = (latent >= thresh).float() # mask is 0's and 1's\n","      latent_sparse = latent * mask\n","\n","      # finally, decode via tied weights\n","      y = F.linear(latent_sparse, self.encoder.weight.t())\n","\n","      return y,latent_sparse\n","\n","    def sparsity_loss(self, z):\n","      return self.sparsity_weight * torch.mean(torch.abs(z))\n","\n","    # penalty on inter-latent covariance\n","    def decorrelation_loss(self, estLatent):\n","      cov = torch.cov(estLatent.T)\n","      off_diag = cov - torch.diag(torch.diag(cov))\n","      return self.decor_weight * torch.sum(off_diag**2)\n","\n","\n","\n","  # create an instance of the autoencoder\n","  ae = SparseAE(input_dim=X.shape[1], k=num_latent//3, latent_dim=num_latent)\n","  ae = ae.to(device)\n","  return ae"],"metadata":{"id":"LQWvtqVWWE4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### test with some data\n","\n","# get an MLP layer activation\n","nhidden = activations['mlp_3'].shape[-1]\n","X = activations['mlp_3'].reshape(-1,nhidden)\n","\n","# create an SAE model instance (using a 2x expansion for the latent layer)\n","aemodel = createTheSAE(X.shape[1]*2)\n","aemodel = aemodel.to(device)\n","\n","# and push some data through\n","aemodel(X)"],"metadata":{"id":"-yq3tJxiP4qg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4u_Hb-FWPDk0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Train layer-specific SAEs"],"metadata":{"id":"gJpy1LlRPxJi"}},{"cell_type":"code","source":["# takes ~3 mins for gpt2-small on GPU\n","# takes ~20 mins for gpt2-large on GPU\n","\n","# initialize\n","latentDensity = np.zeros(model.config.n_layer)\n","densityActivation = np.zeros(model.config.n_layer)\n","finalloss = np.zeros(model.config.n_layer)\n","\n","n_epochs = 75\n","\n","\n","## loop over layers\n","for layeri in range(model.config.n_layer):\n","\n","  # get the activations from this layer, and reshape to (N x hidden)\n","  nhidden = activations[f'mlp_{layeri}'][:,1:,:].shape[-1]\n","  X = activations[f'mlp_{layeri}'].reshape(-1,nhidden) # remove first token\n","\n","  # create an SAE model for this layer (using a 2x expansion for the latent layer)\n","  aemodel = createTheSAE(X.shape[1]*2)\n","  aemodel = aemodel.to(device)\n","\n","  ### train the model\n","  optimizer = optim.Adam(aemodel.parameters(), lr=.0001)\n","  mse_loss  = nn.MSELoss().to(device)\n","\n","  for epoch in range(n_epochs):\n","\n","    # forward pass\n","    optimizer.zero_grad()\n","    x_pred,latent = aemodel(X)\n","\n","    # backprop\n","    loss = mse_loss(x_pred,X) + aemodel.sparsity_loss(latent)\n","    loss.backward()\n","    optimizer.step()\n","\n","  # final loss after all epochs\n","  finalloss[layeri] = loss.item()\n","\n","\n","\n","  ## final run to get latent activations\n","  with torch.no_grad():\n","    aeout,latent = aemodel(X)\n","\n","  # convert to numpy and back to CPU\n","  latent = latent.cpu().numpy()\n","\n","\n","  ### latent layer characteristics\n","  # mask for zero-valued activations\n","  densitymask = np.full(latent.shape,np.nan)\n","  densitymask[latent!=0] = 1\n","\n","  # density is the percent of nonzero activations per latent component\n","  densityPerComponent = 100 * np.nansum(densitymask,axis=0) / densitymask.shape[0]\n","  latentDensity[layeri] = densityPerComponent.mean()\n","\n","  # token-averaged activation magnitude, excluding zeros\n","  nonzeroAct = np.nanmean(np.abs(latent*densitymask),axis=0)\n","  nonzeroAct[np.isnan(nonzeroAct)] = 0\n","\n","  # average nonzero activations per latent component (after minmax scaling)\n","  dpc = (densityPerComponent-densityPerComponent.min()) / (densityPerComponent.max()-densityPerComponent.min())\n","  nza = (nonzeroAct-nonzeroAct.min()) / (nonzeroAct.max()-nonzeroAct.min())\n","  densityActivation[layeri] = np.mean( dpc*nza )\n","\n","  print(f'Finished layer {layeri+1:2}/{model.config.n_layer}')"],"metadata":{"id":"jLASackBSye2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5JmpLl4JQPyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Visualize SAE laminar profiles"],"metadata":{"id":"_UIhbpmGQPqD"}},{"cell_type":"code","source":["# plot the results\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","axs[0].plot(finalloss,'ks',markerfacecolor=[.7,.7,.9],markersize=9)\n","axs[0].set(xlabel='Layer',ylabel='MSE Loss',title='Final SAE loss per layer')\n","\n","axs[1].plot(latentDensity,'ks',markerfacecolor=[.7,.9,.7],markersize=9)\n","axs[1].set(xlabel='Layer',ylabel='Density (% nonzero activations)',title='Latent density per layer')\n","\n","axs[2].plot(densityActivation,'ks',markerfacecolor=[.9,.7,.7],markersize=9)\n","axs[2].set(xlabel='Layer',ylabel='Density $\\\\times$ activation',title='(Density $\\\\times$ activation) per layer')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"MekaXD9xWO6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3o-HaL7LOMvz"},"execution_count":null,"outputs":[]}]}