{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMs8FTm0Zzqr606Tmlyjmeh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><b><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></b></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifying circuits and components<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Sparse autoencoders: theory and code<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"7aWhyg4V8Qv2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DmJFsXi77TI"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"AF6aqa1S_oJd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create latent variables and mix them"],"metadata":{"id":"i-sfCL9v_ppR"}},{"cell_type":"code","source":["# simulate independent sources and mix\n","n_samples = 3000\n","t = np.linspace(0,14*np.pi,n_samples)\n","latent1 = np.sin(t) + 1\n","latent2 = np.sign(np.sin(3*t)) + 1\n","\n","# mix the sources linearly to create the manifest variables\n","mixing_matrix = np.array([ [1,.4], [.6,1] ])\n","data = np.stack([latent1,latent2],axis=1) @ mixing_matrix\n","data.shape"],"metadata":{"id":"dmCJ0USl8Mf6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","_,axs = plt.subplots(2,2,figsize=(10,6))\n","\n","# the latent variables and their statistical independence\n","axs[0,0].plot(latent1,'bo',markerfacecolor=[.7,.7,.9],alpha=.5,markersize=3)\n","axs[0,0].plot(latent2,'gs',markerfacecolor=[.7,.9,.7],alpha=.5,markersize=3)\n","axs[0,0].set(xlabel='Data index',ylabel='Data value',xlim=[-5,n_samples+4],\n","             title=f'Latent sources (r = {np.corrcoef(latent1,latent2)[0,1]:.2f})')\n","\n","# the \"manifest\" variables and their correlation\n","axs[0,1].plot(data[:,0],'mo',markerfacecolor=[.9,.7,.9],alpha=.5,markersize=3)\n","axs[0,1].plot(data[:,1],'ks',markerfacecolor=[.9,.9,.7],alpha=.5,markersize=3)\n","axs[0,1].set(xlabel='Data index',ylabel='Data value',xlim=[-5,n_samples+4],\n","             title=f'Mixed signals (r = {np.corrcoef(data.T)[0,1]:.2f})')\n","\n","\n","# histograms\n","yL1,xL1 = np.histogram(latent1,bins=40,density=True)\n","yL2,xL2 = np.histogram(latent2,bins=40,density=True)\n","axs[1,0].plot(xL1[:-1],yL1,linewidth=3,label='Latent 1')\n","axs[1,0].plot(xL2[:-1],yL2,linewidth=3,label='Latent 2')\n","axs[1,0].set(xlabel='Data value',ylabel='Density',xlim=[min(xL1[0],xL2[0]),max(xL1[-1],xL2[-1])],\n","             title='Histograms of latent variables')\n","axs[1,0].legend()\n","\n","yD1,xD1 = np.histogram(data[:,0],bins=40,density=True)\n","yD2,xD2 = np.histogram(data[:,1],bins=40,density=True)\n","axs[1,1].plot(xD1[:-1],yD1,linewidth=3,label='Variable 1')\n","axs[1,1].plot(xD2[:-1],yD2,linewidth=3,label='Variable 2')\n","axs[1,1].set(xlabel='Data value',ylabel='Density',xlim=[min(xD1[0],xD2[0]),max(xD1[-1],xD2[-1])],\n","             title='Histograms of mixed variables')\n","axs[1,1].legend()\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"3P0qFJ96_ovg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uECrQP7w_nMn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create a sparse autoencoder model"],"metadata":{"id":"e2XUcxCH_5VC"}},{"cell_type":"code","source":["# as a pytorch class\n","class SparseAutoencoder(nn.Module):\n","  def __init__(self, input_dim, hidden_dim, sparsity_weight=1):\n","    super().__init__()\n","    self.encoder = nn.Linear(input_dim, hidden_dim)\n","    self.decoder = nn.Linear(hidden_dim, input_dim)\n","    self.sparsity_weight = sparsity_weight\n","\n","  def forward(self, x):\n","    estLatent = torch.nn.functional.gelu(self.encoder(x))\n","    x_recon = self.decoder(estLatent)\n","    return x_recon,estLatent\n","\n","  # L1 penalty on hidden activations\n","  def sparsity_loss(self, estLatent):\n","    return self.sparsity_weight * torch.mean(torch.abs(estLatent))\n","\n","  # penalty on inter-latent covariance (used in the next demo, not here)\n","  def decorrelation_loss(self, estLatent):\n","    cov = torch.cov(estLatent.T)\n","    off_diag = cov - torch.diag(torch.diag(cov))\n","    return self.sparsity_weight * torch.sum(off_diag**2)\n","\n","\n","\n","# create an instance and inspect!\n","num_hidden = 20\n","AEmodel = SparseAutoencoder(2,num_hidden) # two inputs, >2 hidden dimensions\n","AEmodel"],"metadata":{"id":"conTivbs8V3P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test with bunk input to make sure it works\n","x = torch.randn(10,2)\n","AEmodel(x)"],"metadata":{"id":"WFiZFxgEAZri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yjOrs0xmAZol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train the model on the data"],"metadata":{"id":"i3_QHMtwA4Ap"}},{"cell_type":"code","source":["# training params\n","n_epochs  = 600\n","lr        = .0007\n","optimizer = optim.Adam(AEmodel.parameters(), lr=lr)\n","lossfun   = nn.MSELoss() # loss is mean-squared error to match output to input\n","\n","# data need to be a torch tensor of size obs X features\n","X_tensor = torch.tensor(data,dtype=torch.float)\n","X_tensor.shape"],"metadata":{"id":"vFAGGStD_j7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["losses = np.zeros((n_epochs,2))\n","\n","# train the model!\n","for epoch in range(n_epochs):\n","\n","  # forward pass\n","  optimizer.zero_grad()\n","  x_recon,estLatent = AEmodel(X_tensor)\n","\n","  # calculate and store the lossses (MSE + L1)\n","  L1loss = AEmodel.sparsity_loss(estLatent)\n","  # L1loss = AEmodel.decorrelation_loss(estLatent)\n","  MSEloss = lossfun(x_recon,X_tensor)\n","  losses[epoch,0] = L1loss.item()\n","  losses[epoch,1] = MSEloss.item()\n","\n","  # the actual loss (for backprop) is the sum\n","  allloss = MSEloss + L1loss\n","\n","  # backprop\n","  allloss.backward()\n","  optimizer.step()\n","\n","  # report!\n","  if epoch%47==0:\n","    print(f'Epoch {epoch+1:3}: L1 loss = {L1loss.item():.4f}, MSE loss = {MSEloss.item():.4f}')"],"metadata":{"id":"gyb6ktJQBCvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","# L1 and MSE loss separately\n","axs[0].plot(range(0,n_epochs,20),losses[::20,0],'ko-',markerfacecolor=[.7,.7,.9],markersize=8,label='L1 loss')\n","axs[0].set(xlabel='Epoch',ylabel='L1 Loss',title='Training loss components')\n","axs[0].legend(bbox_to_anchor=[.5,.5,.48,0],frameon=False)\n","\n","ax2 = axs[0].twinx()\n","ax2.plot(range(0,n_epochs,20),losses[::20,1],'ks-',markerfacecolor=[.9,.7,.7],markersize=8,label='MSE loss')\n","ax2.set(ylabel='MSE Loss')\n","ax2.legend(bbox_to_anchor=[.5,.4,.5,0],frameon=False)\n","\n","axs[1].plot(range(0,n_epochs,20),losses[::20,:].sum(axis=1),'k^-',markerfacecolor=[.7,.7,.7],markersize=8,label='Total loss')\n","axs[1].set(xlabel='Epoch',ylabel='Training loss',title='Total losses')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"rUyDWxAMCCzL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mJgyHUHqI8G8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inspect the results"],"metadata":{"id":"LsgqGKGkI8Dx"}},{"cell_type":"code","source":["# one final forward pass\n","x_recon,estLatent = AEmodel(X_tensor)\n","\n","estLatent = estLatent.detach().numpy()\n","estLatent.shape"],"metadata":{"id":"QmYqb6lhDVcJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(np.corrcoef(estLatent.T),cmap='RdBu_r',vmin=-1,vmax=1)\n","plt.colorbar()\n","plt.gca().set(xlabel='Latent variable',xticks=range(0,num_hidden,2),yticks=range(1,num_hidden,2),\n","              ylabel='Latent variable',title='Correlation matrix')\n","plt.show()"],"metadata":{"id":"k_tvq-meJfiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CveazsnsvhET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find the latent component that best correlates with ground truth variables"],"metadata":{"id":"NhTz1amNLhk2"}},{"cell_type":"code","source":["# concatenate the ground truth variable onto the model-latent components\n","latentComps_cat = np.concatenate((estLatent,latent1[:,None]),axis=1)\n","\n","# correlate\n","R1 = abs(np.corrcoef(latentComps_cat.T))\n","\n","# find the strongest correlation with the target (ground truth)\n","best_1 = np.nanargmax(R1[-1,:-1]) # :-1 to ignore self-correlation = 1\n","print(f'Best latent component 1: {best_1} (r = {R1[best_1,-1]:.3f})')\n","\n","\n","# repeat for component 2\n","latentComps_cat = np.concatenate((estLatent,latent2[:,None]),axis=1)\n","R2 = abs(np.corrcoef(latentComps_cat.T))\n","best_2 = np.nanargmax(R2[-1,:-1]) # :-1 to ignore self-correlation = 1\n","print(f'Best latent component 2: {best_2} (r = {R2[best_2,-1]:.3f})')"],"metadata":{"id":"qAMnKikKzRMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","_,axs = plt.subplots(1,2,figsize=(12,3))\n","\n","# the latent variables and their statistical independence\n","axs[0].plot(estLatent[:,best_1],'bo',markerfacecolor=[.7,.7,.9,.5],markersize=3)\n","axs[0].plot(estLatent[:,best_2],'gs',markerfacecolor=[.7,.9,.7,.5],markersize=3)\n","axs[0].set(xlabel='Data index',ylabel='Data value',xlim=[-5,n_samples+4],\n","             title=f'Latent components (r = {np.corrcoef(estLatent[:,[best_1,best_2]].T)[0,1]:.2f})')\n","\n","\n","# histograms\n","yL1,xL1 = np.histogram(estLatent[:,best_1],bins=40)\n","yL2,xL2 = np.histogram(estLatent[:,best_2],bins=40)\n","axs[1].plot(xL1[:-1],yL1,'b',linewidth=3,label='Est. latent 1')\n","axs[1].plot(xL2[:-1],yL2,'g',linewidth=3,label='Est. latent 2')\n","\n","axs[1].set(xlabel='Data value',ylabel='Count',xlim=[min(xL1[0],xL2[0]),max(xL1[-1],xL2[-1])],\n","             title='Histograms of latent variables')\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"botazrt0IXeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FfZa5cQqQILG"},"execution_count":null,"outputs":[]}]}