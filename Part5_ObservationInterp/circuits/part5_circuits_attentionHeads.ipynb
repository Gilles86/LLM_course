{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO3tuM0CLBu3aj/2lFcGz20"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifying circuits and components<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Isolating and investigating attention heads<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"dVdgcMR4IljT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuKeB769HOkN"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import scipy.stats as stats\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import GPT2Model, GPT2Tokenizer\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2Model.from_pretrained('gpt2')\n","model.eval()"],"metadata":{"id":"83A8PoVGRFj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.config"],"metadata":{"id":"jqTjLQ3NrqHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some helpful variables\n","nheads = model.config.n_head\n","head_dim = model.config.n_embd // nheads\n","sqrtD = torch.sqrt(torch.tensor(head_dim)) # used for attention equation\n","\n","print(f'There are {nheads} heads, each with {head_dim} dimensions.')"],"metadata":{"id":"h-IgxbdHA57U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iAu_-SuigluP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Implanting a hook in the model"],"metadata":{"id":"YI_EkZGRHaXr"}},{"cell_type":"code","source":["# hook the query vectors\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    activations[keyName] = output.detach()\n","  return hook\n","\n","# implant the hooks\n","whichlayer = 6\n","keyName = f'attn_{whichlayer}'\n","model.h[whichlayer].attn.c_attn.register_forward_hook(implant_hook(whichlayer))"],"metadata":{"id":"pQ6zFv8PY4B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E1IwRC6lI-oM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Forward pass and get activations"],"metadata":{"id":"BHcZCWR2I-lF"}},{"cell_type":"code","source":["# https://en.wikipedia.org/wiki/Fiji\n","txt = \"The majority of Fiji's islands were formed by volcanic activity starting around 150 million years ago. Some geothermal activity still occurs today on the islands of Vanua Levu and Taveuni.\"\n","\n","# tokenize\n","tokens = tokenizer.encode(txt,return_tensors='pt')\n","ntokens = len(tokens[0])\n","\n","# run through the model\n","with torch.no_grad():\n","  model(tokens)"],"metadata":{"id":"fYPkVigeOv47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking sizes\n","print(activations.keys())\n","print(activations[keyName].shape)"],"metadata":{"id":"BD0Wb8YdMBqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oDDh2AEr9Hss"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Split into heads"],"metadata":{"id":"5w4npn-M9Hop"}},{"cell_type":"code","source":["# first, separate the Q,K,V matrices\n","Q,K,V = torch.split(activations[keyName][0,:,:],model.config.n_embd,dim=1)\n","Q.shape"],"metadata":{"id":"JGs7C-adPhDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now split into heads\n","Q_h = torch.split(Q,head_dim,dim=1)\n","\n","print(f'There are {len(Q_h)} heads')\n","print(f'Each head has size {Q_h[2].shape}')"],"metadata":{"id":"Xd7TpP8VCH-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat for the keys\n","K_h = torch.split(K,head_dim,dim=1)"],"metadata":{"id":"iWG3K1uO6lCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8ZPBXWHd6k8o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Means and standard deviations of different heads over tokens"],"metadata":{"id":"xNPEvMIn-3d8"}},{"cell_type":"code","source":["# averages and standard deviations over all vectors and all tokens\n","Qh_means = np.array([ q[1:].mean() for q in Q_h ])\n","Qh_stds  = np.array([ q[1:].std()  for q in Q_h ])\n","\n","Kh_means = np.array([ k[1:].mean() for k in K_h ])\n","Kh_stds  = np.array([ k[1:].std()  for k in K_h ])"],"metadata":{"id":"UFaVk16TAhzj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualizations\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","axs[0].errorbar(np.arange(nheads)-.2,Qh_means,Qh_stds,fmt='ks',markerfacecolor=[.7,.9,.7],label='Q')\n","axs[0].errorbar(np.arange(nheads)+.2,Kh_means,Kh_stds,fmt='ko',markerfacecolor=[.9,.7,.7],label='K')\n","axs[0].set(xlabel='Heads',ylabel='Activation',title='Means and stds')\n","axs[0].legend()\n","\n","axs[1].plot(Qh_means,Kh_means,'kh',markerfacecolor=[.7,.7,.7],markersize=10)\n","axs[1].set(xlabel='Q heads',ylabel='K heads',title='Average activations')\n","\n","axs[2].plot(Qh_stds,Kh_stds,'k^',markerfacecolor=[.7,.7,.7],markersize=10)\n","axs[2].set(xlabel='Q heads',ylabel='K heads',title='Standard deviations')\n","\n","plt.suptitle(f'Descriptives of attention head activations in layer {whichlayer}',fontweight='bold')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"4f1P_IlW-8Nd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"deEkf5atCodx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Attention scores over tokens"],"metadata":{"id":"JIX1EovRCoS0"}},{"cell_type":"code","source":["# initializations\n","samehead_dp = np.array([])\n","diffhead_dp = np.array([])\n","\n","\n","# loop over pairs of heads\n","for qi in range(nheads):\n","  for ki in range(nheads):\n","\n","    # dot product for last token in Q with all previous tokens in K (excluding first token)\n","    dp = Q_h[qi][-1,:] @ K_h[ki][1:-1,:].t()\n","\n","    # store in the appropriate matrix\n","    if qi==ki:\n","      samehead_dp = np.concatenate((samehead_dp,dp))\n","    else:\n","      diffhead_dp = np.concatenate((diffhead_dp,dp))\n","\n","\n","\n","## visualizations\n","_,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","# plot the raw data\n","axs[0].plot(np.random.randn(len(samehead_dp))/60 - .1,samehead_dp,'ko',markerfacecolor=[.7,.9,.7,.7],markersize=8)\n","axs[0].plot(np.random.randn(len(diffhead_dp))/60 + .1,diffhead_dp,'ks',markerfacecolor=[.9,.7,.7,.7],markersize=8)\n","axs[0].axhline(0,linestyle='--',color=[.7,.7,.7],zorder=-3)\n","axs[0].set(xticks=[-.1,.1],xticklabels=['Same head','Diff heads'],\n","              ylabel='QK$^T$ dot products',title='Raw attention scores',xlim=[-.3,.3])\n","\n","# distributions\n","y,x = np.histogram(samehead_dp,bins=30)\n","axs[1].plot(x[:-1],y/y.max(),'g',linewidth=2,label='Same head')\n","\n","y,x = np.histogram(diffhead_dp,bins=80)\n","axs[1].plot(x[:-1],y/y.max(),'r',linewidth=2,label='Diff heads')\n","\n","axs[1].legend()\n","axs[1].set(xlabel='Dot product value',ylabel='Proportion (norm.)',title='Distributions')\n","axs[1].axvline(0,linestyle='--',color=[.7,.7,.7])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"ia6jLKBNCoIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gT3SKDwDnhfl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Softmax-transformed attention values within each head"],"metadata":{"id":"-2EluHa3Bn-K"}},{"cell_type":"code","source":["# empty initializations\n","final2prev = np.array([])\n","selfAttend = np.array([])\n","firstSelf  = np.array([])\n","\n","\n","# loop over heads\n","for qi in range(nheads):\n","\n","  # raw attention scores with mask\n","  attn_scores = (Q_h[qi] @ K_h[qi].t()) / sqrtD\n","  pastmask = torch.tril(torch.ones(ntokens,ntokens))\n","  attn_scores[pastmask==0] = -torch.inf\n","\n","  # softmax\n","  attn_sm = F.softmax( attn_scores ,dim=-1)\n","\n","  # the final token with all previous tokens (including the first but excluding self-attn)\n","  final_with_prev = attn_sm[-1,:-1]\n","\n","  # matching tokens are self-attention\n","  matching_toks = torch.diag(attn_sm[1:,1:]) # exclude the first token in the sequence\n","  first_selfTok = attn_sm[0,0].unsqueeze(0)  # isolate the first token\n","\n","  # add to dataset\n","  final2prev = np.concatenate((final2prev,final_with_prev))\n","  selfAttend = np.concatenate((selfAttend,matching_toks))\n","  firstSelf  = np.concatenate((firstSelf,first_selfTok))\n","\n","\n","## visualize\n","plt.figure(figsize=(10,4))\n","\n","plt.plot(np.random.randn(len(final2prev))/70 - .1,final2prev,'ko',markerfacecolor=[.7,.9,.7,.7],markersize=8)\n","plt.plot(np.random.randn(len(selfAttend))/70 + .1,selfAttend,'ks',markerfacecolor=[.9,.7,.7,.7],markersize=8)\n","plt.plot(np.random.randn(len(firstSelf))/70  + .3,firstSelf,'ks',markerfacecolor=[.7,.7,.9,.7],markersize=8)\n","\n","plt.gca().set(xticks=[-.1,.1,.3],ylabel='Softmax attention weight',xlim=[-.3,.5],\n","              xticklabels=['Final to\\nprev','Self-attention\\nother tokens','Self-attention\\nfirst token'])\n","\n","plt.show()"],"metadata":{"id":"Fv3k52SW6k2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cpWhDx7F1RS6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Kernel density estimator of softmax-attention distribution"],"metadata":{"id":"8pnbdYgw1RKv"}},{"cell_type":"code","source":["# dataset\n","sparseData = [-1.5,.4,.45,.5,1.4]\n","\n","# high-resolution grid to estimate distribution\n","xgrid = np.linspace(-2,2,301)\n","\n","# create the kde object with the data and a smoothing (bandwidth) parameter\n","kde = stats.gaussian_kde(sparseData,bw_method=.2)\n","\n","# estimate the pdf at the x-grid points\n","y = kde(xgrid)\n","\n","\n","## visualize!\n","plt.figure(figsize=(8,4))\n","\n","# plot the raw data\n","for d in sparseData:\n","  plt.plot([d,d],[0,1],'r')\n","\n","# and the kde\n","plt.plot(xgrid,y,linewidth=2)\n","\n","plt.gca().set(xlim=xgrid[[0,-1]],ylim=[0,None],xlabel='Data value',\n","              ylabel='Probability density estimate',title='Simple demo of KDE estimation')\n","plt.show()"],"metadata":{"id":"E76t06LpeCSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XCwtQduyeCP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# high-resolution grid to estimate distribution\n","smx = np.linspace(0,1,300)\n","\n","# get a kernel density estimator object\n","kde = stats.gaussian_kde(final2prev)\n","\n","# evaluate it at grid points\n","y = kde(smx)\n","\n","# sample sizes differ, and we only care about the shape\n","y = y/y.max()\n","\n","# and plot it!\n","plt.figure(figsize=(10,3))\n","plt.plot(smx,y,'g',linewidth=3,label='Final to previous')\n","\n","\n","## same for self-attention, but more compact :)\n","y = stats.gaussian_kde(selfAttend)(smx)\n","plt.plot(smx,y/y.max(),'r',linewidth=3,label='Self-attention')\n","\n","plt.legend()\n","plt.gca().set(ylim=[-.001,.1],xlim=[0,1],title='KDE distribution of attention scores',\n","              xlabel='Softmax attention scores',ylabel='KDE probability (a.u.)')\n","\n","plt.show()"],"metadata":{"id":"kbLCGdMC6kzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WxunQIZHx0aR"},"execution_count":null,"outputs":[]}]}