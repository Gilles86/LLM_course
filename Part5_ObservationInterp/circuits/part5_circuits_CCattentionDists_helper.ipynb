{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNX/cqPj1oM2UtzWRJY7K6H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifying circuits and components<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Laminar profile of attention head weights<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"dVdgcMR4IljT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuKeB769HOkN"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import scipy.stats as stats\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"WDrsqWdnSBA1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Model, hooks, tokens, activations"],"metadata":{"id":"_6Eq0W5qSA-X"}},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-2.8b')\n","\n","# and their pythia model\n","model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8b\")\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","model.eval()"],"metadata":{"id":"83A8PoVGRFj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.config"],"metadata":{"id":"jqTjLQ3NrqHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some helpful variables\n","nheads =\n","head_dim =\n","sqrtD =   # used for attention equation\n","\n","print(f'There are {} heads, each with {} dimensions.')"],"metadata":{"id":"h-IgxbdHA57U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iAu_-SuigluP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook the query vectors\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","  return hook\n","\n","# implant the hooks\n","for layeri in"],"metadata":{"id":"pQ6zFv8PY4B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E1IwRC6lI-oM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://en.wikipedia.org/wiki/Fiji\n","txt = \"The majority of Fiji's islands were formed by volcanic activity starting around 150 million years ago. Some geothermal activity still occurs today on the islands of Vanua Levu and Taveuni.\"\n","\n","# tokenize\n","tokens = tokenizer\n","ntokens = len(\n","\n","# run through the model\n"],"metadata":{"id":"fYPkVigeOv47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking sizes\n","print(activations.keys(),'\\n')\n","print(activations['attn_4'].shape)"],"metadata":{"id":"BD0Wb8YdMBqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oDDh2AEr9Hss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the code for splitting into heads\n","\n","# first, separate the Q,K,V matrices\n","Q,K,V = torch.split\n","\n","# now split into heads\n","Q_h = torch.split(Q\n","\n","print(f'There are {} heads')\n","print(f'Each head has size {}')"],"metadata":{"id":"JGs7C-adPhDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gT3SKDwDnhfl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Laminar profile of attention weight distributions"],"metadata":{"id":"-2EluHa3Bn-K"}},{"cell_type":"code","source":["# initializations\n","smx = np.linspace(0,1,300)\n","head_distributions = np.zeros((,,))\n","\n","\n","# loop over layers\n","for layeri in\n","\n","  # separate Q and K, and split into heads\n","  Q,K,V = torch.split(activations[f'attn_{layeri}'][0,:,:]\n","  Q_h = torch.split(Q,\n","  K_h = torch.split(K,\n","\n","\n","  # initialize empty arrays\n","  final2prev = np.array([])\n","  selfAttend = np.array([])\n","\n","\n","  # loop over heads\n","  for qi in range(\n","\n","    # raw attention scores with mask\n","    attn_scores =  / sqrtD\n","    pastmask =\n","    attn_scores[pastmask==0] =\n","\n","    # softmax\n","    attn_sm =\n","\n","    # the final token with all previous tokens (including the first but excluding self-attn)\n","    final_with_prev =\n","\n","    # matching tokens are self-attention\n","    matching_toks = torch.diag() # exclude the first token in the sequence\n","\n","    # add to dataset\n","    final2prev = np.concatenate((final2prev,\n","    selfAttend = np.concatenate((selfAttend,\n","\n","\n","  ### head loop is complete; get kde's\n","  y = stats.gaussian_kde()()\n","  head_distributions[layeri,:,0] = # normalize by max\n","\n","\n","  head_distributions[layeri,:,1]\n"],"metadata":{"id":"Fv3k52SW6k2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## visualize one layer\n","plt.figure(figsize=(10,4))\n","\n","plt.plot(,'ko',markerfacecolor=[.7,.9,.7,.7],markersize=8)\n","plt.plot(,'ks',markerfacecolor=[.9,.7,.7,.7],markersize=8)\n","\n","plt.gca().set(xticks=[-.1,.1],xlim=[-.3,.3],xticklabels=['Final to\\nprev','Self-\\nattention',],\n","              ylabel='Softmax attention weight',title='Softmax attention weights from final layer')\n","\n","plt.show()"],"metadata":{"id":"qct5s1aSLVP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cpWhDx7F1RS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show all lines in one plot\n","plt.figure(figsize=(10,4))\n","\n","# plot all the lines\n","for i in range(model.config.num_hidden_layers):\n","\n","  # special case for final layer to get the legend\n","  if i==model.config.num_hidden_layers-1:\n","    plt.plot(,label='Final to previous')\n","    plt.plot(,label='Self-attention')\n","  else:\n","    plt.plot(\n","    plt.plot(\n","\n","\n","plt.gca().set(xlim=[0,1],xlabel='Softmax attention weight',ylabel='Proportion (norm.)',\n","              title='Probability density estimates (each line is a layer)')\n","plt.show()"],"metadata":{"id":"lj8CBxGDM2vE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# heatmaps of layers and probabilities\n","fig,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","# final2prev\n","h = axs[0].imshow(head_distributions[:,:,0]\n","fig.colorbar(h,ax=axs[0],pad=.01)\n","axs[0].set(ylabel='Softmax probability',xlabel='Layer',title='Attention weights for final to previous')\n","\n","# self-attention\n","h = axs[1].imshow(head_distributions[:,:,1]\n","fig.colorbar(h,ax=axs[1],pad=.01)\n","axs[1].set(ylabel='Softmax probability',xlabel='Layer',title='Attention weights for self')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"MarQpsE8M2sk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WxunQIZHx0aR"},"execution_count":null,"outputs":[]}]}