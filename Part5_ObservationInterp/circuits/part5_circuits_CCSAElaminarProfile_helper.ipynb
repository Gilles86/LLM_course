{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPIjkEN5T4++8s9PYRLMNfE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><b><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></b></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifing latent factors<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Laminar profile of autoencoder sparsity<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"mv5YXPmTJfJl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kIpBsXzV4US"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer"]},{"cell_type":"code","source":[],"metadata":{"id":"fo-hzu4uPdsc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import text, tokenize, get MLP activations"],"metadata":{"id":"m0bTR3J9nYFQ"}},{"cell_type":"code","source":["# model & tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model     = AutoModelForCausalLM.from_pretrained('gpt2')\n","\n","# push to GPU in eval mode\n"],"metadata":{"id":"7yWovRbDcLeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a function to hook the activations\n","\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","  return hook\n","\n","# put hooks in all layers\n"],"metadata":{"id":"VUolDrVBV66Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X58L8HPmBW-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","text = requests.get('https://en.wikipedia.org/wiki/Light-emitting_diode').text\n","\n","\n"],"metadata":{"id":"IxdZnQhvWAta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.pad_token = tokenizer.eos_token\n","\n","# tokenize the text\n","tokens =\n","print(f'T"],"metadata":{"id":"nFWb-EsHTO4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["u,c =\n","sidx = np.argsort\n","u = u\n","c = c[\n","\n","print('Top 30 most common tokens:')\n","for i in range(30):\n","  print("],"metadata":{"id":"GPKi4WaHTTXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create 10 batches of 1024 tokens\n","batches =\n","batches.shape"],"metadata":{"id":"okob1EfHXeNG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Forward-pass the tokens to get the activations\n"],"metadata":{"id":"sqVP0eFyumBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Z-bxx4i1TTT8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Create an autoencoder class in a function"],"metadata":{"id":"T_TPzU3_naiU"}},{"cell_type":"code","source":["def createTheSAE(num_latent):\n","\n","  # create the class\n","  class SparseAE(nn.Module):\n","    def __init__(self, input_dim, latent_dim, k=None, sparsity_weight=1, decor_weight=.0005):\n","      super().__init__()\n","      self.encoder = nn.Linear( , , bias=False)\n","      # self.decoder = nn.Linear(latent_dim, input_dim, bias=False)\n","      # note: decoder is tied to encoder in forward()\n","\n","      self.sparsity_weight =\n","      self.decor_weight =\n","\n","      # k-sparse parameter defaults to 50% of input\n","      if k==None:\n","        self.k =\n","      else:\n","        self.k =\n","\n","    def forward(self, x):\n","\n","      # forward pass to the latent layer\n","      latent = F.relu(self.\n","\n","      # \"k-sparsify\": force sparsity by zeroing out small activations\n","      topk_vals = torch.topk(\n","      thresh = topk_vals[]. # kth-largest value is the smallest of the sorted top-k\n","      mask =  # mask is 0's and 1's\n","      latent_sparse =  # multiply the latent activations by the mask\n","\n","      # finally, decode via tied weights (hint: transpose)\n","      y = F.linear(, )\n","\n","      return y,latent_sparse\n","\n","    def sparsity_loss(self, z):\n","      return self.sparsity_weight * # L1 loss\n","\n","    # penalty on inter-latent covariance\n","    def decorrelation_loss(self, estLatent):\n","      cov = torch.cov(estLatent.T)\n","      off_diag = # just the off-diagonal elements\n","      return self.decor_weight * # sum of squared off-diagonal covariances\n","\n","\n","\n","  # create an instance of the autoencoder\n","  ae = SparseAE(input_dim=X.shape[1], k=num_latent//3, latent_dim=\n","  ae = ae.to(device)\n","  return ae"],"metadata":{"id":"LQWvtqVWWE4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### test with some data\n","\n","# get an MLP layer activation\n","nhidden =\n","X = activations['mlp_3'] # need to reshape\n","\n","# create an SAE model instance (using a 2x expansion for the latent layer)\n","aemodel = createTheSAE(\n","aemodel = aemodel.to(device)\n","\n","# and push some data through\n","aemodel(X)"],"metadata":{"id":"-yq3tJxiP4qg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4u_Hb-FWPDk0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Train layer-specific SAEs"],"metadata":{"id":"gJpy1LlRPxJi"}},{"cell_type":"code","source":["# takes ~3 mins for gpt2-small on GPU\n","# takes ~20 mins for gpt2-large on GPU\n","\n","# initialize\n","latentDensity = np.zeros\n","densityActivation =\n","finalloss =\n","\n","n_epochs =\n","\n","\n","## loop over layers\n","for layeri in\n","\n","  # get the activations from this layer, and reshape to (N x hidden)\n","  nhidden = # don't forget to skip the first token :)\n","  X = activations[f'mlp_{layeri}'].reshape\n","\n","  # create an SAE model for this layer (using a 2x expansion for the latent layer)\n","  aemodel = createTheSAE(\n","  aemodel = aemodel.to(device)\n","\n","  ### train the model\n","  optimizer = optim.Adam(\n","  mse_loss  =\n","\n","  for epoch in\n","\n","    # forward pass\n","    optimizer.zero_grad()\n","    x_pred,latent =\n","\n","    # backprop\n","    loss = # calculate and sum the two losses\n","    loss.backward()\n","    optimizer.step()\n","\n","  # final loss after all epochs\n","  finalloss[layeri] =\n","\n","\n","\n","  ## final run to get latent activations\n","  with torch.no_grad():\n","    aeout,latent = aemodel(X)\n","\n","  # convert to numpy and back to CPU\n","  latent =\n","\n","\n","  ### latent layer characteristics\n","  # mask for zero-valued activations\n","  densitymask = np.full(..,np.nan)\n","  densitymask[latent!=0] =\n","\n","  # density is the percent of nonzero activations per latent component\n","  densityPerComponent =\n","  latentDensity[layeri] = # average the density scores\n","\n","  # token-averaged activation magnitude, excluding zeros\n","  nonzeroAct =\n","  nonzeroAct[np.isnan(nonzeroAct)] =\n","\n","  # average nonzero activations per latent component (after minmax scaling)\n","  dpc = # min-max scale\n","  nza = # min-max scale\n","  densityActivation[layeri] =\n","\n","  print(f'Finished layer {layeri+1:2}/{model.config.n_layer}')"],"metadata":{"id":"jLASackBSye2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5JmpLl4JQPyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Visualize SAE laminar profiles"],"metadata":{"id":"_UIhbpmGQPqD"}},{"cell_type":"code","source":["# plot the results\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","axs[0].plot(,'ks',markerfacecolor=[.7,.7,.9],markersize=9)\n","axs[0].set(xlabel='Layer',ylabel='MSE Loss',title='Final SAE loss per layer')\n","\n","axs[1].plot(\n","axs[1].set(xlabel='Layer',ylabel='Density (% nonzero activations)',title='Latent density per layer')\n","\n","axs[2].plot(\n","axs[2].set(xlabel='Layer',ylabel='Density $\\\\times$ activation',title='(Density $\\\\times$ activation) per layer')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"MekaXD9xWO6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3o-HaL7LOMvz"},"execution_count":null,"outputs":[]}]}