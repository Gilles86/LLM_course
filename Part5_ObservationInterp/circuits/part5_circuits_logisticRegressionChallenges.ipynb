{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPEn9oxkwrdE5XH93OnkS9y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifying circuits and components<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Challenges with sparse logistic regression in large datasets<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"vlCAhKXwOThX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIO8xb7NbXpx"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LogisticRegression\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"ntGd8cHybf_G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The data"],"metadata":{"id":"Z2PnLNVhc2hd"}},{"cell_type":"code","source":["# data\n","n_neurons = 3000\n","n_tokens = 200\n","\n","data = np.random.randn(n_tokens,n_neurons)\n","\n","# data labels\n","labels = np.array([0]*(n_tokens//2) + [1]*(n_tokens//2))\n","\n","# shift data to create a constant offset\n","data += labels[:,None]*5\n","\n","\n","# show the histograms for the two labels\n","y0,x0 = np.histogram(data[labels==0,:].flatten(),100)\n","y1,x1 = np.histogram(data[labels==1,:].flatten(),100)\n","\n","fig,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","h = axs[0].imshow(data,aspect='auto',vmin=-3,vmax=6)\n","fig.colorbar(h,ax=axs[0],pad=.01)\n","axs[0].set(xlabel='Neuron index',ylabel='Token index',\n","              title='Image of activations')\n","\n","\n","axs[1].plot(x0[:-1],y0,linewidth=3,label='Category 0')\n","axs[1].plot(x1[:-1],y1,linewidth=3,label='Category 1')\n","\n","axs[1].set(xlabel='Activation value',ylabel='Count',ylim=[-100,None],\n","              title='Distributions of category-specific activations')\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_9-uaIhQbgc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3MENeWSnPOqC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sparsity by L1 regularization amount"],"metadata":{"id":"MoDUoQrzirah"}},{"cell_type":"code","source":["# values of C\n","Cvals = np.linspace(1,5,17)\n","sparsityByC = np.zeros(len(Cvals))\n","\n","# loop over C\n","for i,c in enumerate(Cvals):\n","  logreg = LogisticRegression(penalty='l1', solver='saga', C=c)\n","  logreg.fit(data,labels)\n","  sparsityByC[i] = 100*(logreg.coef_==0).mean()"],"metadata":{"id":"NHJcX-9Fj-SY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,5))\n","\n","plt.plot(Cvals,sparsityByC,'ks-',markerfacecolor=[.9,.7,.7],markersize=10)\n","plt.gca().set(xlabel='C parameter',ylabel='Sparsity (% total params)',title='Sparsity (proportion of zero-valued coefficients) as a function of C')\n","\n","plt.show()"],"metadata":{"id":"zjQ-eYX7j5WT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6MuUf3Gmc2kX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The sparse logistic regression"],"metadata":{"id":"JerUKR4Vc4HG"}},{"cell_type":"code","source":["# Run the logistic regression\n","logreg = LogisticRegression(penalty='l1', max_iter=1000, solver='saga', C=3)\n","logreg.fit(data,labels)\n","coefs = logreg.coef_.squeeze() # beta values\n","\n","\n","### model performance results\n","#  accuracy (do the predictions match the true labels?)\n","accuracy = 100*(logreg.predict(data) == labels).mean()\n","\n","\n","# get sparsity\n","sparsity = 100*(coefs==0).mean()\n","print(f'Accuracy: {accuracy:.2f}, Sparsity: {sparsity:.2f}%')"],"metadata":{"id":"lyfHdOqXbgXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI, the actual number of iterations that were run\n","logreg.n_iter_"],"metadata":{"id":"W6fELN9kjEMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# also FYI, the intercept term is stored separately\n","print(logreg.intercept_)\n","coefs.shape"],"metadata":{"id":"NeizKhTXvcuw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iXSthQ7g124_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Large effect sizes with beta=0 coefficients"],"metadata":{"id":"aUtfKwPujEH8"}},{"cell_type":"code","source":["# effect size (Cohen's d)\n","mean_diff = data[labels==1,:].mean(axis=0) - data[labels==0,:].mean(axis=0)\n","pooled_sd = ( data[labels==1,:].std(axis=0) + data[labels==0,:].std(axis=0) )/2\n","cohens_d = mean_diff / (pooled_sd)\n","\n","# and show the results\n","plt.figure(figsize=(8,6))\n","\n","plt.plot(coefs[coefs!=0],cohens_d[coefs!=0],'ko',markersize=8,alpha=.6,markerfacecolor=[.7,.9,.7])\n","plt.plot(coefs[coefs==0],cohens_d[coefs==0],'ko',markersize=8,alpha=.8,markerfacecolor=[.9,.7,.7])\n","\n","plt.gca().set(ylabel=\"Effect size (Cohen's d)\",xlabel=r'$\\beta$ coefficient',title='Large effect sizes in zeroed coefficients')\n","plt.grid(linestyle='--',linewidth=.4)\n","\n","plt.show()"],"metadata":{"id":"dbRhmt09bgUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"L5YnYVrvirX_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Negative betas?!"],"metadata":{"id":"dMhI02J1irVJ"}},{"cell_type":"code","source":["# find the min and max coefficient\n","minbeta = coefs.argmin()\n","maxbeta = coefs.argmax()\n","\n","# show their distributions\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","axs[0].hist(data[labels==0,minbeta],bins=20,color=[.9,.7,.7],edgecolor='k',linewidth=.1,label='Label 0')\n","axs[0].hist(data[labels==1,minbeta],bins=20,color=[.7,.9,.7],edgecolor='k',linewidth=.1,label='Label 1')\n","axs[0].set(title=f'Neuron {minbeta} with $\\\\beta$={coefs[minbeta]:.2f}',xlabel='Data value',ylabel='Count')\n","\n","axs[1].hist(data[labels==0,maxbeta],bins=20,color=[.9,.7,.7],edgecolor='k',linewidth=.1,label='Label 0')\n","axs[1].hist(data[labels==1,maxbeta],bins=20,color=[.7,.9,.7],edgecolor='k',linewidth=.1,label='Label 1')\n","axs[1].set(title=f'Neuron {maxbeta} with $\\\\beta$={coefs[maxbeta]:.2f}',xlabel='Data value',ylabel='Count')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"6UYOVzoFkSYS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run another regression with only those two neurons\n","extremeData = data[:,[minbeta,maxbeta]]\n","\n","logreg2 = LogisticRegression()\n","logreg2.fit(extremeData,labels)\n","print(logreg2.coef_)"],"metadata":{"id":"89icsGGAkSRh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# these neurons are strongly correlated, so their contributions are redundant\n","plt.plot(extremeData[labels==0,0],extremeData[labels==0,1],'ko',alpha=.5,markerfacecolor=[.7,.9,.9],label='Category 0')\n","plt.plot(extremeData[labels==1,0],extremeData[labels==1,1],'ko',alpha=.5,markerfacecolor=[.9,.7,.9],label='Category 1')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Min-beta neuron',ylabel='Max-beta neuron',label=f'r = {np.corrcoef(extremeData.T)[0,1]:.2f}')\n","plt.show()"],"metadata":{"id":"vBv3o0Ft8XRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vfNDH_rTkSfd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Which neurons get selected?"],"metadata":{"id":"FNpaF3gnkSbi"}},{"cell_type":"code","source":["plt.figure(figsize=(8,5))\n","plt.plot(coefs[coefs!=0],data[:,coefs!=0].var(axis=0),'ko',markersize=8,alpha=.6,markerfacecolor=[.7,.9,.9],label='Non-zero coefs')\n","plt.plot(coefs[coefs==0],data[:,coefs==0].var(axis=0),'ko',markersize=8,alpha=.6,markerfacecolor=[.9,.7,.7],label='Zero coefs')\n","plt.legend()\n","plt.gca().set(ylabel='Data variance',xlabel=r'$\\beta$ coefficient',title='Variance in zeroed coefficients')\n","\n","plt.show()"],"metadata":{"id":"DA6N1gUvkSUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# also doesn't trivially follow inter-variable correlation\n","R0 = np.corrcoef(data[n_tokens//2:,coefs==0].T)\n","R1 = np.corrcoef(data[n_tokens//2:,coefs!=0].T)\n","\n","y0,x0 = np.histogram(R0[np.nonzero(np.triu(R0,1))],80,density=True)\n","y1,x1 = np.histogram(R1[np.nonzero(np.triu(R1,1))],80,density=True)\n","\n","plt.figure(figsize=(10,3))\n","plt.plot(x0[:-1],y0,label=r'$\\beta = 0$')\n","plt.plot(x1[:-1],y1,label=r'$\\beta \\neq 0$')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"biFBQy0MVfq5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"maYIfgQ5uBwU"},"execution_count":null,"outputs":[]}]}