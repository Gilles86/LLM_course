{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM+JwO2mTIPlEm+WOoyHeV8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Cosine similarity (advanced)<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"P1e8oiHCs8Sf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUh283u8V78F"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","import torch\n","\n","# high res matplotlib\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["from transformers import GPT2Model,GPT2Tokenizer\n","\n","# import GPT-2 model, extract tokenizer and embeddings\n","gpt2 = GPT2Model.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","embedding = gpt2.wte.weight.detach().numpy()"],"metadata":{"id":"Iq4WDlRLWHlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"03EJE6bIML4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Manual and Pytorch cosine similarity"],"metadata":{"id":"GBp5xHDIWHgD"}},{"cell_type":"code","source":["# pick two random tokens\n","tokenpair = np.random.choice(np.arange(3000,6001),2)\n","\n","# get their embedding vectors\n","v1 = embedding[\n","v2 =\n","\n","print(f'Token pair: \"{\n","print(f'Embedding shape:"],"metadata":{"id":"BVWx6WdRWnoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate cosine similarity manually\n","num =\n","norm_v1 =\n","norm_v2 =\n","den =\n","\n","print(f'Shape of vectors: {v1.shape}')\n","manual_cs = num/den"],"metadata":{"id":"f9bH1H16daK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and now in torch\n","v1_torch = # make sure it's a dimensional vector\n","v2_torch =\n","\n","print(f'Shape of torch vectors: {v1_torch.shape}')\n","torch_cs = torch."],"metadata":{"id":"jASTkdEveVF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the results\n","print(f'Manual cosine similarity:  {:.5f}')\n","print(f'Pytorch cosine similarity: {:.5f}')"],"metadata":{"id":"WT_mXSuJWHdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0cOKWmGMlNMX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Matrix of pairwise similarities"],"metadata":{"id":"SXJsuGHKfP2m"}},{"cell_type":"code","source":["# using a for-loop\n","numtoks = 30\n","\n","E = embedding\n","\n","# FYI, mean-centering would give correlation coefficient\n","# E -= np.mean(E,axis=1,keepdims=True)\n","\n","print(f'Embedding submatrix has size {E.shape}')\n","\n","cs_matrix = np.zeros(\n","\n","for i in range(numtoks):\n","  for j in range(numtoks):\n","\n","    # cosine similarity\n","    num = np.sum(\n","    den = np.sqrt\n","\n","    # slot into the matrix\n","    cs_matrix[i,j] = /\n","\n","\n","# and show the matrix\n","plt.figure(figsize=(6,5))\n","\n","plt.imshow(,vmin=.2,vmax=.8)\n","ticklabels =\n","plt.gca().set(xticks=range(numtoks),xticklabels=ticklabels,\n","              yticks=range(numtoks),yticklabels=ticklabels)\n","plt.xticks(rotation=45)\n","\n","\n","plt.show()"],"metadata":{"id":"rmjWStxcfPz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat using matrix multiplication in pytorch\n","Et = # first convert to pytorch tensor\n","\n","# normalize (note the vector_norm not matrix_norm!)\n","Et_norm =  / torch.linalg.vector_norm\n","\n","# cosine similarity matrix\n","cs_matrixT =\n","\n","# need to conver to numpy first :P\n","print(f'Mean absolute difference = {:.10f}')"],"metadata":{"id":"HA5rbc0JnT1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vJ7yNS6KfPrP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Softmaxify the cosine similarities matrix"],"metadata":{"id":"0F5uwZWZfPot"}},{"cell_type":"code","source":["numtoks = 6\n","\n","Et = torch.tensor\n","Et_norm =\n","cs_matrixP =\n","cs_matrixP"],"metadata":{"id":"vbQsSB6L1M_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a mask of -infs\n","infmask = torch.tril\n","\n","# mask the cosine similarities\n","mat4softmax = ... + infmask\n","\n","print(infmask), print('')\n","print(mat4softmax)"],"metadata":{"id":"MXioWWeAfPl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# softmax!\n","softmat = torch.exp() /\n","\n","# print the results\n","print(softmat)\n","print(f'\\nSum over all matrix elements = {torch.sum(softmat)}')"],"metadata":{"id":"Qwq4WmOfpL5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nan mask\n","nanmask"],"metadata":{"id":"ycW1YUY6DKTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize the relationship between softmax(cs) and cs\n","C = # copy of cs_matrixP\n","S = # copy of softmat\n","\n","# replace lower-triangle with nan's\n","\n","\n","print(S)\n","\n","# and plot\n","plt.plot(,,'ko')\n","plt.gca().set(xlabel='Cosine similarity',ylabel='Softmax-cosine')\n","plt.show()"],"metadata":{"id":"VQ7aT9EPpLzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4DMu3QbhfPjD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: One-to-all similarity"],"metadata":{"id":"qRJcKfz7fPft"}},{"cell_type":"code","source":["randtoken = np.random.randint(0,tokenizer.vocab_size,1)\n","\n","# vector,matrix\n","one2all_cs = torch.cosine_similarity(torch.tensor(embedding[randtoken,:]),torch.tensor(embedding))\n","print(f'Size of embedding matrix: {embedding.shape}')\n","print(f'Size of one2all_cs: {one2all_cs.shape}')"],"metadata":{"id":"JKhaIuWkpHns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup the figure\n","fig = plt.figure(figsize=(12,4))\n","gs = GridSpec(1,4)\n","ax0 = fig.add_subplot(gs[:-1])\n","ax1 = fig.add_subplot(gs[-1])\n","\n","# plot all similarities\n","ax0.plot(\n","\n","\n","# their distribution\n","ax1.hist(\n","ax1.set(xlabel='Cosine similarity',ylabel='Count',yticks=[],title='Distribution of cs')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"T4Q_tQwTpHqh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.topk to find the 10 closest embeddings\n","values,indices =\n","\n","print(f'Top 10 similar tokens to \"{tokenizer.decode(randtoken)}\":')\n","for v,i in\n","  print(f'  Sc = {v:.3f} with token \"{tokenizer.decode(i)}\"')"],"metadata":{"id":"odzJFmRq2dTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xAgcNcs3pHyp"},"execution_count":null,"outputs":[]}]}