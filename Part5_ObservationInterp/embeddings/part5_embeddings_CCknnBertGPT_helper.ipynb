{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOtep8dbZoaUQNJEVf3O5Tf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: BERT v GPT kNN kompetition<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"C3tvDUkVtF80"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTp8j3TJAqvB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# load BERT tokenizer and model\n","from transformers import BertTokenizer, BertModel\n","\n","model = BertModel.from_pretrained('bert-base-uncased')\n","tokenizerB = BertTokenizer.from_pretrained('bert-base-uncased')\n","embeddingsB = model.embeddings.word_embeddings.weight.detach().numpy()"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pretrained GPT-2 model and tokenizer\n","from transformers import GPT2Model,GPT2Tokenizer\n","model = GPT2Model.from_pretrained('gpt2')\n","tokenizerG = GPT2Tokenizer.from_pretrained('gpt2')\n","embeddingsG = model.wte.weight.detach().numpy()"],"metadata":{"id":"Tn9guU3EWzMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sj-DN5Owb_ed"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: kNN-based synonym-searching of \"ring\""],"metadata":{"id":"IYa6IRn8NZQK"}},{"cell_type":"code","source":["# define a normalized \"seed\" vector\n","seedword = 'ring'\n","\n","# check in BERT\n","seedidxB = tokenizerB.encode\n","\n","# check in GPT\n","seedidxG =\n","\n","print(f'In BERT: \"{seedword}\" has index {seedidxB}')\n","print(f'In GPT2:"],"metadata":{"id":"j5Mq2VmS9Q5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Euclidean distances\n","eucDist_bert =\n","eucDist_gpt2 =\n","\n","# visualize the distributions\n","plt.figure(figsize=(10,4))\n","yB,xB = np.histogram(,bins=,density=)\n","yG,xG =\n","\n","plt.plot(,label='GPT2')\n","plt.plot(,label='BERT')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"38dhFl7YNc8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sort and get top k\n","k = 15\n","topKidx_Bert =\n","topKidx_gpt2 =\n","\n","# and print\n","print('|        BERT        |        GPT2          |')\n","print('|--------------------|----------------------|')\n","for b,g in zip(topKidx_Bert,topKidx_gpt2):\n","  print"],"metadata":{"id":"f7YhIKAmNZNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"31LgVYBy6fBx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Using normalized vectors"],"metadata":{"id":"Y1We4K82-rQj"}},{"cell_type":"code","source":["# normalize\n","embeddingsBnorm =  / np.linalg.norm(\n","embeddingsGnorm =  / embeddingsG...\n","\n","# Euclidean distances\n","eucDist_bert =\n","eucDist_gpt2 = np.sqrt(np.sum( ()**2 ,axis=1))\n","\n","\n","# visualize the distributions\n","plt.figure(figsize=(10,4))\n","yB,xB = np.histogram(eucDist_bert[ # only the nonzero elements\n","\n","\n","plt.show()"],"metadata":{"id":"I1rjjUu6_aW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sort and get top k\n","k = 15\n","topKidx_Bert =\n","topKidx_gpt2 = np.argsort(\n","\n","# and print\n","print('|        BERT        |        GPT2          |')\n","print('|--------------------|----------------------|')\n","for b,g in zip(topKidx_Bert,topKidx_gpt2):\n","  print(f'  {} ({})  |  ({}) {}')"],"metadata":{"id":"_bplpRXL-rM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zbnBxp_B-rEy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: The importance of emptiness"],"metadata":{"id":"vTEQfPdlCZMZ"}},{"cell_type":"code","source":["# no new code here :P"],"metadata":{"id":"PNMuLY2gCb7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9qXzL5r-CZDp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Multitoken words in GPT2"],"metadata":{"id":"NvBcZOJrwK_k"}},{"cell_type":"code","source":["# define a normalized \"seed\" vector\n","seedword = 'beauty'\n","\n","# check in BERT\n","seedidxB =\n","\n","# check in GPT\n","seedidxG =\n","\n","print(f'In BERT: \"{seedword}\" has index {seedidxB}')\n","print(f'In GPT2: \"{seedword}\" has index {seedidxG}')"],"metadata":{"id":"ToZUTBKUwO70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Euclidean distances\n","eucDist_bert\n","eucDist_gpt2\n","\n","# sort and get top k\n","\n","# and print\n"],"metadata":{"id":"Xg13M_nIwKtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o6XJHybuAjY7"},"execution_count":null,"outputs":[]}]}