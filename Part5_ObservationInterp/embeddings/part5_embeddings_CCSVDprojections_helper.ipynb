{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPToUKdU5su+TvmYei88z3C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: SVD projections of related embeddings<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"Ct-OEMBkJ-wV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTp8j3TJAqvB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# load BERT tokenizer and model\n","from transformers import BertTokenizer, BertModel\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","embeddings = model.embeddings.word_embeddings.weight.detach().numpy()"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FF6s6MTMKKec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Tokenize and create embeddings submatrices\n"],"metadata":{"id":"nKAaswpTKKbu"}},{"cell_type":"code","source":["digitTokens = np.zeros(10,dtype=int)\n","\n","# find the token index for this numer\n","for i in range(10):\n","\n","  # confirm they're all single-token words\n","  toks =\n","  print(f'{len(toks)} token for \"{i}\"')\n","\n","  digitTokens[i] ="],"metadata":{"id":"ukSKw-qmKPIA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of 10 EU countries\n","EUstates = ['estonia','france','germany','italy','latvia','lithuania','netherlands','poland','romania','slovenia' ]\n","\n","\n","# find the token index for this numer\n","EUtokens =\n","for i in range(len(EUstates)):\n","\n","  # confirm they're all single-token words\n","\n","\n","  EUtokens[i] ="],"metadata":{"id":"DvbPZRVJU0Ch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the category-mean embeddings vector for later centering\n","\n","# for countries (expand to a row vector for later broadcasting)\n","EU_embedMean = embeddings[].mean(\n","\n","# and for digits\n","digs_embedMean =\n","\n","\n","# any obvious relationship?\n","plt.plot(EU_embedMean,digs_embedMean"],"metadata":{"id":"wKp6eh4ZZgHJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create mean-centered embeddings submatrices\n","\n","# numbers\n","subembDigs = embeddings -\n","\n","# countries\n","subembEU = embeddings -"],"metadata":{"id":"jay5W0ulLT66"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uBLJaftygWl_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Extract and visualize the singular value spectrum"],"metadata":{"id":"iGYjCo9TgWjD"}},{"cell_type":"code","source":["# SVDs (note: python returns Vt [technically Vh], so the rows of V are the singular vectors)\n"," = np.linalg.svd(subembDigs)\n","\n","# and for EU\n","np.linalg.svd\n","\n","# print sizes\n","print(f'Embeddings is size {subembDigs.shape}')\n","print(f'U  is size {\n","print(f's  is size {\n","print(f'Vh is size {"],"metadata":{"id":"FacwE7BciSDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","_,axs = plt.subplots(1,2,figsize=(12,5))\n","\n","# plot their spectra\n","axs[0].plot(s_dig\n","axs[0].plot(s_EU\n","axs[0].legend()\n","\n","# plot the top singular vectors\n","axs[1].set(xlabel='EU basis vector',ylabel='Digits basis vector',\n","           title=f'Singular vectors (r = {np.corrcoef(V_EU[0],V_dig[0])[0,1]:.2f})')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Bwkkw0quLT4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vaBHqdkrLTyn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Project embeddings onto basis vectors"],"metadata":{"id":"fMR6nmYBKgDk"}},{"cell_type":"code","source":["# projection of EU-centered embeddings onto the top eigenvector (first row of Vt)\n","projEU =  @ V_EU[0,:]\n","projDg = (embeddings-digs_embedMean)\n","\n","plt.figure(figsize=(10,4))\n","\n","# histograms\n","\n","plt.legend()\n","\n","plt.gca().set(xlabel='Embedding dimension',ylabel='Density',title='Distributions of projections')\n","plt.show()"],"metadata":{"id":"WjiBEKMUPBsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print top 30 projection tokens\n","sortidx = np.argsort(projEU)\n","\n","# print the top positive projections\n","print('--- POSITIVE projections ---')\n","for i in range(30):\n","\n","  # get this token\n","  token =\n","\n","  # print if it's not in the 'seed' list\n","  if not token in EUstates:\n","    print(f'  {} for \"{token}\"')\n","\n","\n","# repeat for top negative projections\n","print('\\n\\n\\n--- NEGATIVE projections ---')\n"],"metadata":{"id":"WNK2Iz0uPBo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat for numbers\n","\n","# print the top positive projections\n","print('--- POSITIVE projections ---')\n","\n","\n","\n","# repeat for top negative projections\n","print('\\n\\n\\n--- NEGATIVE projections ---')\n","\n"],"metadata":{"id":"T3OjD809PBmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LcpEp4h6PBjG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Cosine similarities of the embeddings"],"metadata":{"id":"XzZzGnT1Py0c"}},{"cell_type":"code","source":["# normalize each vector to its norm (unit length)\n","E_digs =  / np.linalg.norm\n","E_EU   =  / np.linalg.norm\n","\n","# cosine similarity matrices\n","csM_EU = @\n","csM_dg = @\n","\n","fig,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","h = axs[0].imshow(csM_EU,vmin=-.5,vmax=.5)\n","axs[0].set(xticks=range(10),xticklabels=EUstates,yticks=range(10),yticklabels=EUstates)\n","axs[0].tick_params(axis='x',labelrotation=90)\n","fig.colorbar(h,ax=axs[0],pad=.02)\n","\n","axs[1]."],"metadata":{"id":"zdZH66NJNWpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OvIEzaaINWiG"},"execution_count":null,"outputs":[]}]}