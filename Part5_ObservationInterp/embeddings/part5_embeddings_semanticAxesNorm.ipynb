{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNW4bfcK/aOKomtTDVOVXbd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Creating and interpreting linear \"semantic axes\"<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"8roYdi_W2sVf"}},{"cell_type":"code","source":["# !pip install gensim"],"metadata":{"id":"8an79zirRyfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lh-aPx04n2Y"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","\n","import gensim.downloader as api\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# import word2vec\n","w2v = api.load('word2vec-google-news-300')"],"metadata":{"id":"qos7Eh4J4rT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7DeTfZ5w7uI6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Normalize all embedding vectors"],"metadata":{"id":"OJkvmPV-7uGD"}},{"cell_type":"code","source":["print(w2v.vectors.shape)\n","vectors_norm = w2v.vectors / np.linalg.norm(w2v.vectors, axis=1, keepdims=True)"],"metadata":{"id":"tbJ0xtTD20zA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tDal4RAA7uAU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create a \"semantic axis\""],"metadata":{"id":"H6ApwsN67t6q"}},{"cell_type":"code","source":["# pick two words to define the axis\n","word4pos = 'future'#'good'#\n","word4neg = 'past'#'evil'#\n","\n","# get the vectors for those words\n","v2add = w2v[word4pos]\n","v2sub = w2v[word4neg]\n","\n","# create the \"semantic axis\" with \"raw\" vectors\n","semantic_axis = v2add - v2sub\n","semantic_axis /= np.linalg.norm(semantic_axis) # post-subtraction normalization\n","\n","# now starting from the normed vectors\n","v2add = vectors_norm[w2v.key_to_index[word4pos],:]\n","v2sub = vectors_norm[w2v.key_to_index[word4neg],:]\n","semantic_axis_norm = v2add - v2sub\n","\n","\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","axs[0].plot(semantic_axis,label='Pre-norm')\n","axs[0].plot(semantic_axis_norm,label='Post-norm')\n","axs[0].legend()\n","axs[0].set(xlabel='Embeddings dimension',ylabel='Embedding weight')\n","\n","axs[1].plot(semantic_axis,semantic_axis_norm,'ks',markerfacecolor=[.7,.7,.9])\n","axs[1].set(xlabel='Difference of \"raw\" vectors',ylabel='Difference of normed vectors')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"BGpB_3A820wV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Jf-4Ctms8U6x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Filter for \"real\" words"],"metadata":{"id":"pxrT1vLo8U4W"}},{"cell_type":"code","source":["testwords = [ 'theInternet.com','health','FRITZ!Box','headphones' ]\n","filterWords = np.where([word.isalpha() and len(word)>2 for word in testwords])[0]\n","\n","print('Word set:')\n","print([w for w in testwords])\n","\n","print('\\nIncluded words:')\n","print([testwords[w] for w in filterWords])\n","\n","print('\\nExcluded words:')\n","print([testwords[w] for w in ~filterWords])"],"metadata":{"id":"MZBJ04XG8YYj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["allwords = list(w2v.key_to_index.keys())\n","words2use = np.where([word.isalpha() and len(word)>2 for word in allwords])[0]\n","\n","# to test without filtering:\n","# words2use = np.arange(len(allwords))\n","\n","# report\n","print(f'{len(words2use):,} out of {len(allwords):,} ({100*len(words2use)/len(allwords):.2f}%) tokens kept.')"],"metadata":{"id":"ys66KKk68LNy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RpMvBhnSNNq0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Project all words onto the axis"],"metadata":{"id":"_q6LZCQz8NoM"}},{"cell_type":"code","source":["# calculate dot products\n","# dotprods = vectors_norm[words2use] @ semantic_axis\n","dotprods = w2v.vectors[words2use] @ semantic_axis\n","\n","# find top and bottom 10 highest scores\n","top10 = dotprods.argsort()[-10:][::-1]\n","bot10 = dotprods.argsort()[:10]\n","\n","\n","# print them out\n","print('10 most positive-projected words:')\n","for widx in top10:\n","  print(f' Similarity of {dotprods[widx]:.3f} for \"{w2v.index_to_key[words2use[widx]]}\"')\n","\n","print('\\n10 most negative-projected words:')\n","for widx in bot10:\n","  print(f' Similarity of {dotprods[widx]:.3f} for \"{w2v.index_to_key[words2use[widx]]}\"')"],"metadata":{"id":"mEUFF3S38NCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qeefT_km7t3j"},"execution_count":null,"outputs":[]}]}