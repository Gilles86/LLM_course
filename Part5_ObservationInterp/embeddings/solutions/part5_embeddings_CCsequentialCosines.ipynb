{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyN/jzBmefJy2d1ldehPH56A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Cosine similarity in word sequences<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"6zFL-Z90sAGZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTp8j3TJAqvB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","\n","# load BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the embeddings matrix\n","embeddings = model.embeddings.word_embeddings.weight.detach().numpy()\n","nVectors = embeddings.shape[1]\n","\n","# Check shape of embedding matrix (vocab size Ã— embedding dim)\n","print(f'Embedding matrix shape: {embeddings.shape}')"],"metadata":{"id":"Upgfe2mABWNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9v4KCU4fsDFc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Cosine similarity of sequential tokens"],"metadata":{"id":"rytwUuiPsDCb"}},{"cell_type":"code","source":["sentence = \"my phone is in the kitchen near the cold ice cream\"\n","\n","# tokenize the sentence\n","tokens = tokenizer.encode(sentence)[1:-1] # ignore the sep/cls tokens\n","\n","# initialize cosine similarity\n","cossim = np.full(len(tokens),np.nan)\n","\n","# calculate cosine similarity for successive word pairs\n","for ti in range(1,len(tokens)):\n","  v1 = embeddings[tokens[ti],:]\n","  v2 = embeddings[tokens[ti-1],:]\n","  cossim[ti] = np.sum(v1*v2) / np.sqrt( np.sum(v1**2)*np.sum(v2**2) )\n","\n","\n","# plot!\n","plt.figure(figsize=(12,4))\n","plt.bar(np.arange(len(cossim)),cossim,facecolor=[.7,.7,.9],edgecolor='k')\n","plt.gca().set(xticks=range(0,len(tokens)),xticklabels=[tokenizer.decode(t) for t in tokens])\n","plt.gca().tick_params(axis='x',rotation=-45)\n","\n","plt.title('Cosine similarities of sequential token embeddings',fontweight='bold')\n","plt.show()"],"metadata":{"id":"4TPJlmC-4t18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nUyLPFK85IbP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Words that change in meaning"],"metadata":{"id":"7wCKGjxN5ISo"}},{"cell_type":"code","source":["sentences = [\n","    'The conductor waved his hands as the train departed',\n","    'The conductor waved his hands as the orchestra began'\n","]\n","\n","\n","plt.figure(figsize=(12,4))\n","\n","for i,sent in enumerate(sentences):\n","\n","  # tokenize the sentence\n","  tokens = tokenizer.encode(sent)[1:-1] # ignore the sep/cls tokens\n","\n","  # initialize cosine similarity\n","  cossim = np.full(len(tokens),np.nan)\n","\n","  # calculate cosine similarity for successive word pairs\n","  for ti in range(1,len(tokens)):\n","    v1 = embeddings[tokens[ti],:]\n","    v2 = embeddings[tokens[ti-1],:]\n","    cossim[ti] = np.sum(v1*v2) / np.sqrt( np.sum(v1**2)*np.sum(v2**2) )\n","\n","\n","  # plot!\n","  plt.plot(cossim,['ks-','bo-'][i],markersize=10,markerfacecolor=[.7,.7,.7],label=tokenizer.decode(tokens[-2:]))\n","\n","\n","# finish the plot\n","xticklabs = [tokenizer.decode(t) for t in tokens]\n","xticklabs[-2] = f'train/\\norchestra'\n","xticklabs[-1] = 'departed/\\nbegain'\n","plt.gca().set(xticks=range(0,len(tokens)),xticklabels=xticklabs)\n","plt.tick_params(axis='x',rotation=-45)\n","plt.legend()\n","\n","plt.title('Cosine similarities of sequential token embeddings',fontweight='bold')\n","plt.show()"],"metadata":{"id":"JAbk76op5IQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xK_K9UfS4t7M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Multiple examples"],"metadata":{"id":"zyG3vi6rsC82"}},{"cell_type":"code","source":["sentences = [\n","    'The complex houses married and single soldiers and their families',\n","    'four score and seven years ago',\n","    'i like my coffee how i like my broccoli',\n","    'After the spring rain we decided to spring into action'\n","]\n","\n","\n","_,axs = plt.subplots(2,2,figsize=(14,7))\n","axs = axs.flatten()\n","colors = [ [.7,.7,.9],[.7,.9,.9],[.9,.7,.7],[.7,.9,.7] ]\n","\n","for i,sent in enumerate(sentences):\n","\n","  # tokenize the sentence\n","  tokens = tokenizer.encode(sent)[1:-1] # ignore the sep/cls tokens\n","\n","  # initialize cosine similarity\n","  cossim = np.full(len(tokens),np.nan)\n","\n","  # calculate cosine similarity for successive word pairs\n","  for ti in range(1,len(tokens)):\n","    v1 = embeddings[tokens[ti],:]\n","    v2 = embeddings[tokens[ti-1],:]\n","    cossim[ti] = np.sum(v1*v2) / np.sqrt( np.sum(v1**2)*np.sum(v2**2) )\n","\n","\n","  # plot!\n","  axs[i].bar(np.arange(len(cossim)),cossim,facecolor=colors[i],edgecolor='k')\n","  axs[i].set(xticks=range(0,len(tokens)),xticklabels=[tokenizer.decode(t) for t in tokens])\n","  axs[i].tick_params(axis='x',rotation=-45)\n","\n","\n","plt.suptitle('Cosine similarities of sequential token embeddings',fontweight='bold')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"3ix8tOzhsC5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r8Zk85NHsClL"},"execution_count":null,"outputs":[]}]}