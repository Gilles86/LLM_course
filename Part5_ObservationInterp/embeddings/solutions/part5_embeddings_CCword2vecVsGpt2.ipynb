{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyN7m11fWOr/CnFUGwBN/hsE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Word2vec vs. GPT2<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"H9PFUlbmtJ1t"}},{"cell_type":"code","source":["# !pip install gensim"],"metadata":{"id":"MbwCFir0iI3l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lh-aPx04n2Y"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import gensim.downloader as api\n","from transformers import GPT2Model,GPT2Tokenizer\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# import word2vec\n","w2v = api.load('word2vec-google-news-300')"],"metadata":{"id":"qos7Eh4J4rT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YrLzeYrjUNV9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pretrained GPT-2 model and tokenizer\n","gpt2 = GPT2Model.from_pretrained('gpt2')\n","gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# GPT embeddings matrix\n","gpt_embedding = gpt2.wte.weight.detach().numpy()"],"metadata":{"id":"Iq4WDlRLWHlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o2s2NHrJWHi8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Find 100 matching tokens"],"metadata":{"id":"3nz5jQ8h4rLj"}},{"cell_type":"code","source":["# get the word2vec vocab\n","w2v_tokens = list(w2v.key_to_index.keys())\n","w2v_tokens[:10]"],"metadata":{"id":"3xHa5JHo8h35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# same for gpt2\n","gpt2_tokens = [gpt_tokenizer.decode([i]) for i in range(gpt_tokenizer.vocab_size)]\n","gpt2_tokens[10000:10010]"],"metadata":{"id":"j7pvZNo5mxhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find 6-letter tokens in word2vec, and see if they match in gpt2.\n","# take the first 100 matches for RSA\n","tokens2compare = []\n","\n","# loop over word2vec words\n","for word in w2v_tokens:\n","\n","  # skip if word is not 6 characters long\n","  if len(word)!=6: continue\n","\n","  # check if it exists in gpt\n","  try:\n","    gpt2_tokens.index(word) # just see if it works\n","    tokens2compare.append(word)\n","  except: pass\n","\n","  # stopping criteria\n","  if len(tokens2compare)>99:\n","    break"],"metadata":{"id":"1ezwTwvv4rH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for word in tokens2compare:\n","  print(f'\"{word}\" is index {w2v_tokens.index(word):4} in w2v and index {gpt2_tokens.index(word):5} in GPT2.')"],"metadata":{"id":"83NkIwKf4q6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZgALQZwwoRXt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Create embeddings matrices"],"metadata":{"id":"NhH0T7Zu9Win"}},{"cell_type":"code","source":["# embeddings matrix for these words\n","E_w2v = np.array([w2v[w] for w in tokens2compare])\n","E_gpt = np.array([gpt_embedding[gpt_tokenizer.encode(w)[0],:] for w in tokens2compare])"],"metadata":{"id":"9FSjp1RS9WgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check matrices sizes\n","print(f'Size of w2v matrix: {E_w2v.shape}')\n","print(f'Size of gpt matrix: {E_gpt.shape}')\n","\n","# sanity-check that they're really different\n","plt.figure(figsize=(10,4))\n","plt.plot(range(E_gpt.shape[1]),E_gpt[0,:],'o-',label='GPT2')\n","plt.plot(range(E_w2v.shape[1]),E_w2v[0,:],'s-',label='word2vec')\n","plt.gca().set(xlim=[-5,E_gpt.shape[1]+5],xlabel='Dimension',ylabel='Value',title=f'Embeddings for \"{tokens2compare[0]}\"')\n","\n","plt.legend(fontsize=10)\n","plt.show()"],"metadata":{"id":"iCEvfLal9WdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z6ZsXYUIwkZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Cosine similarity matrices"],"metadata":{"id":"BkODSOsBwkW5"}},{"cell_type":"code","source":["# normalize each vector to its norm (unit length)\n","E_w2v_norm = E_w2v / np.linalg.norm(E_w2v,axis=1,keepdims=True)\n","E_gpt_norm = E_gpt / np.linalg.norm(E_gpt,axis=1,keepdims=True)\n","\n","# cosine similarity matrices\n","cs_matrix_w2v = E_w2v_norm @ E_w2v_norm.T\n","cs_matrix_gpt = E_gpt_norm @ E_gpt_norm.T"],"metadata":{"id":"wk0DAW1nq_hT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(12,5))\n","\n","skip = 5\n","\n","# word2vec\n","h = axs[0].imshow(cs_matrix_w2v,vmin=.1,vmax=.6,cmap='plasma')\n","axs[0].set(xticks=range(0,len(tokens2compare),skip),xticklabels=tokens2compare[::skip],\n","           yticks=range(1,len(tokens2compare),skip),yticklabels=tokens2compare[1::skip],\n","           title='Cossim matrix for word2vec')\n","axs[0].tick_params(axis='x',labelrotation=90)\n","fig.colorbar(h,ax=axs[0],pad=.02)\n","\n","\n","# GPT2\n","h = axs[1].imshow(cs_matrix_gpt,vmin=.1,vmax=.6,cmap='plasma')\n","axs[1].set(xticks=range(0,len(tokens2compare),skip),xticklabels=tokens2compare[::skip],\n","           yticks=range(1,len(tokens2compare),skip),yticklabels=tokens2compare[1::skip],\n","           title='Cossim matrix for GPT-2')\n","axs[1].tick_params(axis='x',labelrotation=90)\n","fig.colorbar(h,ax=axs[1],pad=.02)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"8uxAeD13raj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LG_-kCkV9WR7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Quantitative comparison via RSA"],"metadata":{"id":"PEgTG_hhw-7q"}},{"cell_type":"code","source":["# extract the upper-triangular elements\n","unique_w2v = cs_matrix_w2v[np.triu_indices_from(cs_matrix_w2v, k=1)]\n","unique_gpt = cs_matrix_gpt[np.triu_indices_from(cs_matrix_gpt, k=1)]\n","\n","# Pearson correlation\n","r = np.corrcoef(unique_w2v,unique_gpt)[0,1]\n","\n","# cosine similarity\n","num = sum(unique_w2v*unique_gpt)\n","den = sum(unique_w2v**2) * sum(unique_gpt**2)\n","sc = num/np.sqrt(den)\n","\n","# plot\n","plt.plot(unique_w2v,unique_gpt,'ks',markerfacecolor=[.7,.9,.7,.7])\n","plt.gca().set(xlim=[-.2,1],ylim=[-.2,1],xlabel='w2v cosine similarities',ylabel='GPT cosine similarities',\n","              title=f'r = {r:.3f}, $S_c$ = {sc:.3f}')\n","plt.axhline(0,linestyle='--',color=[.8,.8,.8])\n","plt.axvline(0,linestyle='--',color=[.8,.8,.8],zorder=-19)\n","\n","plt.show()"],"metadata":{"id":"8ZEITW0QxA7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"p8Dp-2KtsWJz"},"execution_count":null,"outputs":[]}]}