{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM9jIWHhXq1GIw4df5pbOnW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Cosine similarity (advanced)<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"P1e8oiHCs8Sf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUh283u8V78F"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","import torch\n","\n","# high res matplotlib\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["from transformers import GPT2Model,GPT2Tokenizer\n","\n","# import GPT-2 model, extract tokenizer and embeddings\n","gpt2 = GPT2Model.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","embedding = gpt2.wte.weight.detach().numpy()"],"metadata":{"id":"Iq4WDlRLWHlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"03EJE6bIML4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Manual and Pytorch cosine similarity"],"metadata":{"id":"GBp5xHDIWHgD"}},{"cell_type":"code","source":["# pick two random tokens\n","tokenpair = np.random.choice(np.arange(3000,6001),2)\n","\n","# get their embedding vectors\n","v1 = embedding[tokenpair[0]]\n","v2 = embedding[tokenpair[1]]\n","\n","print(f'Token pair: \"{tokenizer.decode(tokenpair[0])}\" and \"{tokenizer.decode(tokenpair[1])}\"')\n","print(f'Embedding shape: {v1.shape}')"],"metadata":{"id":"BVWx6WdRWnoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate cosine similarity manually\n","num = np.sum(v1*v2)\n","norm_v1 = np.sum(v1**2)\n","norm_v2 = np.sum(v2**2)\n","den = np.sqrt( norm_v1*norm_v2 )\n","\n","print(f'Shape of vectors: {v1.shape}')\n","manual_cs = num/den"],"metadata":{"id":"f9bH1H16daK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and now in torch\n","v1_torch = torch.tensor(v1).unsqueeze(dim=0)\n","v2_torch = torch.tensor(v2).view(1,-1) # both view() and unsqueeze() work in this case\n","\n","print(f'Shape of torch vectors: {v1_torch.shape}')\n","torch_cs = torch.cosine_similarity(v1_torch,v2_torch)"],"metadata":{"id":"jASTkdEveVF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the results\n","print(f'Manual cosine similarity:  {manual_cs:.5f}')\n","print(f'Pytorch cosine similarity: {torch_cs.item():.5f}')"],"metadata":{"id":"WT_mXSuJWHdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0cOKWmGMlNMX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Matrix of pairwise similarities"],"metadata":{"id":"SXJsuGHKfP2m"}},{"cell_type":"code","source":["# using a for-loop\n","numtoks = 30\n","\n","E = embedding[:numtoks,:]\n","\n","# FYI, mean-centering would give correlation coefficient\n","# E -= np.mean(E,axis=1,keepdims=True)\n","\n","print(f'Embedding submatrix has size {E.shape}')\n","\n","cs_matrix = np.zeros((numtoks,numtoks))\n","\n","for i in range(numtoks):\n","  for j in range(numtoks):\n","\n","    # cosine similarity\n","    num = np.sum(E[i,:]*E[j,:])\n","    den = np.sqrt( np.sum(E[i,:]**2)*np.sum(E[j,:]**2) )\n","\n","    # slot into the matrix\n","    cs_matrix[i,j] = num/den\n","\n","\n","# and show the matrix\n","plt.figure(figsize=(6,5))\n","\n","plt.imshow(cs_matrix,vmin=.2,vmax=.8)\n","ticklabels = [tokenizer.decode(t) for t in np.arange(numtoks)]\n","plt.gca().set(xticks=range(numtoks),xticklabels=ticklabels,\n","              yticks=range(numtoks),yticklabels=ticklabels)\n","plt.xticks(rotation=45)\n","plt.colorbar(pad=.02)\n","\n","plt.show()"],"metadata":{"id":"rmjWStxcfPz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat using matrix multiplication in pytorch\n","Et = torch.tensor(E) # first convert to pytorch tensor\n","\n","# normalize (note the vector_norm not matrix_norm!)\n","Et_norm = Et / torch.linalg.vector_norm(Et,axis=1,keepdims=True)\n","\n","# cosine similarity matrix\n","cs_matrixT = Et_norm @ Et_norm.T\n","\n","# need to conver to numpy first :P\n","print(f'Mean absolute difference = {np.mean(abs(cs_matrixT.numpy()-cs_matrix)):.10f}')"],"metadata":{"id":"HA5rbc0JnT1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vJ7yNS6KfPrP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Softmaxify the cosine similarities matrix"],"metadata":{"id":"0F5uwZWZfPot"}},{"cell_type":"code","source":["numtoks = 6\n","\n","Et = torch.tensor( embedding[:numtoks,:] )\n","Et_norm = Et / torch.linalg.vector_norm(Et,axis=1,keepdims=True)\n","cs_matrixP = Et_norm @ Et_norm.T\n","cs_matrixP"],"metadata":{"id":"vbQsSB6L1M_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a mask of -infs\n","infmask = torch.tril(torch.full((numtoks,numtoks),-np.inf))\n","\n","# mask the cosine similarities\n","mat4softmax = cs_matrixP + infmask\n","\n","print(infmask), print('')\n","print(mat4softmax)"],"metadata":{"id":"MXioWWeAfPl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# softmax!\n","softmat = torch.exp(mat4softmax) / torch.sum(torch.exp(mat4softmax))\n","\n","# print the results\n","print(softmat)\n","print(f'\\nSum over all matrix elements = {torch.sum(softmat)}')"],"metadata":{"id":"Qwq4WmOfpL5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nan mask\n","nanmask = torch.tril(torch.full((numtoks,numtoks),torch.nan))\n","nanmask"],"metadata":{"id":"ycW1YUY6DKTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize the relationship between softmax(cs) and cs\n","C = cs_matrixP.clone()\n","S = softmat.clone()\n","\n","# replace lower-triangle with nan's\n","C += nanmask\n","S += nanmask\n","\n","print(S)\n","\n","# and plot\n","plt.plot(C.flatten(),S.flatten(),'ko')\n","plt.gca().set(xlabel='Cosine similarity',ylabel='Softmax-cosine')\n","plt.show()"],"metadata":{"id":"VQ7aT9EPpLzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4DMu3QbhfPjD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: One-to-all similarity"],"metadata":{"id":"qRJcKfz7fPft"}},{"cell_type":"code","source":["randtoken = np.random.randint(0,tokenizer.vocab_size,1)\n","\n","# vector,matrix\n","one2all_cs = torch.cosine_similarity(torch.tensor(embedding[randtoken,:]),torch.tensor(embedding))\n","print(f'Size of embedding matrix: {embedding.shape}')\n","print(f'Size of one2all_cs: {one2all_cs.shape}')"],"metadata":{"id":"JKhaIuWkpHns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup the figure\n","fig = plt.figure(figsize=(12,4))\n","gs = GridSpec(1,4)\n","ax0 = fig.add_subplot(gs[:-1])\n","ax1 = fig.add_subplot(gs[-1])\n","\n","# plot all similarities\n","ax0.plot(one2all_cs,'ko',markerfacecolor='gray',alpha=.5)\n","ax0.set(xlabel='Token index',ylabel='Cosine similarity',xlim=[-10,len(one2all_cs)+10],title=f'Cosine similarity of \"{tokenizer.decode(randtoken)}\" with all tokens')\n","\n","# their distribution\n","ax1.hist(one2all_cs,bins=40,color='k',alpha=.5)\n","ax1.set(xlabel='Cosine similarity',ylabel='Count',yticks=[],title='Distribution of cs')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"T4Q_tQwTpHqh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.topk to find the 10 closest embeddings\n","values,indices = torch.topk(one2all_cs,10)\n","\n","print(f'Top 10 similar tokens to \"{tokenizer.decode(randtoken)}\":')\n","for v,i in zip(values,indices):\n","  print(f'  Sc = {v:.3f} with token \"{tokenizer.decode(i)}\"')"],"metadata":{"id":"odzJFmRq2dTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xAgcNcs3pHyp"},"execution_count":null,"outputs":[]}]}