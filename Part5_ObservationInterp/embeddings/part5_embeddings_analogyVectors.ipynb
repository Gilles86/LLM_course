{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPKSmA+O/KvV1PPvynWfBbR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Embeddings arithmetic and analogies<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"sqj89yNuWgVo"}},{"cell_type":"code","source":["# !pip install gensim"],"metadata":{"id":"Wk7Jq15S06nO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lh-aPx04n2Y"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import gensim.downloader as api\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# import glove\n","glove = api.load('glove-wiki-gigaword-50')"],"metadata":{"id":"qos7Eh4J4rT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b39bHx-XfjSX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the analogy vectors"],"metadata":{"id":"EQgiZntofjP7"}},{"cell_type":"code","source":["# embeddings vectors for three words\n","v1 = glove['king']  # base word\n","v2 = glove['man']   # to subtract\n","v3 = glove['woman'] # to add\n","\n","# analogy vector\n","analogyVector = v1 - v2 + v3\n","\n","# plot the vectors\n","plt.figure(figsize=(10,4))\n","plt.plot(v1,label='king')\n","plt.plot(v2,label='man')\n","plt.plot(v3,label='woman')\n","plt.plot(analogyVector,'k',linewidth=2,label='analogy')\n","\n","plt.gca().set(xlim=[0,len(v1)],xlabel='Embedding dimension',ylabel='Value')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"vRCCMmzmBPpx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"W5fsiIl3fmJP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cosine similarity to all vectors"],"metadata":{"id":"W8iIEP3afmGs"}},{"cell_type":"code","source":["# cossim with all\n","cossim2all = glove.cosine_similarities(analogyVector,glove.vectors)\n","print(cossim2all.shape)\n","\n","# plot the cosine similarities (skip every 100)\n","_,axs = plt.subplots(1,2,figsize=(13,4))\n","axs[0].scatter(range(0,len(cossim2all),100),cossim2all[::100],c=cossim2all[::100],marker='o',alpha=.5,cmap='seismic')\n","axs[0].set(xticks=[],xlabel='Index (with skip)',ylabel='Cosine similarity',title='Similarity with analogy vector')\n","\n","axs[1].plot(np.sort(cossim2all)[-1000:],'ko',markerfacecolor='gray',alpha=.5)\n","axs[1].set(xticks=[],xlabel='Sorted index',ylabel='Cosine similarity',title='Top 1000 similarities')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"55OJXSdEUI6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nEt9zbmVfpo_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find the best matches"],"metadata":{"id":"JE-GYtxDfpjN"}},{"cell_type":"code","source":["# find top 10 highest scores\n","top10 = cossim2all.argsort()[-10:][::-1]\n","\n","# print them out\n","print('Top 10 closest words:')\n","for widx in top10:\n","  print(f'  Similarity of {cossim2all[widx]:.3f} with \"{glove.index_to_key[widx]}\"')"],"metadata":{"id":"yeTg7gi7VMRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2k0EkXKZVMIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Does it work with time?"],"metadata":{"id":"CrLzFmCXryuz"}},{"cell_type":"code","source":["# create a semantic axis vector\n","time_axis = glove['tomorrow'] - glove['yesterday']\n","\n","# similarity to all other vectors\n","cossims = glove.cosine_similarities(time_axis,glove.vectors)\n","\n","# print the results\n","print('Top 5 closest POSITIVE words:')\n","for widx in cossims.argsort()[-5:][::-1]:\n","  print(f'  Similarity of {cossims[widx]:.3f} with \"{glove.index_to_key[widx]}\"')\n","\n","print('\\nTop 5 closest NEGATIVE words:')\n","for widx in cossims.argsort()[:5]:\n","  print(f'  Similarity of {cossims[widx]:.3f} with \"{glove.index_to_key[widx]}\"')"],"metadata":{"id":"fDcPiZ8hWr6h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pbq10oJDWrxz"},"execution_count":null,"outputs":[]}]}