{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNNmRSC0Wr+bVkg+TPLgTD1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Word2vec vs. GPT2<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"H9PFUlbmtJ1t"}},{"cell_type":"code","source":["# !pip install gensim"],"metadata":{"id":"MbwCFir0iI3l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lh-aPx04n2Y"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import gensim.downloader as api\n","from transformers import GPT2Model,GPT2Tokenizer\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# import word2vec\n","w2v = api.load('word2vec-google-news-300')"],"metadata":{"id":"qos7Eh4J4rT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YrLzeYrjUNV9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pretrained GPT-2 model and tokenizer\n","gpt2 = GPT2Model.from_pretrained('gpt2')\n","gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# GPT embeddings matrix\n","gpt_embedding = gpt2.wte.weight.detach().numpy()"],"metadata":{"id":"Iq4WDlRLWHlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o2s2NHrJWHi8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Find 100 matching tokens"],"metadata":{"id":"3nz5jQ8h4rLj"}},{"cell_type":"code","source":["# get the word2vec vocab\n","w2v_tokens ="],"metadata":{"id":"3xHa5JHo8h35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# same for gpt2\n","gpt2_tokens ="],"metadata":{"id":"j7pvZNo5mxhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find 6-letter tokens in word2vec, and see if they match in gpt2.\n","# take the first 100 matches for RSA\n","tokens2compare = []\n","\n","# loop over word2vec words\n","for word in w2v_tokens:\n","\n","  # skip if word is not 6 characters long\n","  if\n","\n","  # check if it exists in gpt\n","  try:\n","\n","\n","  except: pass\n","\n","  # stopping criteria\n","  if len(tokens2compare\n","    break"],"metadata":{"id":"1ezwTwvv4rH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for word in tokens2compare:\n","  print(f'\"{word}\" is index {} in w2v and index {} in GPT2.')"],"metadata":{"id":"83NkIwKf4q6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZgALQZwwoRXt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Create embeddings matrices"],"metadata":{"id":"NhH0T7Zu9Win"}},{"cell_type":"code","source":["# embeddings matrix for these words\n","E_w2v = np.array([w2v[w] for w in\n","E_gpt ="],"metadata":{"id":"9FSjp1RS9WgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check matrices sizes\n","print(f'Size of w2v matrix:\n","print(f'Size of gpt matrix:\n","\n","# sanity-check that they're really different\n","plt.figure(figsize=(10,4))\n","plt.plot(range(E_gpt.shape[1]),E_gpt[0,:],'o-',label='GPT2')\n","\n","\n","plt.legend(fontsize=10)\n","plt.show()"],"metadata":{"id":"iCEvfLal9WdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z6ZsXYUIwkZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Cosine similarity matrices"],"metadata":{"id":"BkODSOsBwkW5"}},{"cell_type":"code","source":["# normalize each vector to its norm (unit length)\n","E_w2v_norm = E_w2v / np.linalg.norm\n","E_gpt_norm =\n","\n","# cosine similarity matrices\n","cs_matrix_w2v =\n","cs_matrix_gpt ="],"metadata":{"id":"wk0DAW1nq_hT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(12,5))\n","\n","skip = 5\n","\n","# word2vec\n","h = axs[0].imshow()\n","axs[0].set(xticks=range(0,len(tokens2compare),skip),xticklabels=tokens2compare[::skip],\n","           yticks=range(1,len(tokens2compare),skip),yticklabels=tokens2compare[1::skip],\n","           title='Cossim matrix for word2vec')\n","axs[0].tick_params(axis='x',labelrotation=90)\n","fig.colorbar(h,ax=axs[0],pad=.02)\n","\n","\n","# GPT2\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"8uxAeD13raj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LG_-kCkV9WR7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Quantitative comparison via RSA"],"metadata":{"id":"PEgTG_hhw-7q"}},{"cell_type":"code","source":["# extract the upper-triangular elements from the cosine similarity matrices\n","unique_w2v =\n","unique_gpt =\n","\n","# Pearson correlation\n","r = np.corrcoef(\n","\n","# cosine similarity\n","num = sum(\n","den = sum(\n","sc = num/\n","\n","# plot\n","plt.plot(unique_w2v,unique_gpt,'ks',markerfacecolor=[.7,.9,.7,.7])\n","\n","plt.show()"],"metadata":{"id":"8ZEITW0QxA7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"p8Dp-2KtsWJz"},"execution_count":null,"outputs":[]}]}