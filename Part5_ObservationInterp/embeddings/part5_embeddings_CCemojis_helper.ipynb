{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP3z0IUFjNLyM790dEe3xQU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Tokenize, embed, and cluster happy emojis\n","<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"2B1b0m8GpiSw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUh283u8V78F"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from sklearn.manifold import TSNE\n","from sklearn.cluster import DBSCAN\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["from transformers import GPT2Model,GPT2Tokenizer\n","\n","# pretrained GPT-2 model and tokenizer\n","gpt2 = GPT2Model.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","embeddings = gpt2.wte.weight.detach().numpy()"],"metadata":{"id":"Iq4WDlRLWHlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HltMKSxOjfYW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# A bit about unicode"],"metadata":{"id":"8idegQNgjfO3"}},{"cell_type":"code","source":["# start with a character\n","c = '😀'#'A'#\n","\n","# print its integer unicode encoding\n","print(ord(c))\n","\n","# convert to hex for unicode standard\n","print(f\"U+{ord(c):04X}\")"],"metadata":{"id":"rYIWGB29jj_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# but it crashes for some \"complex\" emojis\n","# ord('❤️')\n","len('❤️')"],"metadata":{"id":"XKU9bMM_kHQD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for c in '❤️':\n","  print(ord(c))"],"metadata":{"id":"1aO9840PkUOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o2s2NHrJWHi8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Tokenize emojis"],"metadata":{"id":"LQMTSAec3yXm"}},{"cell_type":"code","source":["emoji_list = ['💗', '❤️', '💓', '😍', '💖',\n","              '🔥', '😂', '😢', '💔', '😊']\n","\n","for emoji in emoji_list:\n","  print(f'{emoji} is {len(emoji)} \"characters\" and uses tokens: {}')"],"metadata":{"id":"JSoE1iK13yUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print unicode\n","for emoji in emoji_list:\n","  print(f'Unicode for {} is {}')"],"metadata":{"id":"o0oFDMfnaL35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GP5m1NYaGfV4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Inspect the emoji embeddings"],"metadata":{"id":"BQ8osuudKiEh"}},{"cell_type":"code","source":["# calculate and visualize intra-emoji embeddings\n","\n","_,axs = plt.subplots(2,2,figsize=(8,7))\n","\n","for i,ax in enumerate(axs.flatten()):\n","\n","  # get the tokens and plt the first by the last vectors\n","  toks =\n","  ax.plot(,,'k.')\n","\n","  # cosine similarity\n","  v1,v2 =\n","  cs =  /\n","\n","  ax.set(xticks=[],yticks=[],xlabel='First token embedding',ylabel='Final token embedding',\n","         title=f'{emoji_list[i]} embeddings (S$_c$ = {cs:.2f})')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"xrgRiaTQF8Va"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-61rd1e7F8Lg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Cosine similarity with all other embedding vectors"],"metadata":{"id":"xFQ6O5cuLIXV"}},{"cell_type":"code","source":["# just the first emoji, averaging the embeddings vectors\n","toks =\n","emoji_ave_emb =\n","\n","# cs to all other embeddings\n","one2all_cs =\n","\n","# and plot\n","plt.figure(figsize=(12,4))\n"],"metadata":{"id":"OvwShSh6LISI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# closest matches\n","values,indices = torch.topk\n","\n","print(f'Top 10 similar tokens to \"{emoji_list[0]}\":\\n')\n","for v,i in\n","  string =\n","  print(f'  cossim = {v:.3f} for \"{string}\"')"],"metadata":{"id":"P6h-5Y8ELINO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"I5YOece0LIHy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: tSNE"],"metadata":{"id":"S83uf8q7aTE0"}},{"cell_type":"code","source":["emojis = ['😀', '😃', '😄', '😁', '😆', '😅', '😂', '🤣',\n","          '❤️', '💛', '💚', '💙', '💜', '🖤', '💖', '💗',\n","          '🐶', '🐱', '🐭', '🐰', '🦊', '🐻', '🐼', '🦁',\n","          '☀️', '🌤️', '🌧️', '⛈️', '❄️', '🌈', '🌪️', '🌊' ]"],"metadata":{"id":"0MnH4SMuLIB0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a matrix of average embeddings vectors\n","\n","subEmbed = np.zeros\n","\n","for i,emoji in enumerate(emojis):\n","\n","\n","subEmbed.shape"],"metadata":{"id":"HqJtPbx9M_3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reduce dimensions to 2D with t-SNE\n","tsne = TSNE()\n","tsne_result = tsne.\n","\n","# plot results\n","fig,axs = plt.subplots(1,2,figsize=(13,5))\n","\n","\n","\n","# show the tSNE results\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"7Q7W0Q3e3yPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iLqZBUYoToFX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: DBscan"],"metadata":{"id":"588PmWidaWeT"}},{"cell_type":"code","source":["# zscore the tsne_result to use a normalized eps param\n","tsne_result =\n","\n","## dbscan\n","clustmodel = DBSCAN(\n","groupidx = clustmodel.labels_\n","\n","# number of clusters\n","nclust =\n","\n","# compute cluster centers\n","cents = np.zeros((nclust,2))\n","for ci in range(nclust):\n","  cents[ci,0] = np.mean\n","  cents[ci,1] =\n","\n","# draw lines from each data point to the centroids of each cluster\n","plt.figure(figsize=(8,6))\n","\n","\n","# now draw the raw data in different colors\n","\n","# and now plot the centroid locations\n","plt.gca().set(xlabel='tSNE axis 1',ylabel='tSNE axis 2',title=f'Result of dbscan clustering (k={nclust})')\n","\n","plt.show()"],"metadata":{"id":"GwwmogkK3yL0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cidx in range(-1,nclust):\n","\n","  # find all the tokens in this group\n","  tokensInGroup =\n","\n","  # print them out\n","  if cidx==-1:\n","    print(f'\\nUngrouped tokens:')\n","  else:\n","    print(f'\\nTokens in group {cidx}:')\n","  print(["],"metadata":{"id":"arQoLA1NNqjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XdItNOlQNqWl"},"execution_count":null,"outputs":[]}]}