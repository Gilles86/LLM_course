{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOOwhdfU31xzqgvWEPPT6bX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>kNN for synonym-searching in BERT<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"iTyOaD0stKab"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTp8j3TJAqvB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"s75lnzQymNKZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo of kNN for classification"],"metadata":{"id":"5tN-vdsOJAMb"}},{"cell_type":"code","source":["# data labels and categories (\"X\" is the unlabeled data value)\n","dataLabels = 'ABCDEFGHIJLKMNOPX'\n","categories = ( np.linspace(0,1,len(dataLabels)-1)>.5 ).astype(int)\n","unlabeled  = len(dataLabels)-1 # final value\n","\n","# generate some random data\n","# data = np.random.randn(len(dataLabels),2)\n","\n","\n","# Euclidean distance from unlabeled data value to all others\n","eucldist = np.sqrt(np.sum( (data-data[unlabeled,:])**2 ,axis=1))\n","\n","# plot all letters\n","for i in range(len(data)-1):\n","  c = 'br'[categories[i]]\n","  plt.plot(data[i,0],data[i,1],marker=f'${dataLabels[i]}$',color=c,markersize=10)\n","\n","# plot the seed\n","plt.plot(data[unlabeled,0],data[unlabeled,1],marker=f'${dataLabels[unlabeled]}$',color='k',markersize=12)\n","\n","plt.gca().set(xlim=[-3,3],ylim=[-3,3],xlabel='Embedding dimension \"1\"',ylabel='Embedding dimension \"2\"')\n","plt.show()"],"metadata":{"id":"nwWVmFnPNZYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# distance sorting indices (excluding self-distance)\n","distidx = np.argsort(eucldist)[1:]\n","\n","# print by sorted distance\n","for i in distidx:\n","  print(f'\"{dataLabels[i]}\" is {eucldist[i]:>5.2f} units from \"{dataLabels[unlabeled]}\"')"],"metadata":{"id":"4OgBbt7eNZVj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the categories of the k nearest neighbors\n","k = 3\n","\n","# label the unlabeled\n","targCat = np.median(categories[distidx[:k]]).astype(int)\n","\n","# print the result\n","print(f'The categories of the {k} nearest neighors are {categories[distidx[:k]]}\\n')\n","print(f'The unlabeled data value is in category \"{targCat}\" ({\"br\"[targCat]} in the plot)')"],"metadata":{"id":"aaeFBnhumO_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QwFrFNH3mO78"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now for the synonym-searching"],"metadata":{"id":"3Gq9nB-JmO4y"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","\n","# load BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertModel.from_pretrained(\"bert-base-uncased\")"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the embeddings matrix\n","embeddings = model.embeddings.word_embeddings.weight.detach().numpy()\n","print(f'Embeddings matrix shape: {embeddings.shape}')"],"metadata":{"id":"Upgfe2mABWNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wH242rnshXM5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# kNN on BERT"],"metadata":{"id":"IYa6IRn8NZQK"}},{"cell_type":"code","source":["# pick a \"seed\" vector\n","seedword = 'beauty'\n","\n","seedvect = embeddings[tokenizer.encode(seedword,add_special_tokens=False),:]\n","\n","# Euclidean distance to all other vectors\n","eucDist = np.sqrt( np.sum( (embeddings-seedvect)**2 ,axis=1) )\n","\n","# cosine similarity for comparison\n","E = embeddings / np.linalg.norm(embeddings,axis=1,keepdims=True)\n","cs = (seedvect/np.linalg.norm(seedvect)) @ E.T\n","cs = np.squeeze(cs) # remove singleton dimension\n","\n","# for visualization, replace 0 with non\n","eucDist_nan = eucDist+0\n","eucDist_nan[eucDist==0] = np.nan\n","\n","\n","\n","# visualizations\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","axs[0].scatter(range(len(eucDist)),eucDist_nan,s=50,c=cs,alpha=.4)\n","axs[0].set(xlim=[-20,len(eucDist)+20],xlabel='Token index',ylabel='Euclidean distance',\n","           title=f'Distance to \"{seedword}\", colored by cosine sim.')\n","\n","axs[1].plot(eucDist_nan,cs,'ko',markerfacecolor=[.7,.7,.9,.6])\n","axs[1].set(xlabel='Euclidean distance',ylabel='Cosine similarity',\n","           title='Relation between $S_c$ and Euclidean distance')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"38dhFl7YNc8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now for the top-k closest tokens\n","k = 15\n","topKidx = np.argsort(eucDist)[:k]\n","\n","print(f'Nearest {k} words to \"{seedword}\":')\n","for i in topKidx:\n","  print(f'  Distance of {eucDist[i]:.3f} to \"{tokenizer.decode(i)}\"')"],"metadata":{"id":"f7YhIKAmNZNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"31LgVYBy6fBx"},"execution_count":null,"outputs":[]}]}