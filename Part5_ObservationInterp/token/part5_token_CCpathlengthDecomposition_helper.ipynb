{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOJmIadR8K7BnkzteWP9Jat"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Residual stream decomposition of path lengths <b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"SLcN-NOPU1Zo"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from scipy.stats import pearsonr\n","from statsmodels.stats.multitest import fdrcorrection\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"Kv8VIScVU5g3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YX4hFYfHEaHF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Model, hooks, tokens, activations"],"metadata":{"id":"IacztNnmEaD7"}},{"cell_type":"code","source":["# GPT2-large model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n","model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","model.eval()"],"metadata":{"id":"D_WzgXfsEZO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lj9THg4fZY0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook the attention and mlp adjustments (projection vectors)\n","activations = {}\n","\n","def implant_hook_attn(layer_number):\n","  def hook(module, input, output):\n","    activations[f'attn_proj_{layer_number}'] =\n","  return hook\n","\n","def implant_hook_mlp\n","\n","\n","# reminder: \"proj\" are the adjustments that get added to the embeddings vectors\n","for layeri in range(model.config.n_layer):\n","  model.transformer.h[layeri].\n","  model.transformer.h[layeri]."],"metadata":{"id":"Q3_iseD-1aG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DA3PhziYU5dg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Push text through the model and get hidden_states"],"metadata":{"id":"N9o5OGkXVMOz"}},{"cell_type":"code","source":["# https://en.wikipedia.org/wiki/Friedrich_Nietzsche\n","tokz = tokenizer.encode(\"Nietzsche was a widely influential German philosopher. He began his career as a classical philologist, turning to philosophy early in his academic career. In 1869, aged 24, Nietzsche became the youngest professor to hold the Chair of Classical Philology at the University of Basel. Plagued by health problems for most of his life, he resigned from the university in 1879, after which he completed much of his core writing in the following decade. Nietzsche's work spans philosophical polemics, poetry, cultural criticism, and fiction while displaying a fondness for aphorism and irony. Prominent elements of his philosophy include his radical critique of truth in favour of perspectivism; a genealogical critique of religion and Christian morality and a related theory of master–slave morality; the aesthetic affirmation of life in response to both the 'death of God' and the profound crisis of nihilism; the notion of Apollonian and Dionysian forces; and a characterisation of the human subject as the expression of competing wills, collectively understood as the will to power. He also developed influential concepts such as the Übermensch and his doctrine of eternal return. In his later work, he became increasingly preoccupied with the creative powers of the individual to overcome cultural and moral mores in pursuit of new values and aesthetic health. His body of work touched a wide range of topics, including art, philology, history, music, religion, tragedy, culture, and science, and drew inspiration from Greek tragedy as well as figures such as Zoroaster, Arthur Schopenhauer, Ralph Waldo Emerson, Richard Wagner, Fyodor Dostoevsky, and Johann Wolfgang von Goethe.\",return_tensors='pt')\n","\n"],"metadata":{"id":"hfaEG1i27Kb9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations.keys(), activations['mlp_proj_0'].shape"],"metadata":{"id":"0aiE5UybVY9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(outputs.hidden_states), outputs.hidden_states[33].shape"],"metadata":{"id":"oxIvTEMA-v1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ABc81HxrOVCj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Cosine similarities between attention and MLP"],"metadata":{"id":"xPI6KfMQOU_n"}},{"cell_type":"code","source":["# initialize\n","cossims = np.zeros\n","\n","# loop over layers\n","for layeri in\n","\n","  # cosine similarity between attn and mlp projections within layer\n","  for toki in\n","\n","    # extract the two vectors for this tooken\n","    attn = activations[f'attn_proj_{layeri}']\n","    mlp  = activations\n","\n","    # cosine similarity between them\n","    cossims[layeri,toki] =\n","\n","\n","# plotting! (my fav part :D )\n","_,axs = plt.subplots(1,2,figsize=(11,4))\n","axs[0].errorbar( ,marker='s',color='k',linestyle='none',markerfacecolor=[.7,.7,.7])\n","axs[0].axhline\n","axs[0].set(xlabel='Transformer layer',ylabel='Cosine similarity',title='Average (+std) across all tokens')\n","\n","axs[1].hist( ,bins=80,color=[.7,.7,.7],edgecolor='k',linewidth=.5)\n","axs[1].set(xlabel='Cosine similarity',ylabel='Count',title='Distribution of all tokens and layers')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"HANyYAMaLlWE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Z6AAC7x5LlKi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Path lengths of adjustments and hidden states"],"metadata":{"id":"4wAOA2O4VYsd"}},{"cell_type":"code","source":["# initialize\n","pathlen = np.zeros((model.config.n_layer,len(tokz[0]),3))\n","nextTokenLogits = np.zeros(len(tokz[0]))\n","\n","# loop over all tokens in the text\n","for toki in range(len(tokz[0])):\n","\n","  # path length from previous\n","  for layeri in range(1,model.config.n_layer):\n","\n","    # extract the vector pairs\n","    currAttn = activations[f'attn_proj_ # from this layer\n","    prevAttn = activations[f'attn_proj_ # from the previous layer\n","\n","    currMlp  = # repeat the above for MLP\n","    prevMlp  =\n","\n","    currHs   = # and for the hidden layers\n","    prevHs   =\n","\n","\n","    # norm the difference vectors\n","    pathlen[layeri,toki,0] = torch.norm(  ) # attention\n","    pathlen[layeri,toki,1] = # MLP\n","    pathlen[layeri,toki,2] = # hidden-states\n","\n","pathlen.shape"],"metadata":{"id":"QvAf0LULTSyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(14,3))\n","\n","\n","titles = [ 'Attention','MLP','Hidden state' ]\n","\n","for i in range(3):\n","  h = axs[i].imshow( ,aspect='auto',origin='lower',vmin=0,vmax=50)\n","\n","  fig.colorbar(h,ax=axs[i],pad=.02)\n","\n","plt.show()"],"metadata":{"id":"kn4LPGqVJZOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ld3jb4Q4mDtq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Correlate adjustments and hidden states"],"metadata":{"id":"p4d5OFR8mDrV"}},{"cell_type":"code","source":["Rs = np.zeros((model.config.n_layer,2))\n","Ps = np.zeros((model.config.n_layer,2))\n","\n","for layeri in range(model.config.n_layer):\n","\n","  # correlation coefficient between attn and hs\n","  r =\n","  Rs[layeri,0] = # r value\n","  Ps[layeri,0] = # p-value\n","\n","\n","  # repeat for mlp and hs\n","  r =\n","  Rs[layeri,1] =\n","  Ps[layeri,1] =\n","\n","\n","# p-value threshold based on FDR\n","sigPsA = fdrcorrection(Ps[:,0])\n","sigPsM ="],"metadata":{"id":"-see_IiRbgqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","plt.plot(Rs[:,0],color=[.9,.7,.7])\n","plt.plot(,,'ko',markerfacecolor=[.9,.7,.7],markersize=10,label='Att - HS')\n","plt.plot(,,'ro',markersize=6)\n","\n","\n","plt.plot(,color=[.7,.7,.9])\n","plt.plot(,'ks',markerfacecolor=[.7,.7,.9],markersize=10,label='MLP - HS')\n","plt.plot(,'bs',markersize=6)\n","\n","\n","plt.axhline(0,linestyle='--',color='gray',linewidth=.8)\n","\n","plt.gca().set(xlabel='Transformer layer',ylabel='Correlation',xlim=[0,model.config.n_layer],\n","              title='Correlations between subblock and HS path lengths')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"DMnnbdPwh6ce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(3,4,figsize=(13,8))\n","\n","for i,ax in enumerate(axs.flatten()):\n","\n","  # layer number\n","  lay = i*3 + 1\n","\n","  # scatter plots\n","  ax.plot(,'ko',markerfacecolor=[.9,.7,.7,.3],label='Att')\n","  ax.plot(,label='MLP')\n","\n","  # axis adjustments\n","  ax.set(xticks=[],xlabel='Att or MLP path length',yticks=[],ylabel='HS path length')\n","  ax.set_title(f'Layer {lay}',fontweight='bold')\n","  ax.legend()\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"YimGmJqwJZD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GeXE5z9dpPRi"},"execution_count":null,"outputs":[]}]}