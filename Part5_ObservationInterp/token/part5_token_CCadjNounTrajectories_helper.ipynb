{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOTHqBJie9MHqK5V8Wo21pU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Do nouns or adjectives have longer trajectories?<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"xYyqZBvDoBDp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.decomposition import PCA\n","\n","import requests\n","import spacy\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","# vector matplotlib\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"ghGC7BDmuzL7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Text and token batches"],"metadata":{"id":"4JUrRHVeuzJG"}},{"cell_type":"code","source":["# import tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# import Frankenstein text\n","text = requests.get('https://www.gutenberg.org/cache/epub/84/pg84.txt').text\n","tokens =\n","\n","print(f'There are {} GPT tokens, {} of which are unique.')"],"metadata":{"id":"oqdAP54II-ht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nlp module from spaCy\n","nlp = spacy.load('en_core_web_sm')\n","\n","# tokenize and report count\n","doc = nlp(text)\n","print(f'There are {len(doc):,} spaCy tokens.')"],"metadata":{"id":"UQ3zcnlfPU_8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_o1DvGdHPU3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# batch parameters\n","batchsize   =\n","context_pre =\n","context_pst ="],"metadata":{"id":"PYKV3v4MkemI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize batches\n","batch_noun = torch.zeros\n","batch_adje = torch.zeros\n","\n","# initialize counters\n","seqiN = 0\n","seqiA = 0\n","\n","\n","# loop over the tokens\n","for idx in range(\n","\n","  # get the text for this token\n","  txt =\n","\n","  ### filter\n","  # skip short tokens\n","  if : continue\n","\n","  # skip subwords (this and the next token must start with space)\n","  if  | : continue\n","\n","\n","  ### get part of speech and populate vector for nouns or adjectives\n","  pos = nlp( )[0].pos_\n","\n","  # if it's a noun and we don't have enough\n","  if pos=='NOUN':\n","    if seqiN\n","      batch_noun[seqiN,:] = tokens[0,]\n","      seqiN += 1\n","\n","  # if it's an adjective and we don't have enough\n","\n","\n","  # quit early if there's enough data\n","  if\n","    break"],"metadata":{"id":"fYPkVigeOv47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show some examples\n","print('Some nouns:')\n","for b in batch_noun[:15,context_pre]:\n","  print\n","\n","print('\\nSome adjectives:')\n","for b in batch_adje[:15,context_pre]:\n","  print("],"metadata":{"id":"MylwKVDZN-dL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"R4qoz2Lkkbjm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Import model and get hidden state activations"],"metadata":{"id":"6SkFDdcDSyB1"}},{"cell_type":"code","source":["# load GPT2 model\n","model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the activations and hidden states\n","with torch.no_grad():\n","  out_noun = model(batch_noun\n","  out_adje = model("],"metadata":{"id":"YAMzPriHL7SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nLayers = len(out_noun.hidden_states)"],"metadata":{"id":"8EWfVGaOoSKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out_noun.hidden_states[3].shape"],"metadata":{"id":"FCe1r-uvoSES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zOOrZGXKAbE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: PCA on the activation vectors"],"metadata":{"id":"AqIWNzD9Abaa"}},{"cell_type":"code","source":["# number of rows is layers X batchsize X 2\n","numRows =\n","\n","# initializations\n","all_acts = np.zeros((numRows,model.config.n_embd))\n","lookuptable = np.zeros((numRows,2),dtype\n","rowi = 0\n","\n","\n","# loop over sequences\n","for seqi in range(batchsize):\n","  for layeri in range(nLayers):\n","\n","    ### NOUNS\n","    # get the activation vector for target token in this layer in this sequence\n","    all_acts[rowi,:] = out_noun.\n","\n","    # identifiers\n","    lookuptable[rowi,0] = layeri # which layer\n","    lookuptable[rowi,1] =        # which target\n","    rowi += 1\n","\n","\n","    ### ADJECTIVES\n","    all_acts[rowi,:] = out_adje.hidden_states[layeri][seqi,context_pre,:].cpu().numpy().squeeze()\n","    lookuptable[rowi,0] =  # which layer\n","    lookuptable[rowi,1] =  # which target\n","    rowi += 1\n"],"metadata":{"id":"6o1BruS_lsM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lookuptable.shape, all_acts.shape"],"metadata":{"id":"aXkFb9GAMC7l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CHsvWwqsAjcb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PCA with 20 components just to show variance explained\n","pca = PCA().fit(all_acts)\n","scree = pca.explained_variance_ratio_ # convert to percent\n","\n","# and plot\n","plt.figure(figsize=(8,3))\n","plt.plot(,'ks',markerfacecolor=[.9,.7,.7],markersize=10)\n","\n","plt.gca().set(xlabel='Component number',xticks=range(0,21,2),\n","              ylabel='Percent variance explained',title='Scree plot')\n","\n","plt.show()"],"metadata":{"id":"DHrb1RzgV-NZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# project to 2D\n","proj2d = pca.transform(all_acts) # first two columns or first two rows??\n","\n","print(all_acts.shape,proj2d.shape)"],"metadata":{"id":"BDm0pvajAi-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xA8nIIpEAhf3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Visualize the average trajectories"],"metadata":{"id":"qGQOxUhdAhdU"}},{"cell_type":"code","source":["aveTrajectories = np.zeros((nLayers,2,2))\n","\n","for i in range(nLayers):\n","  aveTrajectories[i,:,0] = proj2d[ THISLAYER & ISNOUN ,:].mean(axis=0) # nouns\n","  aveTrajectories[i,:,1] = # same for adjectives\n","\n","# calculate distances between noun and adjective tokens\n","noun_pts = aveTrajectories[:,:,0]\n","adj_pts  = aveTrajectories[:,:,1]\n","pos_distances ="],"metadata":{"id":"dyYCxqX1oSHG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(14,4))\n","\n","### plot the trajectories in \"layer space\"\n","axs[0].plot(aveTrajectories[:,0,0],label='Nouns PC1')\n","axs[0].plot(aveTrajectories,label='Nouns PC2')\n","axs[0].plot(,label='Adjectives PC1')\n","axs[0].plot(,label='Adjectives PC2')\n","axs[0].set(xlim=[0,nLayers-1],xlabel='Layer',ylabel='Projection (a.u.)',\n","           title='PC projections in \"layer space\"')\n","axs[0].legend()\n","\n","\n","### trajectories in PC state-space\n","# plot the trajectory for nouns\n","axs[1].plot(,'r',zorder=-3,label='Nouns')\n","h = axs[1].scatter(aveTrajectories[:,0,0],aveTrajectories[:,1,0],marker='s',\n","               s=np.linspace(20,120,nLayers),c=np.arange(nLayers),cmap='Reds')\n","\n","# repeat for adjectives\n","axs[1].plot(aveTrajectories[:,0,1],aveTrajectories[:,1,1],'b',zorder=-3,label='Adjectives')\n","axs[1].scatter(,cmap='Blues')\n","\n","axs[1].set(xlabel='PC1',ylabel='PC2',title='Trajectories in \"PC space\"')\n","axs[1].legend()\n","fig.colorbar(h,ax=axs[1],label='Layer',pad=.01)\n","\n","\n","\n","### show distances\n","axs[2].plot(,'g',linewidth=2,zorder=-6)\n","axs[2].scatter(,marker='s',s=100,c=np.arange(nLayers),cmap='Greens')\n","axs[2].set(xlabel='Layer',ylabel='Euclidean distance',title='Nouns vs. adjectives distances',ylim=[-1,None])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"B-WZaL9J4uLh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K3GGj6t8xSPl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Calculate trajectory distances"],"metadata":{"id":"sHedimdBxSNQ"}},{"cell_type":"code","source":["# initialize\n","distances = np.zeros((nLayers-1,2))\n","\n","# using for-loops for clarity\n","for i in range(1,nLayers):\n","  for j in range(2):\n","\n","    # x points\n","    x1 = aveTrajectories[i-1,0,j]\n","    x2 = aveTrajectories[i,0,j]\n","\n","    # y points\n","    y1 =\n","    y2 =\n","\n","    # euclidean distances\n","    distances[i-1,j] = np.sqrt( ()**2 + ()**2 )\n","\n","\n","# plotting\n","_,axs = plt.subplots(1,2,figsize=(10,4))\n","axs[0].plot(,'ks-',markerfacecolor=[.9,.7,.7],linewidth=.2,label='Nouns')\n","axs[0].plot(,'ko-',markerfacecolor=[.7,.9,.7],linewidth=.2,label='Adjectives')\n","axs[0].legend()\n","axs[0].set(xlabel='Layer',ylabel='Log distance',title='Trajectory distances')\n","\n","axs[1].plot(,'k^-',linewidth=.2,markersize=9,markerfacecolor=[.7,.7,.9])\n","axs[1].set(xlabel='Layer',ylabel='Log-distance difference',title='Noun - adj differences')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"xPBM3tVAxTtp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WQ-TVaPW1g6l"},"execution_count":null,"outputs":[]}]}