{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPN3ucFLpgltO/Wt2L6PcNB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Path length and logit token prediction<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"SLcN-NOPU1Zo"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"Kv8VIScVU5g3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary of modelname:identifier\n","model_ids = {\n","    'small':  'gpt2',        # 124M\n","    'medium': 'gpt2-medium', # 355M\n","    'large':  'gpt2-large',  # 774M\n","    'xl':     'gpt2-xl'      # 1.6B\n","}\n","\n","# load all models into a dictionary\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","models = {}\n","for name,id in model_ids.items():\n","  models[name] = AutoModelForCausalLM.from_pretrained(id).to(device)\n","  models[name].eval();"],"metadata":{"id":"DyIMHjXtXzUZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DA3PhziYU5dg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Push text through the SMALL model and get hidden_states"],"metadata":{"id":"N9o5OGkXVMOz"}},{"cell_type":"code","source":["# https://en.wikipedia.org/wiki/Friedrich_Nietzsche\n","tokz = tokenizer.encode(\"Nietzsche was a widely influential German philosopher. He began his career as a classical philologist, turning to philosophy early in his academic career. In 1869, aged 24, Nietzsche became the youngest professor to hold the Chair of Classical Philology at the University of Basel. Plagued by health problems for most of his life, he resigned from the university in 1879, after which he completed much of his core writing in the following decade. Nietzsche's work spans philosophical polemics, poetry, cultural criticism, and fiction while displaying a fondness for aphorism and irony. Prominent elements of his philosophy include his radical critique of truth in favour of perspectivism; a genealogical critique of religion and Christian morality and a related theory of master–slave morality; the aesthetic affirmation of life in response to both the 'death of God' and the profound crisis of nihilism; the notion of Apollonian and Dionysian forces; and a characterisation of the human subject as the expression of competing wills, collectively understood as the will to power. He also developed influential concepts such as the Übermensch and his doctrine of eternal return. In his later work, he became increasingly preoccupied with the creative powers of the individual to overcome cultural and moral mores in pursuit of new values and aesthetic health. His body of work touched a wide range of topics, including art, philology, history, music, religion, tragedy, culture, and science, and drew inspiration from Greek tragedy as well as figures such as Zoroaster, Arthur Schopenhauer, Ralph Waldo Emerson, Richard Wagner, Fyodor Dostoevsky, and Johann Wolfgang von Goethe.\",return_tensors='pt')\n","tokz = tokz.to(device)\n","\n","with torch.no_grad():\n","  outputs = models['small'](tokz, output_hidden_states=True, labels=tokz)"],"metadata":{"id":"hfaEG1i27Kb9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0aiE5UybVY9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Calculate path lengths and correlate with next-token logits"],"metadata":{"id":"4wAOA2O4VYsd"}},{"cell_type":"code","source":["# initialize\n","pathlen = np.zeros((models['small'].config.n_layer+1,len(tokz[0])))\n","nextTokenLogits = np.zeros(len(tokz[0]))\n","\n","\n","for toki in range(len(tokz[0])):\n","\n","  # path length from previous\n","  for layeri in range(1,models['small'].config.n_layer+1): # +1 b/c hs[0] is embeddings\n","\n","    # extract hidden-state vectors, and norm the difference vector\n","    currAct = outputs.hidden_states[layeri  ][0,toki,:]\n","    prevAct = outputs.hidden_states[layeri-1][0,toki,:]\n","    pathlen[layeri,toki] = torch.norm( currAct-prevAct ).cpu()\n","\n","\n","  # logit for correct next-token prediction\n","  if toki<len(tokz[0])-1:\n","    logits = outputs.logits[0,toki,:].detach().cpu()\n","    nextTokenLogits[toki] = logits[tokz[0,toki+1]]\n","\n","\n","# cumulative pathlengths\n","cumpathlen = np.cumsum(pathlen,axis=0)\n","cumpathlen[0,:] = np.nan\n","r_pathLogits = np.corrcoef(nextTokenLogits[1:],pathlen[-1,1:])[0,1]\n","\n","\n","# show the results!\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","# path length progressions for each token\n","for i in range(1,cumpathlen.shape[1]):\n","  axs[0].plot(range(0,models['small'].config.n_layer+1),cumpathlen[:,i],'s-',alpha=.2,linewidth=.5,markersize=7,\n","              color=mpl.cm.plasma(i/cumpathlen.shape[1]))\n","\n","axs[0].set(title='Cumulative path length (each line is a token)',xlabel='Transformer layer',ylabel='Cumulative path length')\n","\n","axs[1].plot(nextTokenLogits[1:],pathlen[-1,1:],'ko',markersize=10,markerfacecolor=[.9,.7,.7],alpha=.6)\n","axs[1].set(title=f'r = {r_pathLogits:.2}',xlabel='Next-token logits',ylabel='Path length at final layer')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"QvAf0LULTSyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y33XJw8PU5T5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Repeat for all models"],"metadata":{"id":"d8f8l1fsYPIF"}},{"cell_type":"code","source":["_,axs = plt.subplots(2,2,figsize=(11,7))\n","axs = axs.flatten()\n","axi = 0\n","\n","linecolors = [ [.9,.7,.7],[.7,.9,.7],[.7,.7,.9],[.7,.9,.9] ]\n","\n","\n","for name, model in models.items():\n","\n","  ### push data through the model\n","  with torch.no_grad():\n","    outputs = model(tokz, output_hidden_states=True, labels=tokz)\n","\n","\n","  # initialize\n","  pathlen = np.zeros((model.config.n_layer+1,len(tokz[0])))\n","  nextTokenLogits = np.zeros(len(tokz[0]))\n","\n","\n","  for toki in range(len(tokz[0])):\n","\n","    # path length from previous\n","    for layeri in range(1,model.config.n_layer+1): # +1 b/c hs[0] is embeddings\n","      pathlen[layeri,toki] = torch.norm( outputs.hidden_states[layeri][0,toki,:] - outputs.hidden_states[layeri-1][0,toki,:] )\n","\n","    # error for correct next-token prediction\n","    if toki<len(tokz[0])-1:\n","      logits = outputs.logits[0,toki,:].detach()\n","      nextTokenLogits[toki] = logits[tokz[0,toki+1]]\n","\n","\n","  # cumulative pathlengths\n","  cumpathlen = np.cumsum(pathlen,axis=0)\n","  cumpathlen[0,:] = np.nan\n","  r_pathLogits = np.corrcoef(nextTokenLogits[1:],pathlen[-1,1:])[0,1]\n","\n","\n","  print(f'{name:>8} has pathlength-logit correlation of {r_pathLogits:.3f}')\n","\n","  axs[axi].plot(nextTokenLogits[1:],pathlen[-1,1:],'ko',markersize=10,\n","                markerfacecolor=linecolors[axi],alpha=.6)\n","  axs[axi].set(title=f'{name} (r = {r_pathLogits:.2})',xlabel='Next-token logits',ylabel='Cumulative path length')\n","  axi += 1\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"dd_puEomYQs9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mhdG-jHPYOxc"},"execution_count":null,"outputs":[]}]}