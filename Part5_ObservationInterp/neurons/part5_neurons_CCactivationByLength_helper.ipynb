{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyP/WWkpkZS1yor7YVgEvbdo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Activation histograms by token length<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"EBT02JwWkMri"}},{"cell_type":"code","source":["# run first to install and then restart\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"p3a7ZCgN_YeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","from datasets import load_dataset\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"CDYgJm9whU-z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import the model and implant hooks"],"metadata":{"id":"cj6kQvI7hU8A"}},{"cell_type":"code","source":["# for exercises 1-6\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# for exercise 7\n","# tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')\n","# model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B')"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU and switch to eval\n"],"metadata":{"id":"pkljw5bmOyWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook function\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # store in the dictionary\n","\n","  return hook\n","\n","\n","# put hooks in all layers\n"],"metadata":{"id":"-5ZWugpPnt85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# number of MLP expansion neurons\n","nneurons ="],"metadata":{"id":"DMvzP8wMhrd-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iiFw-dHeN0Rn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Import and tokenize fineweb"],"metadata":{"id":"uu2yxPwQ0M-g"}},{"cell_type":"code","source":["fineweb = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)\n","fw_iterator = iter(fineweb)  # create iterator\n","\n","# get multiple examples:\n","for _ in range(5):\n","  example = next(fw_iterator)\n","  print"],"metadata":{"id":"O6PU8Wp5LiSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how many tokens in total\n","desiredTokenCount =\n","\n","# initialize empty tensor (must be ints!)\n","allTokens =\n","allTokenLengths =\n","\n","# reinitialize iterator\n","fw_iterator = iter(fineweb)\n","\n","\n","# keep importing data until we have enough\n","while\n","\n","  # import the text\n","  text = next(fw_iterator)['text']\n","\n","  # tokenize\n","  tokens =\n","\n","  # get token lengths\n","  tokenLengths =\n","\n","  # stack the tokens and the lengths\n","  allTokens =\n","  allTokenLengths =\n","\n","\n","# trim the vectors\n","\n","\n","print(allTokens.shape)\n","print(allTokenLengths.shape)"],"metadata":{"id":"fZe2eq1I9QdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bar plot of token counts, with median\n","u,c = np.unique\n","medianTokLength =\n","\n","# make the bar graph\n","plt.figure(figsize=(10,4))\n","plt.bar()\n","plt.axvline(,,,label='Median')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Token character count',ylabel='Frequency',title='Distribution of token lengths')\n","plt.show()"],"metadata":{"id":"dYC66sYzk_bo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print a summary\n","print(f'There are {} tokens shorter than the median.')\n","print(f'There are {} tokens longer than the median.')\n","print(f'There are {} tokens equal to the median.')"],"metadata":{"id":"xfO5bjU1g9sO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QSS5mC2Jt6V9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Get activations"],"metadata":{"id":"S65TJXA6n320"}},{"cell_type":"code","source":["# get a batch of tokens\n","print(allTokens.shape)\n","batch =\n","batch.shape,type(batch)"],"metadata":{"id":"P3OBDpuF9pf4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass the batch\n","# ~1 min on cpu for 125m\n","# 2 secs on gpu for 1.3B (lol)\n","with torch.no_grad():\n","  model("],"metadata":{"id":"oYAzCoDTRj_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations.keys()"],"metadata":{"id":"_bG4KHiGRpJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check shape -- should be batch X tokens X nneurons\n","activations['mlp_10'].shape"],"metadata":{"id":"JQ--6im_ypeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9Fg13-wav-Uf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Activations distributions by median split"],"metadata":{"id":"iYoGs0axtI1p"}},{"cell_type":"code","source":["# extract and flatten activations\n","acts = activations['mlp_4']\n","\n","# activations by length split\n","binedges =\n","yS,_ = torch.histogram(,bins=binedges,density=True)\n","yL,_ = torch.histogram(\n","yM,_ = torch.histogram(\n","\n","# visualize\n","plt.figure(figsize=(10,5))\n","plt.plot(,linewidth=2,label='Short tokens')\n","plt.plot(,label='Long tokens')\n","plt.plot(='Median tokens')\n","\n","plt.gca().set(xlim=binedges[[0,-1]],xlabel='Activations',ylabel='Density',\n","              title='Distribution of activations by token length')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ierevlqGtIyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j68J7qH1Ucfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Activation-length correlations in one layer"],"metadata":{"id":"p-D8BmjvtIrS"}},{"cell_type":"code","source":["# get the flattened activations and numpyify\n","acts = activations['mlp_4']\n","\n","# standardize the activations from all neurons\n","zacts = (acts-mean) / np.std"],"metadata":{"id":"MF02laNjHDwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm\n","zacts.shape, zacts[:,600].mean(), zacts[:,600].std(ddof=1)"],"metadata":{"id":"FXL5sx-FHDuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalize the token lengths\n","zTokenLens =\n","\n","# confirm\n","zTokenLens.mean(), zTokenLens.std(ddof=1)"],"metadata":{"id":"kRJFfI_BibjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm one correlation value\n","np.corrcoef(acts[:,0],allTokenLengths)"],"metadata":{"id":"sZb8TJytEtkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# covariance of standardized variables\n"],"metadata":{"id":"dEkZXE2PjqeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate all correlation coefficients\n","allCorrs = np.zeros(nneurons)\n","\n","for ni in range(nneurons):\n","  allCorrs[ni] =  /"],"metadata":{"id":"CTPxZadHEth4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and visualize!\n","plt.figure(figsize=(8,4))\n","plt.hist(\n","\n","plt.gca().set(xlabel='Correlation coefficient',ylabel='Count',title='Histogram of all correlation coefficients')\n","plt.show()"],"metadata":{"id":"q21DB7W8Etcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iy8db4sKHDzr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 6: Correlations in all layers"],"metadata":{"id":"Pg9E9eLmjesR"}},{"cell_type":"code","source":["allCorrs = np.zeros(())\n","\n","# loop over all the layers\n","for layeri in\n","\n","  # get and normalize the activations\n","  acts = activations[f'mlp_{layeri}']\n","  zacts =\n","\n","  # loop over all the neurons and correlate\n","  for ni in range(nneurons):\n","    allCorrs[layeri,ni] ="],"metadata":{"id":"WV5MS2TXEtfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# histograms\n","rEdges =\n","rHistCounts =\n","\n","# get histogram of each layer\n","for layeri in range(model.config.num_layers\n","  rHistCounts[layeri,:],_ = np.histogram"],"metadata":{"id":"DQttY_pVHDrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and visualize\n","fig,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","for layeri in range(model.config.num_layers):\n","  axs[0].plot(,color=mpl.cm.plasma(layeri/(model.config.num_layers-1)),label=f'MLP h.{layeri}')\n","\n","\n","# colorbar for line color (layer number)\n","cmap = mpl.colormaps['plasma']\n","norm = mpl.colors.BoundaryNorm(np.arange(model.config.num_layers), cmap.N)\n","sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n","cbar = fig.colorbar(sm, ax=axs[0], pad=.01)\n","\n","\n","# image\n","h = axs[1].imshow\n","axs[1].set(xlabel='Correlation coefficient',ylabel='Transformer block',title='Image of all histograms')\n","fig.colorbar(h,ax=axs[1],pad=.01)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"If3UT54CHDl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d7wu9xPBHDjR"},"execution_count":null,"outputs":[]}]}