{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyN/hcJB7sGS2nMHu3p+6Ca0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Category-tuned MLP projections<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"ai-jT0IojqZV"}},{"cell_type":"code","source":["import numpy as np\n","import scipy.stats as stats\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","import matplotlib as mpl\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"U-oRuiQi7TR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import gpt tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"Kx2tBTUW90Z-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mo3q8ET0INkh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Tokenize text and find target locations"],"metadata":{"id":"ECdOCwjOgxwi"}},{"cell_type":"code","source":["# generated by claude\n","sentences = [\n","\n","    # sentences with \"toothpaste,\" \"toothbrush,\" \"floss,\" or \"mouthwash\"\n","\n","    \"She bought a new electric toothbrush that has a timer built in.\",\n","    \"I ran out of toothpaste this morning and had to borrow some from my roommate.\",\n","    \"After brushing, he used mouthwash to freshen his breath before the interview.\",\n","    \"The dentist recommended using dental floss daily to prevent gum disease.\",\n","    \"My travel toothbrush folds up to fit easily in my toiletry bag.\",\n","    \"The minty toothpaste made my mouth feel clean and refreshed.\",\n","    \"He swished the mouthwash around for thirty seconds before spitting it out.\",\n","    \"Waxed floss works better for me because it doesn't fray between my teeth.\",\n","    \"The hotel provided complimentary toothbrush and toothpaste in the bathroom.\",\n","    \"My children prefer fruit-flavored toothpaste over mint varieties.\",\n","    \"The dentist gave me a free sample of antibacterial mouthwash after my cleaning.\",\n","    \"Learning to floss properly took me several attempts and a lesson from my hygienist.\",\n","    \"I keep a spare toothbrush at work for after-lunch brushing.\",\n","    \"The natural toothpaste didn't contain fluoride but still cleaned effectively.\",\n","    \"Using mouthwash too frequently can disrupt the beneficial bacteria in your mouth.\",\n","    \"Water flossers are a good alternative for people who struggle with traditional floss.\",\n","    \"The bristles on my toothbrush were frayed, indicating it was time for a replacement.\",\n","    \"She squeezed the last bit of toothpaste from the tube by rolling it up from the bottom.\",\n","    \"Alcohol-free mouthwash is often recommended for people with sensitive gums.\",\n","    \"The dental hygienist demonstrated the correct technique for using floss picks.\",\n","\n","    # sentences with \"dishwasher,\" \"nightstand,\" \"bookshelf,\" or \"doorknob\"\n","\n","    \"The dishwasher hummed quietly as it ran through its cleaning cycle.\",\n","    \"She placed her reading glasses on the nightstand before turning off the lamp.\",\n","    \"The antique bookshelf dominated the wall with its impressive collection of leather-bound books.\",\n","    \"He turned the doorknob slowly, trying not to wake the sleeping baby in the next room.\",\n","    \"Loading the dishwasher efficiently is truly an art form that few have mastered.\",\n","    \"The water glass on the nightstand had left a circular mark on the wooden surface.\",\n","    \"They installed a new bookshelf to accommodate their growing collection of novels.\",\n","    \"The brass doorknob gleamed after being polished with special metal cleaner.\",\n","    \"The dishwasher leaked water onto the kitchen floor, creating a small puddle.\",\n","    \"Her phone alarm buzzed loudly on the nightstand, jolting her awake.\",\n","    \"The bookshelf sagged in the middle from the weight of encyclopedias and textbooks.\",\n","    \"The doorknob came loose in his hand when he tried to enter the storage room.\",\n","    \"Energy-efficient dishwasher models use significantly less water than older versions.\",\n","    \"The small drawer in the nightstand contained a journal and several pens.\",\n","    \"The children's favorite picture books were displayed on the lowest shelf of the bookshelf.\",\n","    \"The vintage glass doorknob was the original fixture from when the house was built.\",\n","    \"He loaded the dirty dishes into the dishwasher while singing along to the radio.\",\n","    \"The prescription medication and reading light sat on the nightstand within easy reach.\",\n","    \"They secured the tall bookshelf to the wall to prevent it from tipping over.\",\n","    \"The decorative doorknob added a touch of elegance to the otherwise plain door.\"\n","]\n","\n","n_sentences = len(sentences)\n","labels = (np.linspace(0,1,n_sentences)>.5).astype(int)\n","labels"],"metadata":{"id":"z3R01lFsaHCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["targetwords = [' toothpaste',' toothbrush',' floss',    ' mouthwash',\n","               ' dishwasher',' nightstand',' bookshelf',' doorknob'  ]\n","\n","targtoks = []\n","\n","for word in targetwords:\n","  targtoks.append(tokenizer.encode(word))\n","  print\n","\n","targtoks"],"metadata":{"id":"RTvTrB8IaG_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize the sentences\n","tokenizer.pad_token = tokenizer.eos_token\n","tokens = tokenizer(, return_tensors='pt', padding)\n","seq_len =\n","\n","tokens['input_ids'][0], tokens['attention_mask'][0]"],"metadata":{"id":"rFu2-ekZaG8r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (see code file part5_neurons_multitokenWords for more details)\n","\n","# initialize\n","targetlocs = np.zeros(n_sentences,dtype=int)\n","targetvals = np.zeros(n_sentences,dtype=int)\n","\n","\n","# loop over sentences\n","for senti in range(n_sentences):\n","\n","  # loop over target words\n","  for targi in range(len(targtoks)):\n","\n","    # how many tokens in this target word?\n","    targlen = len(targtoks[targi])\n","\n","    # loop over all tokens in this sequence\n","    for ti in range(targlen,seq_len+1):\n","\n","      # target token if the previous mini-sequence matches\n","      if torch.equal(tokens['input_ids'][senti,ti-targlen:ti],torch.tensor(targtoks[targi])):\n","        # target index in sequence\n","        # target value\n","        print(f\"Sentence {} target {} at index {}: '{}'\")"],"metadata":{"id":"vV8kFhwvg8DU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirmation\n","for t in tokens['input_ids'][1]:\n","  print("],"metadata":{"id":"AW3ESv6CojHk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["u,c = np.unique(targetvals,)\n","for uu,cc in zip(u,c):\n","  print"],"metadata":{"id":"KnBayT4-vh0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gx4l0POOifEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Forward pass and get target-word activations"],"metadata":{"id":"oulpT236aG52"}},{"cell_type":"code","source":["# import the model\n","model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","model.eval()"],"metadata":{"id":"XN5MikvOjqWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a function to hook the activations\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    activations[f'mlp_{layer_number}'] =\n","  return hook\n","\n","# put hooks in all layers\n","for layer2hook in range(model.config.n_layer):\n","  .register_forward_hook(implant_hook(layer2hook))"],"metadata":{"id":"0sJWNVHjlz-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process the tokens\n"],"metadata":{"id":"LYnUcW06aG3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations.keys(), activations['mlp_4'].shape"],"metadata":{"id":"TOh6tZB8fT-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize\n","targetActs =\n","\n","# loop over sentences\n","for senti in range(n_sentences):\n","\n","  # loop over layers\n","  for layeri in range(model.config.n_layer):\n","\n","    # get activations from target embeddings vector\n","    targetActs[layeri,senti,:] =\n","\n","targetActs.shape"],"metadata":{"id":"_S6ZOIUbnM2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0MongvrXopmd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: T-tests for dental vs. furniture categories"],"metadata":{"id":"bWwXvi8Jopjr"}},{"cell_type":"code","source":["# two-samples t-test on all neurons between the two categories\n","tres = stats.ttest_ind(\n","    targetActs,\n","    targetActs, axis=)\n","\n","# extract t-values and p-values\n","data1_t =\n","data1_p =\n","\n","# print the shape\n","print(data1_t.shape)"],"metadata":{"id":"mM7B1vx0pp5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# significance threshold (p<.05, Bonferroni)\n","sigPthresh =\n","\n","# setup the figure\n","fig = plt.figure(figsize=(14,4))\n","gs = GridSpec(1,3,figure=fig)\n","ax1 = fig.add_subplot(gs[:2])\n","ax2 = fig.add_subplot(gs[2])\n","\n","\n","# loop over layers\n","numsig = np.zeros(model.config.n_layer)\n","for layeri in range(model.config.n_layer):\n","\n","  # find the significant neurons\n","  issig =\n","  numsig[layeri] =\n","\n","  # draw them\n","  ax1.plot(,,'rx',alpha=.2,markersize=2)\n","  ax1.plot(,,'ko',markerfacecolor=mpl.cm.plasma(layeri/model.config.n_layer),markersize=8,alpha=.3)\n","\n","ax1.set(xlabel='Layer',ylabel='t-statistic',xticks=range(model.config.n_layer),\n","        xlim=[-1,model.config.n_layer],title='T-values for activations of dental vs. furniture')\n","\n","\n","# percent significant neurons per layer\n","ax2.bar()\n","ax2.set(xlabel='Layer',ylabel='% of neurons per layer',xticks=range(0,model.config.n_layer,3),\n","        xlim=[-1,model.config.n_layer],title='Percent significant neurons')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"ro5Ff--NpTqn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5cWfW-AmjNdr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Per-target activations in t-max neurons"],"metadata":{"id":"KxzoBxcMGBhz"}},{"cell_type":"code","source":["# only showing one layer in this plot\n","layeri = 16\n","\n","# pick the max-statistic neurons for each category\n","maxt = data1_t\n","mint = data1_t\n","\n","\n","# draw the figure!\n","plt.figure(figsize=(10,4))\n","for i in range(targetvals.max()+1):\n","\n","  # find max activations for this target and plot\n","  ys =\n","  plt.plot()\n","\n","  # find max activations for this target and plot\n","  ys =\n","  plt.plot\n","\n","plt.axhline\n","plt.axvline\n","plt.gca().set(xticks=range(targetvals.max()+1),xticklabels=targetwords,ylabel='Activation',\n","              title=f'Word and category activations in layer {layeri}')\n","plt.legend([f'L{layeri} / #{maxt}',f'L{layeri} / #{mint}'])\n","\n","plt.show()"],"metadata":{"id":"sL24qLYOwghO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uRWRBMPIppvP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Tokenize multitarget test sentences"],"metadata":{"id":"SBUrqC7gaGuT"}},{"cell_type":"code","source":["# test sentences with both dental and furniture words\n","sentences_mix = [\n","    \"She left her toothbrush on the nightstand after falling asleep reading.\",\n","    \"The tube of toothpaste rolled off the bookshelf and landed on the floor.\",\n","    \"He gargled mouthwash while waiting for the dishwasher cycle to finish.\",\n","    \"I keep dental floss in the small drawer of my nightstand for convenient access.\",\n","    \"The toothpaste cap fell behind the bookshelf, making it difficult to retrieve.\",\n","    \"Her electric toothbrush charging base sits next to the doorknob on the bathroom counter.\",\n","    \"After using mouthwash, he placed the cap back on and returned it to the bookshelf.\",\n","    \"The floss container was knocked off the nightstand by the cat during the night.\",\n","    \"He squeezed the last bit of toothpaste from the tube before placing it in the dishwasher's utensil basket.\",\n","    \"The mouthwash spilled when he bumped into the doorknob while carrying too many items.\",\n","    \"She organized her bathroom supplies, placing her toothbrush and floss in a basket on top of the bookshelf.\",\n","    \"The minty smell of toothpaste lingered near the nightstand where she had been brushing her teeth.\",\n","    \"He accidentally splashed mouthwash on the dishwasher while rinsing his mouth.\",\n","    \"The children's floss picks were stored in a decorative jar on the bookshelf.\",\n","    \"Her toothbrush fell from her hand as she reached for the doorknob in a hurry.\",\n","    \"The blue toothpaste stained the wood of the nightstand when the cap wasn't secured properly.\",\n","    \"He kept a travel-sized mouthwash in the drawer of his nightstand for freshening up in the morning.\",\n","    \"The floss got tangled around the doorknob when the package fell and unraveled.\",\n","    \"She placed her toothbrush in the dishwasher's silverware basket to sanitize it thoroughly.\",\n","    \"The bathroom renovation included a built-in bookshelf with designated spots for toothpaste, mouthwash, and floss.\"\n","]"],"metadata":{"id":"M13XA59naGnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyjay7iHi51p"},"outputs":[],"source":["tokens_mix =\n","seq_len_mix =\n","n_sentences_mix = len(sentences_mix)"]},{"cell_type":"code","source":["tokens_mix['input_ids'][0]"],"metadata":{"id":"M0K1AO27XBPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MMjSzm96-mpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["targetvals_mat = np.full((n_sentences_mix,seq_len_mix),-1,dtype=int)\n","\n","# loop over sentences\n","for senti in range(n_sentences_mix):\n","\n","  # loop over targets\n","  for targi in range(len(targtoks)):\n","\n","    # get target length (number of tokens)\n","    targlen = len(targtoks[targi])\n","\n","    # loop over tokens in sentence\n","    for ti in range(targlen,seq_len_mix+1):\n","\n","      # if this mini-sequence matches the target\n","      if torch.equal(tokens_mix['input_ids'][senti,ti-targlen:ti],torch.tensor(targtoks[targi])):\n","        targetvals_mat\n","        print(f\"Sentence {} target {} at index {}: '{}'\")"],"metadata":{"id":"cHK9of5EXBLd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DbmuldxtWVVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["u,c = np.unique(,return_counts=True)\n","for uu,cc in zip(u,c):"],"metadata":{"id":"JZVfxoOw9Rm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(1,figsize=(10,4))\n","\n","# Note: The colormap in the video only contains 8 possible colors, so 6&7 were mapped to the same color.\n","#       The code here correctly assigns a unique color to each label (and -1).\n","cm_name = 'Set1_r'\n","\n","ax.imshow(,aspect='auto',cmap=cm_name,vmin=-2,vmax=8)\n","\n","\n","# discrete colorbar\n","cmap = mpl.colormaps[cm_name]\n","norm = mpl.colors.BoundaryNorm(np.arange(-1,9), cmap.N)\n","sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n","cbar = fig.colorbar(sm,ax=ax,pad=.01,label='Target index')\n","\n","plt.show()"],"metadata":{"id":"8QyZxWd0XBF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# push through the model\n","with torch.no_grad(): model(**tokens_mix)"],"metadata":{"id":"7rUyvgB87Cf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Activation names:',activations.keys(),'\\n')\n","print('Activation matrix size:',activations['mlp_4'].shape,'\\n')"],"metadata":{"id":"ZgkH6ACg7CdG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IbbSHj1n8Lag"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 6: T-tests on test data"],"metadata":{"id":"x2Ca7_mea5dE"}},{"cell_type":"code","source":["# this cell takes ~2.5 mins\n","\n","# initialize\n","data2_t = np.zeros\n","data2_p = np.zeros\n","\n","# loop over layers\n","for layeri in range(model.config.n_layer):\n","\n","  # loop over neurons\n","  for neuri in range(model.config.n_embd):\n","\n","    # get the activations\n","    tmpa = activations[f'mlp_{layeri}'][:,:,neuri]\n","\n","    # t-test and store the results\n","    ttmp = stats.ttest_ind\n","    data2_t[layeri,neuri] =\n","    data2_p[layeri,neuri] =\n"],"metadata":{"id":"T1f3M5ze7Cag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-S1jg4xWpb0Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vectorize for convenience\n","data1_t_vector =\n","data1_p_vector =\n","data2_t_vector =\n","data2_p_vector =\n","\n","# find where one or both are significant\n","bothSig =\n","\n","\n","# \"concordancy\" as same sign and at least one is significant\n","concordance =\n","\n","# compare the two sets\n","plt.plot(data1_t_vector,data2_t_vector,'rx',markersize=3,alpha=.1,label='Neither sig')\n","plt.plot(data1_t_vector,data2_t_vector,'ko',markerfacecolor=[.9,.7,.7],markersize=5,alpha=.4,label='One sig.')\n","plt.plot(data1_t_vector,data2_t_vector,'ks',markerfacecolor=[.7,.9,.7],markersize=8,label='Both sig.')\n","plt.legend()\n","\n","plt.show()\n","# aka the pistachio canoli plot :P"],"metadata":{"id":"aMXqSXP_7CXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DylffV9j5pZl"},"execution_count":null,"outputs":[]}]}