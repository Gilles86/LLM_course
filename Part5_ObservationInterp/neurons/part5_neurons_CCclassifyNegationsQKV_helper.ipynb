{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOLPGezAQEuPCBulZ0wO90F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Negation tuning in attention neurons<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"fXOrNMhxq2Ya"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib.gridspec import GridSpec\n","\n","import statsmodels.api as sm\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","import requests\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"w-eOA7jdEGch"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1a: Import the model and implant attention hooks"],"metadata":{"id":"jZ_MnC6QEGZz"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# use GPU"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hooks\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    activations\n","  return hook\n"],"metadata":{"id":"QuZiR671yepS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nneurons ="],"metadata":{"id":"-Eja2yLBsJm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h-ni2ehqyemp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1b: Get text, find negations and affirmations"],"metadata":{"id":"CqifcaigyOOF"}},{"cell_type":"code","source":["# https://gutenberg.org/ebooks/32154\n","text = requests.get('https://gutenberg.org/cache/epub/32154/pg32154.txt').text\n","tokens =\n","num_tokens =\n","print(f'There are {} tokens, of which {} are unique.')"],"metadata":{"id":"zrLDgnVZxq1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# context window size (tokens before and after each target)\n","context_pre =\n","context_pst ="],"metadata":{"id":"ptjNsXJS7C8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of negation words to match exactly\n","negation_words = []\n","\n","# initialize vector\n","isNegation = np.zeros(num_tokens,dtype=\n","\n","# loop over all tokens\n","for ti in range(context_pre,num_tokens-context_pst):\n","\n","  # current token\n","  currtok =\n","\n","  # token contains a 't contraction\n","  condA =\n","\n","  # word need to match completely (c.f., not->noted, nor->enormous)\n","  condB =\n","\n","  # next token starts with a space (c.f., not->connotative)\n","  condC =\n","\n","  # test\n","  if\n","    isNegation[ti] = 1\n","\n","\n","# count the targets\n","numNegationTokens =\n","negationsIdx ="],"metadata":{"id":"P7EgksOB2Rqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jFCT1MvIHxXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of affirmation words to match exactly\n","affirmation_words = ['agree','always','allow','can','certainly','could','definitely','may','might','shall','should']\n","\n","# initialize vector\n","isAffirmation =\n","\n","# loop over all tokens\n","for ti in\n","\n","  # current token\n","  currtok =\n","\n","  # next token can't be 'not'\n","  condA =\n","\n","  # word need to match completely (c.f., not->noted, nor->enormous)\n","  condB =\n","\n","  # next token starts with a space (c.f., not->connotative)\n","  condC =\n","\n","  # test\n","  if condA and condB and condC:\n","    isAffirmation[ti] = 1\n","\n","# count the number of target tokens\n","numAffirmationTokens = sum(isAffirmation)\n","affirmationsIdx = np.where(isAffirmation)[0]"],"metadata":{"id":"m3k0aozuHxUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fp22909mI1kg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1c: Create batches and get activations"],"metadata":{"id":"_cn5COMB93ew"}},{"cell_type":"code","source":["# create batches\n","batch_negations =\n","batch_affirmations =\n","\n","\n","# negation sequences\n","for b in range(numNegationTokens):\n","  tokenLoc =\n","  batch_negations[b,:] = tokens\n","\n","# affirmation sequences\n","for b in range(numAffirmationTokens):\n","\n","\n","#\n","print('Shape of negations batch:',batch_negations.shape)\n","print('Shape of affirmations batch:',batch_affirmations.shape)"],"metadata":{"id":"-FMLAjEmI1hm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process the target (negation) tokens\n","with torch.no_grad():\n","  model(batch_negations.to(device))\n","\n","# copy the activations\n","\n","\n","\n","### repeat for affirmations tokens\n"],"metadata":{"id":"KtC8DXBhxqkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(affirmations_activations.keys(),'\\n')\n","\n","affirmations_activations['attn_5'].shape"],"metadata":{"id":"obLbw14UeX0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MCiMqALDrlL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Laminar profile of classification"],"metadata":{"id":"PfH-c__nTdmy"}},{"cell_type":"code","source":["# vector of category labels\n","category_labels = np.hstack"],"metadata":{"id":"weyicIH1LFYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize matrix to store the classifier results\n","pvalues  =\n","betas    =\n","accuracy =\n","\n","\n","# loop over layers\n","for layeri\n","\n","  # loop over neurons for per-neuron analysis\n","  for neuroni in range(nneurons):\n","    # vectorize the activations over batches\n","    targs = negations_activations\n","    comps = affirmations_activations\n","\n","    # build and run the model\n","    try: # sometimes crashes for linear-algebra reasons\n","      result = sm.Logit\n","\n","      # extract the results (p-value, beta, and accuracy)\n","      pvalues[layeri,neuroni]  =\n","      betas[layeri,neuroni]    =\n","      accuracy[layeri,neuroni] =\n","\n","    except: pass\n","\n","  print(f'Finished layer {layeri+1:2}/{model.config.n_layer}')"],"metadata":{"id":"HVLbGTqNTdkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create two masks\n","pvalue_mask =\n","posbet_mask =\n","\n","# get accuracy only from masked neurons\n","# gratuitously confusingly, np.ma.masked_where() actually keeps the False values, and masks *out* True values\n","masked_accuracy = np.ma.masked_where\n","\n","\n","# split the accuracy matrix into the Q/K/V matrices\n","q,k,v = torch.tensor(masked_accuracy).split\n","\n","\n","# make the plot\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","axs[0].plot(,'kH',markerfacecolor=[.7,.7,.7],markersize=9)\n","axs[0].set(xlabel='Layer',ylabel='Percent significant tests (%)',title='Laminar profile of significance')\n","\n","axs[1].plot(,'ko',markerfacecolor=[.9,.7,.7],markersize=9,label='Q')\n","axs[1].plot(,'ks',markerfacecolor=[.7,.9,.7],markersize=9,label='K')\n","axs[1].plot(,'k^',markerfacecolor=[.7,.7,.9],markersize=9,label='V')\n","axs[1].legend()\n","\n","\n","plt.show()"],"metadata":{"id":"cen2pJQPTdhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K-hrgYR3XBws"},"execution_count":null,"outputs":[]}]}