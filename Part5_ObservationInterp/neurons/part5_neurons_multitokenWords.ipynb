{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNwwO3fMdjaroIj+yFLeLQw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Dealing with multitoken word embeddings<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"ai-jT0IojqZV"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer"],"metadata":{"id":"U-oRuiQi7TR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import gpt and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model     = AutoModelForCausalLM.from_pretrained('gpt2')\n","model.eval()"],"metadata":{"id":"Kx2tBTUW90Z-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7mgC4YZulz7s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exploring multitoken word tokenization"],"metadata":{"id":"ECdOCwjOgxwi"}},{"cell_type":"code","source":["# target (multitoken) words and their tokens\n","#                     0           1            2                3             4                5\n","targetwords = [' toothpaste','toothpaste','time machine',' time machine','time-machine',' time-machine' ]\n","\n","# tokenize (in a list b/c number of tokens varies)\n","targtoks = []\n","for word in targetwords:\n","  targtoks.append(tokenizer.encode(word))\n","  print(f'\"{word}\" comprises {len(targtoks[-1])} tokens:\\n    {[tokenizer.decode(t) for t in targtoks[-1]]} -> {targtoks[-1]}\\n')"],"metadata":{"id":"RTvTrB8IaG_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create sentences\n","sentences = []\n","for tw in targetwords:\n","  sentences.append( \"I'd like to read a story about that famous \" + tw )\n","\n","n_sentences = len(sentences)\n","\n","\n","# now to tokenize\n","tokenizer.pad_token = tokenizer.eos_token\n","tokens = tokenizer(sentences, return_tensors='pt', padding=True)\n","seq_len = tokens['input_ids'].shape[-1]\n","tokens"],"metadata":{"id":"rFu2-ekZaG8r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences"],"metadata":{"id":"cc7AwyhNYj89"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sxGWfZOeZ8-n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find the final target tokens in the sentences"],"metadata":{"id":"zSO7obv-Yjz6"}},{"cell_type":"code","source":["targetlocs = np.zeros(n_sentences,dtype=int)\n","\n","# loop over sentences\n","for senti in range(n_sentences):\n","\n","  # loop over target words\n","  for targi in range(len(targtoks)):\n","\n","    # number of tokens in this target\n","    targlen = len(targtoks[targi])\n","\n","    # loop over the token sequence for this sentence\n","    for ti in range(targlen,seq_len+1):\n","\n","      # see if it matches the mini-sequence of target tokens\n","      if torch.equal(tokens['input_ids'][senti,ti-targlen:ti],torch.tensor(targtoks[targi])):\n","        targetlocs[senti] = ti-1\n","        print(f\"Sentence {senti} containts target {targi} at index {ti-1:2}: '{tokenizer.decode(tokens['input_ids'][senti,ti-targlen:ti])}'\")\n","\n","# hint: try removing ' toothpaste' from the target list!"],"metadata":{"id":"vV8kFhwvg8DU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gx4l0POOifEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Forward pass and get target-word activations"],"metadata":{"id":"oulpT236aG52"}},{"cell_type":"code","source":["# process the tokens\n","with torch.no_grad():\n","  outputs = model(**tokens,output_hidden_states=True)\n","\n","hs = outputs.hidden_states"],"metadata":{"id":"LYnUcW06aG3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hs[0].shape"],"metadata":{"id":"TOh6tZB8fT-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["targetActs = np.zeros((model.config.n_layer+1,n_sentences,hs[3].shape[-1],2))\n","\n","# loop over sentences\n","for senti in range(n_sentences):\n","\n","  # loop over layers\n","  for layeri in range(model.config.n_layer+1):\n","\n","    # grab the activation from the final target token\n","    targetActs[layeri,senti,:,0] = hs[layeri][senti,targetlocs[senti],:].numpy()\n","\n","    # and the second-last token\n","    targetActs[layeri,senti,:,1] = hs[layeri][senti,targetlocs[senti]-1,:].numpy()\n","\n","targetActs.shape"],"metadata":{"id":"_S6ZOIUbnM2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"czg0OpOYcWWV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# One quick visualization (difference vector norms)"],"metadata":{"id":"88H8RXGTcWTJ"}},{"cell_type":"code","source":["# Note: the reasoning and math of 'diffNorms' is explained in detail in lecture \"Path length and logit token prediction\"\n","\n","# initialize\n","diffNorms = np.zeros((model.config.n_layer+1,3))\n","\n","\n","# loop over layers\n","for layeri in range(1,targetActs.shape[0]):\n","\n","  # calculate the difference vector from previous attention layer\n","  diffVects = targetActs[layeri,:,:,0] - targetActs[layeri-1,:,:,0]\n","\n","  # calculate its norm\n","  diffNorms[layeri,0] = np.linalg.norm(diffVects,axis=1).mean()\n","\n","\n","  ## repeat for the second-last token\n","  diffVects = targetActs[layeri,:,:,1] - targetActs[layeri-1,:,:,1]\n","  diffNorms[layeri,1] = np.linalg.norm(diffVects,axis=1).mean()\n","\n","\n","  ## repeat for final target tokens within this layer\n","  diffVects = targetActs[layeri,:,:,0] - targetActs[layeri,:,:,1]\n","  diffNorms[layeri,2] = np.linalg.norm(diffVects,axis=1).mean()\n","\n","\n","# plot the difference vector norms\n","plt.figure(figsize=(10,4))\n","plt.plot(range(1,targetActs.shape[0]),diffNorms[1:,0],'ks-',markerfacecolor=[.7,.9,.7],markersize=10,linewidth=.4,label='Final token ($\\Delta$ layer)')\n","plt.plot(range(1,targetActs.shape[0]),diffNorms[1:,1],'ko-',markerfacecolor=[.9,.7,.7],markersize=10,linewidth=.4,label='2nd-last token ($\\Delta$ layer)')\n","plt.plot(range(1,targetActs.shape[0]),diffNorms[1:,2],'k^-',markerfacecolor=[.7,.7,.9],markersize=10,linewidth=.4,label='Last two tokens ($\\Delta$ token)')\n","plt.gca().set(title='How much the embeddings changed',xlabel='Layer',ylabel='$\\Delta$ vector norm')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"1uvq491KgVP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZkcapiHeXBC1"},"execution_count":null,"outputs":[]}]}