{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNAyo5yoxt+d83Y+xHWUcKo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Proper noun tuning in GPT2-medium<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"hoS127gruQQj"}},{"cell_type":"code","source":["# run first to install and then restart\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"JeE3q80J_txG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib.gridspec import GridSpec\n","\n","import statsmodels.api as sm\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","from datasets import load_dataset\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-medium')\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU\n","model = model.to(device)\n","model.eval();"],"metadata":{"id":"mk9BlJEQzyTw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yVd9_AegyOQ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hooks"],"metadata":{"id":"9iG7k4Xsyc3C"}},{"cell_type":"code","source":["model"],"metadata":{"id":"JqbkosMK1Rvu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hooks\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # store in the dictionary\n","    activations[f'mlp_{layer_number}'] = module.c_fc(input[0]).detach().cpu()\n","  return hook\n","\n","# put hooks in all layers\n","for layeri in range(len(model.transformer.h)):\n","  model.transformer.h[layeri].mlp.register_forward_hook(implant_hook(layeri))"],"metadata":{"id":"QuZiR671yepS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h-ni2ehqyemp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get wiki text and find the proper nouns"],"metadata":{"id":"CqifcaigyOOF"}},{"cell_type":"code","source":["# get wiki text\n","text = load_dataset('wikitext','wikitext-2-raw-v1',split='test')\n","\n","# get tokens from longer entries\n","tokens = torch.tensor([],dtype=torch.long)\n","for i in range(200):\n","  if len(text['text'][i])>50:\n","    tokens = torch.concatenate((tokens,tokenizer.encode(text['text'][i],return_tensors='pt')),dim=1)\n","\n","num_tokens = torch.numel(tokens)\n","print(f'{num_tokens:,} tokens imported')"],"metadata":{"id":"zrLDgnVZxq1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example of finding proper nouns\n","' Mike'.strip()[0].isupper()"],"metadata":{"id":"ptjNsXJS7C8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple algorithm, may include false positives and false negatives\n","\n","# initialize vector\n","isProperNoun = np.zeros(num_tokens,dtype=int)\n","\n","# initialize first previous token\n","prevToken = tokenizer.decode(tokens[0,0])\n","\n","# loop over all tokens\n","for ti in range(1,num_tokens):\n","\n","  # current token\n","  thisToken = tokenizer.decode(tokens[0,ti])\n","\n","  # check the previous token\n","  if len(thisToken)>2:\n","\n","    # conditionals\n","    condA = thisToken.strip()[0].isupper() # first letter is upper-case (ignoring leading spaces)\n","    condB = not prevToken.endswith(('.','!','?',':'))  # previous token can't end in an end-of-sentence marker\n","\n","    # if conditionals are met, this is likely a proper noun\n","    if condA and condB:\n","      isProperNoun[ti] = 1\n","\n","  # update previous token\n","  prevToken = thisToken"],"metadata":{"id":"P7EgksOB2Rqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# examine some proper nouns\n","w = np.where(isProperNoun)[0]\n","\n","print(f'There are {len(w)} proper nouns, including:')\n","for i in range(50):\n","  print(f'{tokenizer.decode(tokens[0,w[i]])}')"],"metadata":{"id":"YIzCv9xAxqrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# in context...\n","tokenizer.decode(tokens[0,w[1]-5:w[1]+5])"],"metadata":{"id":"xn3DdPXG5xKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4NWgatD1xqnl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Forward pass through the model to get the activations"],"metadata":{"id":"_cn5COMB93ew"}},{"cell_type":"code","source":["seq_len = 1024\n","\n","# push some data through\n","with torch.no_grad():\n","  model(tokens[0,:seq_len].to(device))"],"metadata":{"id":"KtC8DXBhxqkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations.keys(), activations['mlp_21'].shape"],"metadata":{"id":"kYCKqtGrxqhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nneurons = activations['mlp_21'].shape[-1]"],"metadata":{"id":"8j8d9imbxqe2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"obLbw14UeX0g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Logistic regression exploration"],"metadata":{"id":"W_Xa2c4teXxw"}},{"cell_type":"code","source":["# in one neuron as an illustration\n","\n","# get the proper nouns in this sequence\n","propnounsBatch = np.where(isProperNoun[:seq_len])[0]\n","\n","# get an equal-sample-size set of non-proper-noun tokens for comparison\n","comparisonTokens = np.array([c for c in np.arange(seq_len) if c not in propnounsBatch])\n","np.random.shuffle(comparisonTokens)\n","comparisonTokens = comparisonTokens[:len(propnounsBatch)]\n","\n","# build a logistic model\n","X = np.hstack((activations['mlp_10'][0,comparisonTokens,33],activations['mlp_10'][0,propnounsBatch,33]))\n","y = np.hstack((np.zeros(len(propnounsBatch)),np.ones(len(propnounsBatch))))\n","\n","result = sm.Logit(y,sm.add_constant(X)).fit()\n","\n","# Print the summary, which includes coefficients, p-values, and confidence intervals.\n","print(result.summary())"],"metadata":{"id":"gfoPTT2G8Z8_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d3lWkTFbD3bm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Linear classifier over all neurons in one layer"],"metadata":{"id":"Xj3IFDcODv0O"}},{"cell_type":"code","source":["# sample of proper nouns\n","propnounsBatch = np.where(isProperNoun[:seq_len])[0]\n","\n","# initialize matrix to store the classifier results\n","classifierResults = np.zeros((nneurons,2))\n","\n","# pick an MLP block\n","whichLayer2use = '18'\n","\n","\n","# loop over neurons in this layer\n","for neuroni in range(nneurons):\n","\n","  # build a model\n","  X = np.hstack((\n","      activations[f'mlp_{whichLayer2use}'][0,comparisonTokens,neuroni],\n","      activations[f'mlp_{whichLayer2use}'][0,propnounsBatch,neuroni])\n","      )\n","  y = np.hstack((np.zeros(len(propnounsBatch)),np.ones(len(propnounsBatch))))\n","\n","  # run the model\n","  result = sm.Logit(y,sm.add_constant(X)).fit(disp=0)\n","\n","  # extract the results (p-value and beta)\n","  classifierResults[neuroni,0] = result.pvalues[1]\n","  classifierResults[neuroni,1] = result.params[1]"],"metadata":{"id":"7NiErlVG8Z3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualization of model significance and sign\n","\n","# setup the figure\n","fig = plt.figure(figsize=(12,4))\n","gs = GridSpec(1,4,figure=fig)\n","\n","ax0 = fig.add_subplot(gs[:3])\n","ax1 = fig.add_subplot(gs[3])\n","\n","# find the negative and positive betas, and the supra-threshold results\n","negBetas = classifierResults[:,1]<0\n","posBetas = classifierResults[:,1]>0\n","pvalThresh = .05/nneurons # p<.05, Bonferroni-corrected\n","sigBetas = classifierResults[:,0] < pvalThresh\n","\n","\n","# positive significant betas\n","idx2plot = posBetas & sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'ro',markerfacecolor=[.7,.7,.7],label='Positive and sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log(classifierResults[idx2plot,0]),'ro',markerfacecolor=[.7,.7,.7,.5])\n","\n","# positive non-significant betas\n","idx2plot = posBetas & ~sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'rx',markersize=3,label='Positive and non-sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log(classifierResults[idx2plot,0]),'rx',markersize=3)\n","\n","# negative significant betas\n","idx2plot = negBetas & sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'go',markerfacecolor=[.7,.7,.7],label='Negative and sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log(classifierResults[idx2plot,0]),'go',markerfacecolor=[.7,.7,.7,.5])\n","\n","# negative non-significant betas\n","idx2plot = negBetas & ~sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'gx',markersize=3,label='Negative and non-sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log(classifierResults[idx2plot,0]),'gx',markersize=3)\n","\n","ax0.set(ylabel='Beta coefficient',xlabel='Neuron index',xlim=[-10,nneurons+9],\n","              title='Statistical parameters of proper noun classification')\n","ax0.legend(fontsize=8)\n","\n","\n","ax1.axhline(-np.log(pvalThresh),linestyle='--',color='b',label='Significance threshold')\n","ax1.set(xlabel='Beta coeff',ylabel='-log(p)',title='Betas by p-values')\n","ax1.legend(fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"K3BPO_j-_MwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the neuron with best classification\n","\n","# largest positive beta that's also significant\n","maxBeta = np.max(classifierResults[sigBetas,1])\n","maxBetaNeuron = np.where(classifierResults[:,1]==maxBeta)[0][0]\n","\n","# largest negative beta that's also significant\n","minBeta = np.min(classifierResults[sigBetas,1])\n","minBetaNeuron = np.where(classifierResults[:,1]==minBeta)[0][0]\n","\n","maxBetaNeuron,minBetaNeuron"],"metadata":{"id":"h5_rRiCpEVBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wG5DSXhm8ZvY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Heatmap of neuron's activation in-sample"],"metadata":{"id":"EGhqeyOZ8Zo9"}},{"cell_type":"code","source":["# min-max scale the activations for colormapping\n","posActs = activations[f'mlp_{whichLayer2use}'][0,:,maxBetaNeuron].squeeze()\n","posActsNorm = (posActs - posActs.min()) / (posActs.max()-posActs.min())\n","\n","negActs = activations[f'mlp_{whichLayer2use}'][0,:,minBetaNeuron].squeeze()\n","negActsNorm = (negActs - negActs.min()) / (negActs.max()-negActs.min())"],"metadata":{"id":"iaf_3HXCH0vk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get width of one letter\n","fig,ax = plt.subplots(figsize=(10,2))\n","\n","# draw a text object\n","temp_text = ax.text(0,0,'n',fontsize=12,fontfamily='monospace')\n","\n","# Get its bounding box in display coordinates\n","bbox = temp_text.get_window_extent(renderer=fig.canvas.get_renderer())\n","\n","# convert from display to axis coordinates\n","inv = ax.transAxes.inverted()\n","bbox_axes = inv.transform([[bbox.x0,bbox.y0], [bbox.x1,bbox.y1]])\n","en_width = bbox_axes[1,0] - bbox_axes[0,0] # bbox is [(x0,y0),(x1,y1)]\n","\n","plt.close(fig)"],"metadata":{"id":"pHd_EFsqqBsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokCount = 0\n","\n","x_pos = 0  # starting x position (in axis coordinates)\n","y_pos = 1  # vertical center\n","\n","fig, ax = plt.subplots(figsize=(10,2))\n","ax.axis('off')\n","\n","for toki in range(seq_len//2):\n","\n","  # text of this token\n","  toktext = tokenizer.decode([tokens[0,toki]])\n","\n","  # width of the token\n","  token_width = en_width*len(toktext)\n","\n","  # text object with background color matching the activation\n","  ax.text(x_pos+token_width/2, y_pos, toktext, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","          bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.Reds(posActsNorm[toki]**2), edgecolor='none', alpha=.8))\n","\n","  # update the token counter and x_pos\n","  tokCount += 1\n","  x_pos += token_width + .01 # plus a small gap\n","\n","  # end of the line; reset coordinates and counter\n","  if tokCount>=20:\n","    y_pos -= .17\n","    x_pos = 0\n","    tokCount = 0\n","\n","plt.show()"],"metadata":{"id":"x9vSYIo1qBou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3j75XeCrxqyg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Process the next batch to examine generalization"],"metadata":{"id":"oDAQssueM8WD"}},{"cell_type":"code","source":["# push some data through\n","with torch.no_grad():\n","  model(tokens[0,seq_len:seq_len*2].to(device))"],"metadata":{"id":"sBTyvHzCM8Tt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# min-max scale the activations for colormapping\n","posActs = activations[f'mlp_{whichLayer2use}'][0,:,maxBetaNeuron].squeeze()\n","posActsNorm = (posActs - posActs.min()) / (posActs.max()-posActs.min())"],"metadata":{"id":"XjRSn4xnM8Q0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokCount = 0\n","\n","x_pos = 0  # starting x position (in axis coordinates)\n","y_pos = 1  # vertical center\n","\n","fig, ax = plt.subplots(figsize=(10,2))\n","ax.axis('off')\n","\n","for toki in range(seq_len//2):\n","\n","  # text of this token\n","  toktext = tokenizer.decode([tokens[0,seq_len+toki]])\n","\n","  # width of the token\n","  token_width = en_width*len(toktext)\n","\n","  # text object with background color matching the activation\n","  ax.text(x_pos+token_width/2, y_pos, toktext, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","          bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.Reds(posActsNorm[toki]**2), edgecolor='none', alpha=.8))\n","\n","  # update the token counter and x_pos\n","  tokCount += 1\n","  x_pos += token_width + .01 # plus a small gap\n","\n","  # end of the line; reset coordinates and counter\n","  if tokCount>=20:\n","    y_pos -= .17\n","    x_pos = 0\n","    tokCount = 0\n","\n","plt.show()"],"metadata":{"id":"C6Xo4nCSM8NT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RuCeVtIUNWs5"},"execution_count":null,"outputs":[]}]}