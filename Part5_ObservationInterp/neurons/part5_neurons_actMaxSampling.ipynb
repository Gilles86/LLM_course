{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"12V11BB9f-wLGMIcZ7SHsypfoeQome92O","timestamp":1752060835298}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPneqEqWs77m5Wigd/Jck3x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Activation maximization via data sampling<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"vjwYlttCAj7X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import numpy as np\n","\n","import requests"]},{"cell_type":"code","source":["# Eleuther's tokenizer and 125m model\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# -> GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qH0so0epSGWu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import and process texts"],"metadata":{"id":"bCqF2dybPuwr"}},{"cell_type":"code","source":["# Through the Looking Glass (aka Alice in Wonderland)\n","text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n","tokens = torch.tensor( tokenizer.encode(text),dtype=torch.long )\n","\n","# summary\n","print(f'Alice in Wonderland has  {len(tokens):7,} tokens.')"],"metadata":{"id":"SS8ySA1SHPhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data sample size parameters\n","seq_len    = 256 # max sequence length\n","batch_size =  32"],"metadata":{"id":"CwOb8RCqT92V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rzXSuqVU_XN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# One example"],"metadata":{"id":"J6INAPla_XKD"}},{"cell_type":"code","source":["# create a batch of data\n","ix = torch.randint(len(tokens)-seq_len,size=(batch_size,))\n","X  = tokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","# forward pass and hidden-state activations\n","with torch.no_grad():\n","  outputs = model(X,output_hidden_states=True)\n","\n","len(outputs.hidden_states), outputs.hidden_states[4].shape"],"metadata":{"id":"fW8W0QRA_YS5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pick a layer and a dimension\n","layer = 2\n","dim = 345"],"metadata":{"id":"hZLNMIma_XG7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the token with the max activation\n","layeracts = outputs.hidden_states[layer][:,:,dim].detach().cpu()\n","maxtokenidx = X.flatten()[np.argmax(layeracts)].item()\n","maxtoken = tokenizer.decode([maxtokenidx])\n","\n","print(f'Token with max activation is \"{maxtoken}\"')"],"metadata":{"id":"ArRUuiLr_XD0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-JhncV9lT9tS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Lots of examples"],"metadata":{"id":"x1XR5KM7C6F-"}},{"cell_type":"code","source":["# (~2 min)\n","\n","num_samples = 1000\n","\n","# initialize the results\n","maxtokens = np.zeros(num_samples,dtype=int)\n","\n","# loop over training\n","for sampli in range(num_samples):\n","\n","  # create a batch of data\n","  ix = torch.randint(len(tokens)-seq_len,size=(batch_size,))\n","  X  = tokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # forward pass and hidden-state activations\n","  with torch.no_grad(): outputs=model(X,output_hidden_states=True)\n","\n","  # find the token with the max activation\n","  layeracts = outputs.hidden_states[layer][:,:,dim].detach().cpu()\n","  maxtokens[sampli] = X.flatten()[np.argmax(layeracts)].item()\n"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the unique max-activation counts\n","u,c = np.unique(maxtokens,return_counts=True)\n","sidx = np.argsort(c)[::-1]\n","\n","# print out the results\n","for t,cc in zip(u[sidx],c[sidx]):\n","  print(f'{cc:3} ({cc*100/num_samples:4.1f}%) max-acts for token \"{tokenizer.decode([t])}\"')"],"metadata":{"id":"0QULjdl7D5ZM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P9YC0HXyHPzb"},"execution_count":null,"outputs":[]}]}