{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPvdsaJVbSI0TpZrXvtFvaP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Negation tuning in MLP neurons<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"fXOrNMhxq2Ya"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib.gridspec import GridSpec\n","\n","import statsmodels.api as sm\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","import requests\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"w-eOA7jdEGch"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import the model and implant MLP hooks"],"metadata":{"id":"jZ_MnC6QEGZz"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# move the model to the GPU\n"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this time, hook the output instead of re-calculating the linear layer\n","activations =\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # store in the dictionary\n","    activations\n","  return hook\n","\n","# put hooks in all layers\n"],"metadata":{"id":"x01-3ZnZ4VgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nneurons ="],"metadata":{"id":"-Eja2yLBsJm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h-ni2ehqyemp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Get text, find negations and affirmations"],"metadata":{"id":"CqifcaigyOOF"}},{"cell_type":"code","source":["# https://gutenberg.org/ebooks/32154\n","text = requests.get('https://gutenberg.org/cache/epub/32154/pg32154.txt').text\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","num_tokens =\n","print(f'There are {num_tokens:,} tokens, of which {} are unique.')"],"metadata":{"id":"zrLDgnVZxq1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# context window size (tokens before and after each target)\n","context_pre =\n","context_pst ="],"metadata":{"id":"ptjNsXJS7C8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of negation words to match exactly\n","negation_words = []\n","\n","# initialize vector\n","isNegation = np.zeros(num_tokens,dtype=)\n","\n","# loop over all tokens\n","for ti in\n","\n","  # current token\n","  currtok = tokenizer.decode\n","\n","  # token contains a 't contraction\n","  condA =\n","\n","  # word need to match completely (c.f., not->noted, nor->enormous)\n","  condB =\n","\n","  # next token starts with a space (c.f., not->connotative)\n","  condC =\n","\n","  # test\n","  if ...:\n","    isNegation[ti] = 1\n","\n","\n","# count the targets\n","numNegationTokens ="],"metadata":{"id":"P7EgksOB2Rqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["negationsIdx =\n","context_win = 15\n","\n","# examine some proper nouns\n","print(f'There are {numNegationTokens} \"negation\" tokens in the dataset.\\n')\n","for t in negationsIdx[:10]:\n","  print(f'Example {t}:"],"metadata":{"id":"YIzCv9xAxqrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all the negation tokens\n","tokenizer.decode(tokens[0,negationsIdx])"],"metadata":{"id":"nL7ElzyH-otM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jFCT1MvIHxXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of affirmation words to match exactly\n","affirmation_words = []\n","\n","# initialize vector\n","isAffirmation = np.zeros\n","\n","# loop over all tokens\n","for ti in\n","\n","  # current token\n","  currtok =\n","\n","  # next token can't be 'not'\n","  condA =\n","\n","  # word need to match completely (c.f., not->noted, nor->enormous)\n","  condB =\n","\n","  # next token starts with a space (c.f., not->connotative)\n","  condC =\n","\n","  # test\n","  if\n","    isAffirmation[ti] = 1\n","\n","# count the number of target tokens\n","numAffirmationTokens ="],"metadata":{"id":"m3k0aozuHxUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["affirmationsIdx =\n","\n","# examine some examples\n","print(f'There are {numAffirmationTokens} \"affirmation\" tokens in the dataset.\\n')\n","for t in affirmationsIdx[:10]:\n","  print("],"metadata":{"id":"ejzpwwa8Ic8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fp22909mI1kg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Create batches and get activations"],"metadata":{"id":"_cn5COMB93ew"}},{"cell_type":"code","source":["# create batches\n","batch_negations = torch.zeros((),dtype=)\n","batch_affirmations = torch.zeros\n","\n","\n","# negation sequences\n","for b in range(numNegationTokens):\n","  tokenLoc =\n","  batch_negations[b,:] = tokens\n","\n","# affirmation sequences\n","for b in range(numAffirmationTokens):\n","\n","  batch_affirmations\n","\n","#\n","print('Shape of negations batch:',batch_negations.shape)\n","print('Shape of affirmations batch:',batch_affirmations.shape)"],"metadata":{"id":"-FMLAjEmI1hm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process the target (negation) tokens\n","with torch.no_grad():\n","  model(batch_negations.to(device))\n","\n","# copy the activations\n","\n","\n","\n","### repeat for affirmations tokens\n"],"metadata":{"id":"KtC8DXBhxqkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(affirmations_activations.keys(),'\\n')\n","\n","affirmations_activations['mlp_5'].shape"],"metadata":{"id":"obLbw14UeX0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MCiMqALDrlL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Logistic regression in all neurons from one layer"],"metadata":{"id":"Xj3IFDcODv0O"}},{"cell_type":"code","source":["# a vector of category labels (0's for affirmation words, 1's for negation words)\n","category_labels ="],"metadata":{"id":"weyicIH1LFYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MLP transformer layer\n","whichLayer2use = 13"],"metadata":{"id":"VoBfPMPDEkoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm getting the right token\n","tokenizer.decode(batch_negations[3,context_pre]), tokenizer.decode(batch_affirmations[3,context_pre])"],"metadata":{"id":"FF4v03tjTl4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize matrix to store the classifier results\n","classifierResults = np.full(size,np.nan)\n","\n","# loop over neurons for per-neuron analysis\n","for neuroni in\n","\n","  # vectorize the activations over batches\n","  targs = negations_activations\n","  comps = affirmations_activations\n","\n","  # build and run the model\n","  try:\n","    result = sm.Logit(\n","        ,\n","\n","        ).fit(maxiter=3000,disp=0)\n","\n","    # extract the results (p-value and beta)\n","    classifierResults[neuroni,0] = # p-value of beta1\n","    classifierResults[neuroni,1] = # beta1\n","  except: pass"],"metadata":{"id":"7NiErlVG8Z3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualization of model significance and sign\n","\n","# setup the figure\n","fig = plt.figure(figsize=(12,4))\n","gs = GridSpec(1,4,figure=fig)\n","\n","ax0 = fig.add_subplot(gs[:3])\n","ax1 = fig.add_subplot(gs[3])\n","\n","# find the negative and positive betas, and the supra-threshold results\n","negBetas =\n","posBetas =\n","pvalThresh =  # p<.05, Bonferroni-corrected\n","sigBetas =\n","\n","\n","# positive significant betas\n","idx2plot = posBetas & sigBetas\n","ax0.plot(np.where()[0],classifierResults[],'ro',markerfacecolor=[.7,.7,.7],label='Positive and sig.')\n","ax1.plot(classifierResults[idx2plot,1],,'ro',markerfacecolor=[.7,.7,.7,.5])\n","\n","# positive non-significant betas\n","\n","\n","# negative significant betas\n","\n","\n","# negative non-significant betas\n","\n","\n","ax0.set(ylabel='Beta coefficient',xlabel='Neuron index',xlim=[-10,nneurons+9],\n","              title='Statistical parameters of negation-term classification')\n","ax0.legend(fontsize=8)\n","\n","\n","ax1.axhline(-np.log(pvalThresh),linestyle='--',color='b',label='Significance threshold')\n","ax1.set(xlabel='Beta coeff',ylabel='-log(p)',title='Betas by p-values')\n","ax1.legend(fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"K3BPO_j-_MwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the neuron with best classification\n","maxBeta = np.max(\n","maxBetaNeuron = np.where\n","maxBetaNeuron"],"metadata":{"id":"h5_rRiCpEVBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wW_QcMhGv6PP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show the prediction probabilities for the max neuron\n","\n","# need to re-run the model for that neuron\n","targs = negations_activations[f'mlp_{whichLayer2use}']\n","comps = affirmations_activations\n","result = sm.Logit(,sm.add_constant).fit\n","\n","# per-token accuracy\n","accuracy ="],"metadata":{"id":"r4r0O_Syv6Jq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualization\n","plt.figure(figsize=(10,4))\n","\n","plt.plot(category_labels,'rx')\n","plt.plot(result.predict(),'ko',markerfacecolor=[.7,.9,.7,.5],markersize=8)\n","\n","\n","plt.show()"],"metadata":{"id":"AxfLic93RluM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v_NdfrwEOzBY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Text heatmap of “best” neuron’s activations"],"metadata":{"id":"CQH4jSJOEygR"}},{"cell_type":"code","source":["# scale the activity for colormapping\n","negActs = negations_activations\n","negActsNorm ="],"metadata":{"id":"iaf_3HXCH0vk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all of this code is copied from several previous code files\n","fig,ax = plt.subplots(figsize=(10,2))\n","temp_text = ax.text(0,0,'n',fontsize=12,fontfamily='monospace')\n","bbox = temp_text.get_window_extent(renderer=fig.canvas.get_renderer())\n","inv = ax.transAxes.inverted()\n","bbox_axes = inv.transform([[bbox.x0,bbox.y0], [bbox.x1,bbox.y1]])\n","en_width = bbox_axes[1,0] - bbox_axes[0,0] # bbox is [(x0,y0),(x1,y1)]\n","plt.close(fig)"],"metadata":{"id":"pHd_EFsqqBsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_pos = 0  # starting x position (in axis coordinates)\n","y_pos = 1  # vertical center\n","\n","fig, ax = plt.subplots(figsize=(10,2))\n","ax.axis('off')\n","\n","\n","# loop over rows in the batch\n","for batchi in range(20): # just the first 20 sequences\n","\n","  for toki in range(context_pre-5,batch_negations.shape[1]):\n","\n","    # text of this token\n","    toktext = tokenizer.decode([batch_negations[batchi,toki]])\n","\n","    # width of the token\n","    token_width = en_width*len(toktext)\n","\n","    # text object with background color matching the activation\n","    ax.text(x_pos+token_width/2, y_pos, toktext, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","            bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.Reds(negActsNorm[batchi,toki]**2), edgecolor='none', alpha=.8))\n","\n","    # update the token counter and x_pos\n","    x_pos += token_width + .01 # plus a small gap\n","\n","  # end of the line; reset coordinates and counter\n","  y_pos -= .17\n","  x_pos = 0\n","\n","plt.show()"],"metadata":{"id":"x9vSYIo1qBou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3j75XeCrxqyg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 6: Laminar profile of classification"],"metadata":{"id":"PfH-c__nTdmy"}},{"cell_type":"code","source":["# initialize matrix to store the classifier results\n","pvalues  =  # initialize to 1's to ignore in subsequent mask\n","betas    = np.zeros\n","accuracy =\n","\n","\n","# loop over layers\n","for layeri\n","\n","  # loop over neurons for per-neuron analysis\n","  for neuroni\n","    # vectorize the activations over batches\n","    targs = negations_activations\n","    comps = affirmations_activations\n","\n","    # build and run the model\n","    try: # sometimes crashes for linear-algebra reasons\n","      result =\n","\n","      # extract the results (p-value, beta, and accuracy)\n","      pvalues[layeri,neuroni]  =\n","      betas[layeri,neuroni]    =\n","      accuracy[layeri,neuroni] =\n","\n","    except: pass\n","\n","  print(f'Finished layer {layeri+1:2}/{model.config.n_layer}')"],"metadata":{"id":"HVLbGTqNTdkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create two masks\n","pvalue_mask =\n","posbet_mask =\n","\n","# get accuracy only from masked neurons\n","# gratuitously confusingly, np.ma.masked_where() actually keeps the False values, and masks *out* True values\n","masked_accuracy = np.ma.masked_where\n","\n","# make the plot\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","axs[0].plot(,'kH',markerfacecolor=[.7,.7,.7],markersize=9)\n","axs[0].set(xlabel='Layer',ylabel='Percent significant tests (%)',title='Laminar profile of significance')\n","\n","axs[1].plot(,'ko',markerfacecolor=[.9,.7,.7],markersize=9,label='All tests')\n","axs[1].plot(,'ks',markerfacecolor=[.7,.9,.7],markersize=9,label='Only sig. $\\\\beta$s>0')\n","axs[1].legend()\n","\n","axs[1].set(xlabel='Layer',ylabel='Prediction accuracy (%)',title='Laminar profile of prediction accuracy')\n","plt.show()"],"metadata":{"id":"cen2pJQPTdhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K-hrgYR3XBws"},"execution_count":null,"outputs":[]}]}