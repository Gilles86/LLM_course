{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMngUpRyywRycNJ50o1jQyc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Negation tuning in attention neurons<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"fXOrNMhxq2Ya"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib.gridspec import GridSpec\n","\n","import statsmodels.api as sm\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","import requests\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"w-eOA7jdEGch"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1a: Import the model and implant attention hooks"],"metadata":{"id":"jZ_MnC6QEGZz"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hooks\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    activations[f'attn_{layer_number}'] = output.detach().cpu()\n","  return hook\n","\n","# put hooks in all layers\n","for layeri in range(len(model.transformer.h)):\n","  model.transformer.h[layeri].attn.c_attn.register_forward_hook(implant_hook(layeri))"],"metadata":{"id":"QuZiR671yepS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nneurons = model.transformer.h[0].attn.c_attn.weight.shape[-1]"],"metadata":{"id":"-Eja2yLBsJm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h-ni2ehqyemp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1b: Get text, find negations and affirmations"],"metadata":{"id":"CqifcaigyOOF"}},{"cell_type":"code","source":["# https://gutenberg.org/ebooks/32154\n","text = requests.get('https://gutenberg.org/cache/epub/32154/pg32154.txt').text\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","num_tokens = len(tokens[0])\n","print(f'There are {num_tokens:,} tokens, of which {len(np.unique(tokens[0])):,} are unique.')"],"metadata":{"id":"zrLDgnVZxq1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# context window size (tokens before and after each target)\n","context_pre = 90\n","context_pst = 10"],"metadata":{"id":"ptjNsXJS7C8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of negation words to match exactly\n","negation_words = ['not','cannot','nor','never']\n","\n","# initialize vector\n","isNegation = np.zeros(num_tokens,dtype=int)\n","\n","# loop over all tokens\n","for ti in range(context_pre,num_tokens-context_pst):\n","\n","  # current token\n","  currtok = tokenizer.decode(tokens[0,ti]).strip().lower()\n","\n","  # token contains a 't contraction\n","  condA = (\"'t\" in currtok) or (\"n't\" in currtok)\n","\n","  # word need to match completely (c.f., not->noted, nor->enormous)\n","  condB = currtok in negation_words\n","\n","  # next token starts with a space (c.f., not->connotative)\n","  condC = tokenizer.decode(tokens[0,ti+1])[0] == ' '\n","\n","  # test\n","  if (condA or condB) and condC:\n","    isNegation[ti] = 1\n","\n","\n","# count the targets\n","numNegationTokens = sum(isNegation)\n","negationsIdx = np.where(isNegation)[0]"],"metadata":{"id":"P7EgksOB2Rqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jFCT1MvIHxXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of affirmation words to match exactly\n","affirmation_words = ['agree','always','allow','can','certainly','could','definitely','may','might','shall','should']\n","\n","# initialize vector\n","isAffirmation = np.zeros(num_tokens,dtype=int)\n","\n","# loop over all tokens\n","for ti in range(context_pre,num_tokens-context_pst):\n","\n","  # current token\n","  currtok = tokenizer.decode(tokens[0,ti]).strip().lower()\n","\n","  # next token can't be 'not'\n","  condA = tokenizer.decode(tokens[0,ti+1]) != ' not'\n","\n","  # word need to match completely (c.f., not->noted, nor->enormous)\n","  condB = currtok in affirmation_words\n","\n","  # next token starts with a space (c.f., not->connotative)\n","  condC = tokenizer.decode(tokens[0,ti+1])[0] == ' '\n","\n","  # test\n","  if condA and condB and condC:\n","    isAffirmation[ti] = 1\n","\n","# count the number of target tokens\n","numAffirmationTokens = sum(isAffirmation)\n","affirmationsIdx = np.where(isAffirmation)[0]"],"metadata":{"id":"m3k0aozuHxUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fp22909mI1kg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1c: Create batches and get activations"],"metadata":{"id":"_cn5COMB93ew"}},{"cell_type":"code","source":["# create batches\n","batch_negations = torch.zeros((numNegationTokens,context_pre+context_pst+1),dtype=torch.long)\n","batch_affirmations = torch.zeros((numAffirmationTokens,context_pre+context_pst+1),dtype=torch.long)\n","\n","\n","# negation sequences\n","for b in range(numNegationTokens):\n","  tokenLoc = negationsIdx[b]\n","  batch_negations[b,:] = tokens[0,tokenLoc-context_pre:tokenLoc+context_pst+1]\n","\n","# affirmation sequences\n","for b in range(numAffirmationTokens):\n","  tokenLoc = affirmationsIdx[b]\n","  batch_affirmations[b,:] = tokens[0,tokenLoc-context_pre:tokenLoc+context_pst+1]\n","\n","#\n","print('Shape of negations batch:',batch_negations.shape)\n","print('Shape of affirmations batch:',batch_affirmations.shape)"],"metadata":{"id":"-FMLAjEmI1hm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process the target (negation) tokens\n","with torch.no_grad():\n","  model(batch_negations.to(device))\n","\n","# copy the activations\n","negations_activations = activations.copy()\n","\n","\n","### repeat for affirmations tokens\n","with torch.no_grad():\n","  model(batch_affirmations.to(device))\n","affirmations_activations = activations.copy()"],"metadata":{"id":"KtC8DXBhxqkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(affirmations_activations.keys(),'\\n')\n","\n","affirmations_activations['attn_5'].shape"],"metadata":{"id":"obLbw14UeX0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MCiMqALDrlL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Laminar profile of classification"],"metadata":{"id":"PfH-c__nTdmy"}},{"cell_type":"code","source":["# we'll use this vector repeatedly\n","category_labels = np.hstack((np.zeros(numAffirmationTokens),np.ones(numNegationTokens)))"],"metadata":{"id":"weyicIH1LFYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize matrix to store the classifier results\n","pvalues  = np.ones((model.config.n_layer,nneurons)) # initialize to 1's to ignore in subsequent mask\n","betas    = np.zeros((model.config.n_layer,nneurons))\n","accuracy = np.zeros((model.config.n_layer,nneurons))\n","\n","\n","# loop over layers\n","for layeri in range(model.config.n_layer):\n","\n","  # loop over neurons for per-neuron analysis\n","  for neuroni in range(nneurons):\n","    # vectorize the activations over batches\n","    targs = negations_activations[f'attn_{layeri}'][:,context_pre,neuroni]\n","    comps = affirmations_activations[f'attn_{layeri}'][:,context_pre,neuroni]\n","\n","    # build and run the model\n","    try: # sometimes crashes for linear-algebra reasons\n","      result = sm.Logit(category_labels,sm.add_constant(np.hstack((comps,targs)))).fit(maxiter=3000,disp=0)\n","\n","      # extract the results (p-value, beta, and accuracy)\n","      pvalues[layeri,neuroni]  = result.pvalues[1]\n","      betas[layeri,neuroni]    = result.params[1]\n","      accuracy[layeri,neuroni] = 100*((result.predict()>.5)==category_labels).mean()\n","\n","    except: pass\n","\n","  print(f'Finished layer {layeri+1:2}/{model.config.n_layer}')"],"metadata":{"id":"HVLbGTqNTdkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create two masks\n","pvalue_mask = pvalues<.05/nneurons\n","posbet_mask = betas>0\n","\n","# get accuracy only from masked neurons\n","# gratuitously confusingly, np.ma.masked_where() actually keeps the False values, and masks *out* True values\n","masked_accuracy = np.ma.masked_where(~(pvalue_mask & posbet_mask),accuracy)\n","\n","\n","# split the accuracy matrix into the Q/K/V matrices\n","q,k,v = torch.tensor(masked_accuracy).split(model.config.n_embd,dim=1)\n","\n","\n","# make the plot\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","axs[0].plot(100*np.mean(pvalue_mask,axis=1),'kH',markerfacecolor=[.7,.7,.7],markersize=9)\n","axs[0].set(xlabel='Layer',ylabel='Percent significant tests (%)',title='Laminar profile of significance')\n","\n","axs[1].plot(torch.mean(q,dim=1),'ko',markerfacecolor=[.9,.7,.7],markersize=9,label='Q')\n","axs[1].plot(torch.mean(v,axis=1),'ks',markerfacecolor=[.7,.9,.7],markersize=9,label='K')\n","axs[1].plot(torch.mean(k,axis=1),'k^',markerfacecolor=[.7,.7,.9],markersize=9,label='V')\n","axs[1].legend()\n","\n","axs[1].set(xlabel='Layer',ylabel='Prediction accuracy (%)',title='Laminar profile of prediction accuracy')\n","plt.show()"],"metadata":{"id":"cen2pJQPTdhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K-hrgYR3XBws"},"execution_count":null,"outputs":[]}]}