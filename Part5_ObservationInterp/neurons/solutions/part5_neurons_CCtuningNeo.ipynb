{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNNphFPf8PTKjvgvYj2wmf8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Grammar tuning in MLP neurons?<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"EBT02JwWkMri"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import requests\n","import scipy.stats as stats\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","# high res matplotlib\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"KPPVI-Rp0NBc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Get nouns and verbs"],"metadata":{"id":"uu2yxPwQ0M-g"}},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","tokenizer.pad_token_id = tokenizer.encode(' ')[0]\n","\n","# load in GPTneo\n","model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","model.eval()\n","\n","embed_dim = model.config.hidden_size"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lists of verbs\n","url = 'https://raw.githubusercontent.com/david47k/top-english-wordlists/refs/heads/master/top_english_verbs_lower_10000.txt'\n","verbs = requests.get(url).text\n","verbs = verbs.split('\\n')[:100]\n","verbs"],"metadata":{"id":"ng-VWPU_eXPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat for nouns\n","url = 'https://raw.githubusercontent.com/david47k/top-english-wordlists/refs/heads/master/top_english_nouns_lower_10000.txt'\n","nouns = requests.get(url).text\n","nouns = nouns.split('\\n')[:100]\n","nouns"],"metadata":{"id":"D9LEmR85sH3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dYC66sYzk_bo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Implant a hook"],"metadata":{"id":"gEaT5J0hn0mw"}},{"cell_type":"code","source":["model"],"metadata":{"id":"lbUitFlPnuAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# number of MLP 'expansion' units\n","nneurons = model.transformer.h[8].mlp.c_fc.weight.shape[0]"],"metadata":{"id":"JLPDDBnKoaO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a hook function to grab the activations\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # get the activations\n","    acts = module.c_fc(input[0])  # [batch, seq, 4xembed_dim]\n","\n","    # store in the dictionary\n","    activations[f'mlp_{layer_number}_x'] = acts\n","  return hook\n","\n","\n","# pick the layer to hook\n","layer2hook = 8\n","hookName = f'mlp_{layer2hook}_x'\n","\n","# surgery ;)\n","model.transformer.h[layer2hook].mlp.register_forward_hook(implant_hook(layer2hook))"],"metadata":{"id":"-5ZWugpPnt85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QSS5mC2Jt6V9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test\n","text = 'Are you not entertained?'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","\n","# forward pass to trigger the hook\n","with torch.no_grad(): model(tokens)\n","activations[hookName].shape"],"metadata":{"id":"1-Xo9ZQrnt3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X_XUC_d3nt1g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Get activations for all words"],"metadata":{"id":"S65TJXA6n320"}},{"cell_type":"code","source":["# initialize tensor for all activations\n","all_activations = np.zeros((2,len(verbs),nneurons))\n","\n","\n","# loop over the tokens\n","for wordi in range(len(verbs)):\n","\n","  # forward pass this verb\n","  with torch.no_grad(): model(tokenizer.encode(verbs[wordi],return_tensors='pt'))\n","\n","  # grab the activations\n","  all_activations[0,wordi,:] = activations[hookName].mean(dim=1).squeeze().detach().numpy()\n","\n","\n","\n","  ### repeat for nouns\n","  with torch.no_grad(): model(tokenizer.encode(nouns[wordi],return_tensors='pt'))\n","  all_activations[1,wordi,:] = activations[hookName].mean(dim=1).squeeze().detach().numpy()\n"],"metadata":{"id":"JQ--6im_ypeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","axs[0].imshow(all_activations[0,:,:],aspect='auto',vmin=-2,vmax=1)\n","axs[0].set(xlabel='Neurons',ylabel='Verbs (index)',title='Verbs activations')\n","\n","axs[1].plot(all_activations[1,:,:].mean(axis=0),'ko',markerfacecolor=[.7,.7,.7,.6])\n","axs[1].set(xlim=[-6,nneurons+7],xlabel='Neurons',ylabel='Activation',title='Mean activations over all nouns')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"oygn48FUv-XA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9Fg13-wav-Uf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Compare activations with t-tests"],"metadata":{"id":"iYoGs0axtI1p"}},{"cell_type":"code","source":["all_activations.shape\n","np.diff(all_activations,axis=0).shape\n","# np.diff(all_activations,axis=0)"],"metadata":{"id":"ierevlqGtIyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run the t-test\n","t,p = stats.ttest_1samp(np.diff(all_activations,axis=0),popmean=0,axis=1)"],"metadata":{"id":"432Hkk4ZtIu5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# p-value threshold (Bonferroni correction for multiple comparisons)\n","pThresh = .05/nneurons\n","\n","plt.figure(figsize=(10,4))\n","plt.plot(np.where(p>pThresh)[1],t[p>pThresh],'rs',markersize=4,markerfacecolor=[.9,.6,.6],label='Non-significant')\n","plt.plot(np.where(p<pThresh)[1],t[p<pThresh],'bo',markersize=6,markerfacecolor=[.5,.5,.9],label='Significant')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Neuron',ylabel='T-value',xlim=[-7,nneurons+8],\n","              title='Statistical significance of NOUN-VERB in MLP activations')\n","plt.show()"],"metadata":{"id":"p-D8BmjvtIrS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["maxTneuron = np.argmax(t)\n","minTneuron = np.argmin(t)\n","maxTneuron,minTneuron"],"metadata":{"id":"tdM0E8fw5my3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C01ZYuBk5x6b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Test generalizability using a heatmap in new text"],"metadata":{"id":"U5UotMCg5mv-"}},{"cell_type":"code","source":["# https://en.wikipedia.org/wiki/Randomness\n","text = \"In common usage, randomness is the apparent or actual lack of definite pattern or predictability in information.[1][2] A random sequence of events, symbols or steps often has no order and does not follow an intelligible pattern or combination. Individual random events are, by definition, unpredictable, but if there is a known probability distribution, the frequency of different outcomes over repeated events (or 'trials') is predictable. For example, when throwing two dice, the outcome of any particular roll is unpredictable, but a sum of 7 will tend to occur twice as often as 4. In this view, randomness is not haphazardness; it is a measure of uncertainty of an outcome. Randomness applies to concepts of chance, probability, and information entropy.\"\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","\n","# get activations from forward pass\n","with torch.no_grad(): model(tokens)"],"metadata":{"id":"KXwBGCLe5mmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# min-max scale the activations\n","\n","activationMax = activations[hookName][0,:,maxTneuron]\n","activationMin = activations[hookName][0,:,minTneuron]\n","\n","activationMax = (activationMax-activationMax.min()) / (activationMax.max()-activationMax.min())\n","activationMin = (activationMin-activationMin.min()) / (activationMin.max()-activationMin.min())"],"metadata":{"id":"nFcsMHnO4B8h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7DlyT3f54B3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate letter width\n","fig,ax = plt.subplots(figsize=(10,2))\n","\n","# draw a text object and get its bounding box\n","temp_text = ax.text(0,0,'n',fontsize=12,fontfamily='monospace')\n","bbox = temp_text.get_window_extent(renderer=fig.canvas.get_renderer())\n","\n","# convert to axis coordinates\n","inv = ax.transAxes.inverted()\n","bbox_axes = inv.transform([[bbox.x0,bbox.y0], [bbox.x1,bbox.y1]])\n","en_width = bbox_axes[1,0] - bbox_axes[0,0] # bbox is [(x0,y0),(x1,y1)]\n","plt.close(fig) # close the figure"],"metadata":{"id":"Pgvy8t5C5mp_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# counter to reset position values\n","tokenCount = 0\n","\n","x_pos = 0  # starting x position (in axis coordinates)\n","y_pos = 1  # vertical center\n","\n","# setup the figure\n","fig, axs = plt.subplots(2,1,figsize=(10,6))\n","axs[0].axis('off')\n","axs[1].axis('off')\n","\n","for toki in range(len(tokens[0])):\n","\n","  # decode the token\n","  word = tokenizer.decode(tokens[0,toki])\n","\n","  # width of the word\n","  word_width = en_width*len(word)\n","\n","  # text object with background color matching the activation\n","  axs[0].text(x_pos+word_width/2, y_pos, word, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","          bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.Reds(activationMax[toki]), edgecolor='none', alpha=.8))\n","\n","  axs[1].text(x_pos+word_width/2, y_pos, word, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","          bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.Blues(activationMin[toki]), edgecolor='none', alpha=.8))\n","\n","\n","  # update the word counter and x_pos\n","  tokenCount += 1\n","  x_pos += word_width + .015 # plus a small gap\n","\n","  # end of the line; reset coordinates and counter\n","  if tokenCount>=20:\n","    y_pos -= .12\n","    x_pos = 0\n","    tokenCount = 0\n","\n","\n","plt.show()"],"metadata":{"id":"5gMylySo0CDA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-zs09g0T19bD"},"execution_count":null,"outputs":[]}]}