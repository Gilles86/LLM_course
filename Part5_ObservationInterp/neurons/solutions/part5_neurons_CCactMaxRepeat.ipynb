{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMZFcAftWnO2p/NoadxUJBa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Reproducibility of activation maximization<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"ASxL1K-LHBcD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import GPT2Model, GPT2Tokenizer\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"WSrdm1Qc5LEp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: A training function"],"metadata":{"id":"4KqWNydr5K71"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = GPT2Model.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# store a copy of the original embeddings\n","embeddings = model.wte.weight.detach().cpu()\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trainingFunction():\n","\n","  # initialize random embeddings and normalize the stds\n","  optimized_embeddings = torch.randn((1, seq_len, embeddings.shape[1]), requires_grad=True, device=device)\n","  torch.nn.init.normal_(optimized_embeddings, mean=0, std=torch.std(embeddings))\n","\n","  # create an optimizer\n","  optimizer = torch.optim.Adam([optimized_embeddings], lr=lr)\n","\n","\n","  # loop over training steps\n","  for step in range(n_steps):\n","\n","    # clear gradient\n","    optimizer.zero_grad()\n","\n","    # patch embeddings directly into the model\n","    outputs = model(inputs_embeds = optimized_embeddings,output_hidden_states=True)\n","    allActivations = outputs.hidden_states[layer_idx]\n","\n","    # extract the activations (averaged over tokens)\n","    neuron_activation = allActivations[0,:,dim_idx].mean()\n","\n","    # calculate loss and run gradient descent\n","    loss = -neuron_activation + lambda_l2 * torch.sum(optimized_embeddings**2)\n","    loss.backward()\n","    optimizer.step()\n","\n","  # output the optimized embeddings\n","  return optimized_embeddings"],"metadata":{"id":"l-JamUCN8nxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# length of the token sequence\n","seq_len = 5\n","\n","# dimension and layer to maximize\n","layer_idx = 8\n","dim_idx = 91\n","\n","n_steps = 300   # optimization steps\n","lr = .001       # learning rate\n","lambda_l2 = .01 # regularization amount"],"metadata":{"id":"TLZG_X9Q9cL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### test it\n","optimized_embeddings = trainingFunction()"],"metadata":{"id":"iYGznpMR8n0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# decode embeddings to closest tokens\n","optimized_tokens = []\n","\n","for emb in optimized_embeddings[0]:\n","\n","  # cosine similarity with embedding weights\n","  similarities = F.cosine_similarity(emb.unsqueeze(0).detach().cpu(), embeddings)\n","\n","  # find the max similarity\n","  maxtok = np.argmax(similarities)\n","  optimized_tokens.append(maxtok)\n","\n","tokenizer.decode(optimized_tokens)"],"metadata":{"id":"sOrwIW-K8n50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wKHcfz7J8n8o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Run it 10x"],"metadata":{"id":"zlknfiG78n_P"}},{"cell_type":"code","source":["# number of experiment repetitions\n","numberRepeats = 10\n","\n","# initialize a matrix for all optimized tokens\n","all_optimized_tokens = np.zeros((numberRepeats,seq_len),dtype=int)\n","\n","\n","# start the loop!\n","for runi in range(numberRepeats):\n","\n","  # call the training function\n","  optimized_embeddings = trainingFunction()\n","\n","  # decode embeddings to closest tokens\n","  optimized_tokens = []\n","\n","  for embi,emb in enumerate(optimized_embeddings[0]):\n","\n","    # cosine similarity with embedding weights\n","    similarities = F.cosine_similarity(emb.unsqueeze(0).detach().cpu(), embeddings)\n","\n","    # find the max similarity\n","    maxtok = np.argmax(similarities)\n","    all_optimized_tokens[runi,embi] = maxtok\n","\n","  # status\n","  print(f'Finished repeat {runi+1} of {numberRepeats}')\n"],"metadata":{"id":"Rb-4FdAD8oB8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_optimized_tokens"],"metadata":{"id":"5tAuhGgRlpCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unitokens,counts = np.unique(all_optimized_tokens,return_counts=True)\n","\n","print(f'{len(unitokens)}/{np.prod(all_optimized_tokens.shape)} tokens are unique.\\n')\n","\n","for t,c in zip(unitokens,counts):\n","  print(f'{c:2} optimization for token \"{tokenizer.decode([t])}\"')"],"metadata":{"id":"BrP6QHdc5K4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ry88jBUz5KzM"},"execution_count":null,"outputs":[]}]}