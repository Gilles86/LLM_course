{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPrrl+VBD92qAmfvmKZNP1+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Negation tuning in MLP neurons<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"fXOrNMhxq2Ya"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib.gridspec import GridSpec\n","\n","import statsmodels.api as sm\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","import requests\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"w-eOA7jdEGch"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import the model and implant MLP hooks"],"metadata":{"id":"jZ_MnC6QEGZz"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this time, hook the output instead of re-calculating the linear layer\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # store in the dictionary\n","    activations[f'mlp_{layer_number}'] = output.detach().cpu()\n","  return hook\n","\n","# put hooks in all layers\n","for layeri in range(len(model.transformer.h)):\n","  model.transformer.h[layeri].mlp.c_fc.register_forward_hook(implant_hook(layeri))"],"metadata":{"id":"x01-3ZnZ4VgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nneurons = model.transformer.h[0].mlp.c_fc.weight.shape[-1]\n","model.transformer.h[0].mlp.c_fc.weight.shape"],"metadata":{"id":"-Eja2yLBsJm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h-ni2ehqyemp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Get text, find negations and affirmations"],"metadata":{"id":"CqifcaigyOOF"}},{"cell_type":"code","source":["# https://gutenberg.org/ebooks/32154\n","text = requests.get('https://gutenberg.org/cache/epub/32154/pg32154.txt').text\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","num_tokens = len(tokens[0])\n","print(f'There are {num_tokens:,} tokens, of which {len(np.unique(tokens[0])):,} are unique.')"],"metadata":{"id":"zrLDgnVZxq1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# context window size (tokens before and after each target)\n","context_pre = 90\n","context_pst = 10"],"metadata":{"id":"ptjNsXJS7C8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of negation words to match exactly\n","negation_words = ['not','cannot','nor','never']\n","\n","# initialize vector\n","isNegation = np.zeros(num_tokens,dtype=int)\n","\n","# loop over all tokens\n","for ti in range(context_pre,num_tokens-context_pst):\n","\n","  # current token\n","  currtok = tokenizer.decode(tokens[0,ti]).strip().lower()\n","\n","  # token contains a 't contraction\n","  condA = (\"'t\" in currtok) or (\"n't\" in currtok)\n","\n","  # word need to match completely (c.f., not->noted, nor->enormous)\n","  condB = currtok in negation_words\n","\n","  # next token starts with a space (c.f., not->connotative)\n","  condC = tokenizer.decode(tokens[0,ti+1])[0] == ' '\n","\n","  # test\n","  if (condA or condB) and condC:\n","    isNegation[ti] = 1\n","\n","\n","# count the targets\n","numNegationTokens = sum(isNegation)"],"metadata":{"id":"P7EgksOB2Rqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["negationsIdx = np.where(isNegation)[0]\n","context_win = 15\n","\n","# examine some proper nouns\n","print(f'There are {numNegationTokens} \"negation\" tokens in the dataset.\\n')\n","for t in negationsIdx[:10]:\n","  print(f'Example {t}:\\n{tokenizer.decode(tokens[0,t-context_win:t+context_win])}\\n')"],"metadata":{"id":"YIzCv9xAxqrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all the negation tokens\n","tokenizer.decode(tokens[0,negationsIdx])"],"metadata":{"id":"nL7ElzyH-otM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jFCT1MvIHxXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of affirmation words to match exactly\n","affirmation_words = ['agree','always','allow','can','certainly','could','definitely','may','might','shall','should']\n","\n","# initialize vector\n","isAffirmation = np.zeros(num_tokens,dtype=int)\n","\n","# loop over all tokens\n","for ti in range(context_pre,num_tokens-context_pst):\n","\n","  # current token\n","  currtok = tokenizer.decode(tokens[0,ti]).strip().lower()\n","\n","  # next token can't be 'not'\n","  condA = tokenizer.decode(tokens[0,ti+1]) != ' not'\n","\n","  # word need to match completely (c.f., not->noted, nor->enormous)\n","  condB = currtok in affirmation_words\n","\n","  # next token starts with a space (c.f., not->connotative)\n","  condC = tokenizer.decode(tokens[0,ti+1])[0] == ' '\n","\n","  # test\n","  if condA and condB and condC:\n","    isAffirmation[ti] = 1\n","\n","# count the number of target tokens\n","numAffirmationTokens = sum(isAffirmation)"],"metadata":{"id":"m3k0aozuHxUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["affirmationsIdx = np.where(isAffirmation)[0]\n","\n","# examine some examples\n","print(f'There are {numAffirmationTokens} \"affirmation\" tokens in the dataset.\\n')\n","for t in affirmationsIdx[:10]:\n","  print(f'Example {t}:\\n{tokenizer.decode(tokens[0,t-context_win:t+context_win])}\\n')"],"metadata":{"id":"ejzpwwa8Ic8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fp22909mI1kg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Create batches and get activations"],"metadata":{"id":"_cn5COMB93ew"}},{"cell_type":"code","source":["# create batches\n","batch_negations = torch.zeros((numNegationTokens,context_pre+context_pst+1),dtype=torch.long)\n","batch_affirmations = torch.zeros((numAffirmationTokens,context_pre+context_pst+1),dtype=torch.long)\n","\n","\n","# negation sequences\n","for b in range(numNegationTokens):\n","  tokenLoc = negationsIdx[b]\n","  batch_negations[b,:] = tokens[0,tokenLoc-context_pre:tokenLoc+context_pst+1]\n","\n","# affirmation sequences\n","for b in range(numAffirmationTokens):\n","  tokenLoc = affirmationsIdx[b]\n","  batch_affirmations[b,:] = tokens[0,tokenLoc-context_pre:tokenLoc+context_pst+1]\n","\n","#\n","print('Shape of negations batch:',batch_negations.shape)\n","print('Shape of affirmations batch:',batch_affirmations.shape)"],"metadata":{"id":"-FMLAjEmI1hm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process the target (negation) tokens\n","with torch.no_grad():\n","  model(batch_negations.to(device))\n","\n","# copy the activations\n","negations_activations = activations.copy()\n","\n","\n","### repeat for affirmations tokens\n","with torch.no_grad():\n","  model(batch_affirmations.to(device))\n","affirmations_activations = activations.copy()"],"metadata":{"id":"KtC8DXBhxqkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(affirmations_activations.keys(),'\\n')\n","\n","affirmations_activations['mlp_5'].shape"],"metadata":{"id":"obLbw14UeX0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MCiMqALDrlL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Logistic regression in all neurons from one layer"],"metadata":{"id":"Xj3IFDcODv0O"}},{"cell_type":"code","source":["# we'll use this vector repeatedly\n","category_labels = np.hstack((np.zeros(numAffirmationTokens),np.ones(numNegationTokens)))"],"metadata":{"id":"weyicIH1LFYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MLP transformer layer\n","whichLayer2use = 13"],"metadata":{"id":"VoBfPMPDEkoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm getting the right token\n","tokenizer.decode(batch_negations[3,context_pre]), tokenizer.decode(batch_affirmations[3,context_pre])"],"metadata":{"id":"FF4v03tjTl4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize matrix to store the classifier results\n","classifierResults = np.full((nneurons,2),np.nan)\n","\n","# loop over neurons for per-neuron analysis\n","for neuroni in range(nneurons):\n","\n","  # vectorize the activations over batches\n","  targs = negations_activations[f'mlp_{whichLayer2use}'][:,context_pre,neuroni]\n","  comps = affirmations_activations[f'mlp_{whichLayer2use}'][:,context_pre,neuroni]\n","\n","  # build and run the model\n","  try:\n","    result = sm.Logit(\n","        category_labels,\n","        sm.add_constant(np.hstack((comps,targs)))\n","        ).fit(maxiter=3000,disp=0)\n","\n","    # extract the results (p-value and beta)\n","    classifierResults[neuroni,0] = result.pvalues[1]\n","    classifierResults[neuroni,1] = result.params[1]\n","  except: pass"],"metadata":{"id":"7NiErlVG8Z3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualization of model significance and sign\n","\n","# setup the figure\n","fig = plt.figure(figsize=(12,4))\n","gs = GridSpec(1,4,figure=fig)\n","\n","ax0 = fig.add_subplot(gs[:3])\n","ax1 = fig.add_subplot(gs[3])\n","\n","# find the negative and positive betas, and the supra-threshold results\n","negBetas = classifierResults[:,1]<0\n","posBetas = classifierResults[:,1]>0\n","pvalThresh = .05/nneurons # p<.05, Bonferroni-corrected\n","sigBetas = classifierResults[:,0] < pvalThresh\n","\n","\n","# positive significant betas\n","idx2plot = posBetas & sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'ro',markerfacecolor=[.7,.7,.7],label='Positive and sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log(classifierResults[idx2plot,0]),'ro',markerfacecolor=[.7,.7,.7,.5])\n","\n","# positive non-significant betas\n","idx2plot = posBetas & ~sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'rx',markersize=3,label='Positive and non-sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log(classifierResults[idx2plot,0]),'rx',markersize=3)\n","\n","# negative significant betas\n","idx2plot = negBetas & sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'go',markerfacecolor=[.7,.7,.7],label='Negative and sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log(classifierResults[idx2plot,0]),'go',markerfacecolor=[.7,.7,.7,.5])\n","\n","# negative non-significant betas\n","idx2plot = negBetas & ~sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'gx',markersize=3,label='Negative and non-sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log(classifierResults[idx2plot,0]),'gx',markersize=3)\n","\n","ax0.set(ylabel='Beta coefficient',xlabel='Neuron index',xlim=[-10,nneurons+9],\n","              title='Statistical parameters of negation-term classification')\n","ax0.legend(fontsize=8)\n","\n","\n","ax1.axhline(-np.log(pvalThresh),linestyle='--',color='b',label='Significance threshold')\n","ax1.set(xlabel='Beta coeff',ylabel='-log(p)',title='Betas by p-values')\n","ax1.legend(fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"K3BPO_j-_MwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the neuron with best classification\n","maxBeta = np.max(classifierResults[sigBetas,1])\n","maxBetaNeuron = np.where(classifierResults[:,1]==maxBeta)[0][0]\n","maxBetaNeuron"],"metadata":{"id":"h5_rRiCpEVBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wW_QcMhGv6PP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show the prediction probabilities for the max neuron\n","\n","# need to re-run the model for that neuron\n","targs = negations_activations[f'mlp_{whichLayer2use}'][:,context_pre,maxBetaNeuron]\n","comps = affirmations_activations[f'mlp_{whichLayer2use}'][:,context_pre,maxBetaNeuron]\n","result = sm.Logit(category_labels,sm.add_constant(np.hstack((comps,targs)))).fit(disp=0)\n","\n","# per-token accuracy\n","accuracy = (result.predict()>.5)==category_labels"],"metadata":{"id":"r4r0O_Syv6Jq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualization\n","plt.figure(figsize=(10,4))\n","\n","plt.plot(category_labels,'rx')\n","plt.plot(result.predict(),'ko',markerfacecolor=[.7,.9,.7,.5],markersize=8)\n","\n","plt.axvline(numAffirmationTokens+.5,linestyle='--',color='k')\n","plt.axhline(.5,linestyle='--',color='k')\n","\n","plt.gca().set(xlabel='Data sample',ylabel='Probability or category',\n","              xlim=[-3,numAffirmationTokens+numNegationTokens+2],title=f'Accuracy from neuron #{maxBetaNeuron} = {100*accuracy.mean():.2f}%')\n","\n","plt.show()"],"metadata":{"id":"AxfLic93RluM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v_NdfrwEOzBY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Text heatmap of “best” neuron’s activations"],"metadata":{"id":"CQH4jSJOEygR"}},{"cell_type":"code","source":["# scale the activity for colormapping\n","negActs = negations_activations[f'mlp_{whichLayer2use}'][:,:,maxBetaNeuron]\n","negActsNorm = (negActs - negActs.min()) / (negActs.max()-negActs.min())"],"metadata":{"id":"iaf_3HXCH0vk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all of this code is copied from several previous code files\n","fig,ax = plt.subplots(figsize=(10,2))\n","temp_text = ax.text(0,0,'n',fontsize=12,fontfamily='monospace')\n","bbox = temp_text.get_window_extent(renderer=fig.canvas.get_renderer())\n","inv = ax.transAxes.inverted()\n","bbox_axes = inv.transform([[bbox.x0,bbox.y0], [bbox.x1,bbox.y1]])\n","en_width = bbox_axes[1,0] - bbox_axes[0,0] # bbox is [(x0,y0),(x1,y1)]\n","plt.close(fig)"],"metadata":{"id":"pHd_EFsqqBsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_pos = 0  # starting x position (in axis coordinates)\n","y_pos = 1  # vertical center\n","\n","fig, ax = plt.subplots(figsize=(10,2))\n","ax.axis('off')\n","\n","\n","# loop over rows in the batch\n","for batchi in range(20): # just the first 20 sequences\n","\n","  for toki in range(context_pre-5,batch_negations.shape[1]):\n","\n","    # text of this token\n","    toktext = tokenizer.decode([batch_negations[batchi,toki]])\n","\n","    # width of the token\n","    token_width = en_width*len(toktext)\n","\n","    # text object with background color matching the activation\n","    ax.text(x_pos+token_width/2, y_pos, toktext, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","            bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.Reds(negActsNorm[batchi,toki]**2), edgecolor='none', alpha=.8))\n","\n","    # update the token counter and x_pos\n","    x_pos += token_width + .01 # plus a small gap\n","\n","  # end of the line; reset coordinates and counter\n","  y_pos -= .17\n","  x_pos = 0\n","\n","plt.show()"],"metadata":{"id":"x9vSYIo1qBou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3j75XeCrxqyg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 6: Laminar profile of classification"],"metadata":{"id":"PfH-c__nTdmy"}},{"cell_type":"code","source":["# initialize matrix to store the classifier results\n","pvalues  = np.ones((model.config.n_layer,nneurons)) # initialize to 1's to ignore in subsequent mask\n","betas    = np.zeros((model.config.n_layer,nneurons))\n","accuracy = np.zeros((model.config.n_layer,nneurons))\n","\n","\n","# loop over layers\n","for layeri in range(model.config.n_layer):\n","\n","  # loop over neurons for per-neuron analysis\n","  for neuroni in range(nneurons):\n","    # vectorize the activations over batches\n","    targs = negations_activations[f'mlp_{layeri}'][:,context_pre,neuroni]\n","    comps = affirmations_activations[f'mlp_{layeri}'][:,context_pre,neuroni]\n","\n","    # build and run the model\n","    try: # sometimes crashes for linear-algebra reasons\n","      result = sm.Logit(category_labels,sm.add_constant(np.hstack((comps,targs)))).fit(maxiter=3000,disp=0)\n","\n","      # extract the results (p-value, beta, and accuracy)\n","      pvalues[layeri,neuroni]  = result.pvalues[1]\n","      betas[layeri,neuroni]    = result.params[1]\n","      accuracy[layeri,neuroni] = 100*((result.predict()>.5)==category_labels).mean()\n","\n","    except: pass\n","\n","  print(f'Finished layer {layeri+1:2}/{model.config.n_layer}')"],"metadata":{"id":"HVLbGTqNTdkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create two masks\n","pvalue_mask = pvalues<.05/nneurons\n","posbet_mask = betas>0\n","\n","# get accuracy only from masked neurons\n","# gratuitously confusingly, np.ma.masked_where() actually keeps the False values, and masks *out* True values\n","masked_accuracy = np.ma.masked_where(~(pvalue_mask & posbet_mask),accuracy)\n","\n","# make the plot\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","axs[0].plot(100*np.mean(pvalue_mask,axis=1),'kH',markerfacecolor=[.7,.7,.7],markersize=9)\n","axs[0].set(xlabel='Layer',ylabel='Percent significant tests (%)',title='Laminar profile of significance')\n","\n","axs[1].plot(np.mean(accuracy,axis=1),'ko',markerfacecolor=[.9,.7,.7],markersize=9,label='All tests')\n","axs[1].plot(np.mean(masked_accuracy,axis=1),'ks',markerfacecolor=[.7,.9,.7],markersize=9,label='Only sig. $\\\\beta$s>0')\n","axs[1].legend()\n","\n","axs[1].set(xlabel='Layer',ylabel='Prediction accuracy (%)',title='Laminar profile of prediction accuracy')\n","plt.show()"],"metadata":{"id":"cen2pJQPTdhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K-hrgYR3XBws"},"execution_count":null,"outputs":[]}]}