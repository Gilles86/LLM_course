{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNlM8ksP49N4Eigj+sWyQFG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating layers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>The Logit Lens<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"4oPXepmbV8_D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5FgvNT00WEAW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text and activations"],"metadata":{"id":"SLZIWpxWjoX0"}},{"cell_type":"code","source":["text = 'The way you do anything is the way you do everything'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","numTokens = len(tokens[0])\n","\n","with torch.no_grad():\n","  output = model(tokens,output_hidden_states=True)"],"metadata":{"id":"cPawkvwDWD9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(output.hidden_states), output.hidden_states[3].shape"],"metadata":{"id":"P5uSuUfBWD6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4mkQJ8lnem5A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Illustration of logit-lens procedure and outcome"],"metadata":{"id":"E1WNaki_enY8"}},{"cell_type":"code","source":["# reference:\n","# https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"],"metadata":{"id":"B2DURtllkH9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the logits and softmax them\n","\n","# extract the activations from one layer\n","activations = output.hidden_states[3][0]\n","\n","# and the unembedding matrix (tied to the initial embedding in GPT2)\n","unembedding = model.lm_head.weight.detach()\n","\n","# calculate the raw logits\n","logits = activations @ unembedding.t()\n","\n","# check the shape\n","logits.shape"],"metadata":{"id":"R7KjYbIkcoYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# softmax and plot\n","lsm_outs = F.log_softmax(logits,dim=-1)\n","\n","# max-softmax is the next prediction\n","predictedToken = np.argmax(lsm_outs[3,:])\n","print(f'Next token in the text is \"{tokenizer.decode(tokens[0,4])}\"')\n","print(f'Predicted next token at this layer is \"{tokenizer.decode(predictedToken)}\"')\n","\n","# show softmax for one token\n","plt.figure(figsize=(8,4))\n","plt.plot(predictedToken,lsm_outs[3,predictedToken],'ro',markersize=8)\n","plt.plot(lsm_outs[3,:],'k.',alpha=.3)\n","plt.gca().set(xlabel='Token index',ylabel='Log-softmax prob',xlim=[-10,tokenizer.vocab_size+9],\n","              title=f'Log-softmax logits for the token following \"{tokenizer.decode(tokens[0,3])}\"')\n","plt.show()"],"metadata":{"id":"s4YGy1I6WD36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mntkgy8zWD08"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Logit-lens over all layers"],"metadata":{"id":"G_lw1d3oet05"}},{"cell_type":"code","source":["# initialize an empty list\n","all_token_predictions = []\n","\n","# initialize softmax probs\n","softmax_probs = np.zeros((model.config.n_layer,numTokens))\n","\n","\n","for layeri,acts in enumerate(output.hidden_states[1:]): # [1:] to skip embedding layer\n","\n","  # calculate the logits\n","  logits = acts[0] @ unembedding.t()\n","\n","  # find predicted next tokens\n","  # note: we don't need to softmax b/c it doesn't affect argmax\n","  predictedNextToks = logits.argmax(dim=-1)\n","\n","  # but here take softmax for subsequent visualization\n","  sm = F.softmax(logits,-1) # [tokens, vocab]\n","  softmax_probs[layeri,:] = [sm[i,pi].item() for i,pi in enumerate(predictedNextToks)]\n","\n","  # get the text predictions for all tokens in the text\n","  all_token_predictions.append([tokenizer.decode([i.item()]) for i in predictedNextToks])\n"],"metadata":{"id":"FrHFYABQWDyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_token_predictions"],"metadata":{"id":"JoZBXc9_n341"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Original text:\\n', text, '\\n')\n","print('Predictions at first transformer block:\\n', ''.join(all_token_predictions[0]), '\\n')\n","print('Predictions at final transformer block:\\n', ''.join(all_token_predictions[-1]))"],"metadata":{"id":"-PHo0gG5WDva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Hrjwd3n2hUwV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization"],"metadata":{"id":"hhnmDbihhUtk"}},{"cell_type":"code","source":["fig,ax = plt.subplots(1,figsize=(10,5))\n","\n","# original text (separated into a list of decoded tokens)\n","target = [tokenizer.decode(t) for t in tokens[0]]\n","\n","# loop over layers\n","for layeri,layerToks in enumerate(all_token_predictions[:]):\n","\n","  # y-axis coordinate for this layer\n","  yCoord = 1-layeri/model.config.n_layer\n","\n","  # print the layer number in the left margin\n","  ax.text(-.07,yCoord,f'Layer {layeri+1}:',ha='right')\n","\n","  # loop over the predicted tokens in this layer\n","  for xi,tok in enumerate(layerToks):\n","    ax.text(xi/numTokens,yCoord,tok,ha='center',\n","            bbox=dict(boxstyle='round,pad=0.3', facecolor=mpl.cm.Reds(softmax_probs[layeri,xi]), edgecolor='none',alpha=.4))\n","\n","ax.axis('off')\n","\n","# finally, draw the target tokens at the bottom\n","ax.text(-.07,yCoord-.1,f'Target:',ha='right',fontweight='bold')\n","for xi,tok in enumerate(target[1:]):\n","  ax.text(xi/numTokens,yCoord-.1,tok,ha='center',fontweight='bold')\n","\n","plt.show()"],"metadata":{"id":"oaH9ZTWKhUqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9WmZDvrPcKvU"},"execution_count":null,"outputs":[]}]}