{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPWiBOxSzeRqp21id6gn+6T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating layers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>\"Effective dimensionality\" analysis with PCA<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"tL45trWPK0av"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib import gridspec\n","\n","import requests\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-xl')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n","\n","model.to(device)\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"It_OngQWLRZe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Alice in Wonderland\n","text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n","allTokens = tokenizer.encode(text,return_tensors='pt')\n","tokens = allTokens[:,10000:11000]\n","\n","print(tokenizer.decode(tokens[0]))"],"metadata":{"id":"1QCtJSG8LRVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokensShuffle = tokens[0,torch.randperm(len(tokens[0]))].unsqueeze(0)\n","print(tokenizer.decode(tokensShuffle[0]))"],"metadata":{"id":"_KEpro9XLRDg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tjJsEaWGOKuI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Push the data and get the activations"],"metadata":{"id":"JgE1LyrTOKri"}},{"cell_type":"code","source":["# push through the model (~3 mins with gpt2-xl on CPU, or <1s on GPU, lol)\n","with torch.no_grad():\n","  outputs_real = model(tokens.to(device),output_hidden_states=True)\n","  outputs_shuf = model(tokensShuffle.to(device),output_hidden_states=True)\n","\n","outputs_real.hidden_states[0].shape"],"metadata":{"id":"DfiAwJNxLRSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numHidden = len(outputs_real.hidden_states)\n","numHidden"],"metadata":{"id":"sz29aI05ldS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4AnMiN9kmdTV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## calculate dimensionality metrics (~1 min)\n","\n","# initialize\n","cumVarExplained = np.zeros((numHidden,outputs_real.hidden_states[4].shape[1],2))\n","effectiveCompCount = np.zeros((numHidden,2),dtype=int)\n","\n","\n","# loop over layers\n","for layeri in range(numHidden):\n","\n","  # extract all the activations from this layer (assuming no batches!)\n","  acts = outputs_real.hidden_states[layeri].squeeze().cpu().numpy()\n","\n","  # mean-center the activations\n","  acts -= acts.mean(axis=0,keepdims=True)\n","\n","  # get singular values\n","  s = np.linalg.svd(acts)[1]\n","\n","  # percent explained (cumulative)\n","  pctExplained = 100 * s**2 / np.sum(s**2)\n","  cumVarExplained[layeri,:,0] = np.cumsum(pctExplained)\n","\n","  # count the components until 95% variance is explained\n","  effectiveCompCount[layeri,0] = np.where(cumVarExplained[layeri,:,0]>95)[0][0]+1\n","\n","\n","\n","  ### repeat for shuffled tokens\n","  acts = outputs_shuf.hidden_states[layeri].squeeze().cpu().numpy()\n","  acts -= acts.mean(axis=0,keepdims=True)\n","  s = np.linalg.svd(acts)[1] # get singular values\n","  pctExplained = 100 * s**2 / np.sum(s**2) # percent explained\n","  cumVarExplained[layeri,:,1] = np.cumsum(pctExplained) # cumulative\n","  effectiveCompCount[layeri,1] = np.where(cumVarExplained[layeri,:,1]>95)[0][0]+1\n"],"metadata":{"id":"R8VSmM2OLRPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The plot in the slides...\n","\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","acts = outputs_real.hidden_states[5].squeeze().cpu().numpy()\n","acts -= acts.mean(axis=0,keepdims=True)\n","s = np.linalg.svd(acts)[1] # get singular values\n","pctExplained = 100 * s**2 / np.sum(s**2) # percent explained\n","\n","axs[0].plot(pctExplained,'ks-',markerfacecolor=[.7,.9,.7])\n","axs[0].set(xlim=[-1,100],xlabel='Component number',ylabel='Percent variance explained')\n","\n","axs[1].plot(np.cumsum(pctExplained),'ks-',markerfacecolor=[.7,.7,.9])\n","axs[1].axhline(80,linestyle='--',color='gray')\n","axs[1].axvline(53,linestyle='--',color='gray')\n","axs[1].set(xlim=[-1,100],xlabel='Component number',ylabel='Cumulative % variance explained')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"B59BVelJg-Rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hn-GlZxHIwLs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization"],"metadata":{"id":"q5rD5sniZXJg"}},{"cell_type":"code","source":["# setup the figure and axes\n","fig = plt.figure(figsize=(10,7))\n","gs = gridspec.GridSpec(2,2)\n","axs1 = fig.add_subplot(gs[0,0])\n","axs2 = fig.add_subplot(gs[0,1])\n","axs3 = fig.add_subplot(gs[1,:])\n","\n","# normalization function for mapping layer index onto color\n","norm = mpl.colors.Normalize(vmin=0,vmax=numHidden)\n","\n","\n","# plt the cumulative variance explained\n","for layeri in range(numHidden):\n","  axs1.plot(cumVarExplained[layeri,:,0],color=mpl.cm.plasma(norm(layeri)))\n","  axs2.plot(cumVarExplained[layeri,:,1],color=mpl.cm.plasma(norm(layeri)))\n","\n","\n","# add colorbars\n","sm = mpl.cm.ScalarMappable(cmap=mpl.cm.plasma,norm=norm)\n","cbar = plt.colorbar(sm,ax=axs1)\n","cbar.set_label(r'Hidden layer')\n","cbar = plt.colorbar(sm,ax=axs2)\n","cbar.set_label(r'Hidden layer')\n","\n","# make it look nicer\n","axs1.set(xlabel='Component number',ylabel='% explained (cumulative)',ylim=[50,100.5],xlim=[-2,500],title='(Real) variance explained')\n","axs2.set(xlabel='Component number',ylabel='% explained (cumulative)',ylim=[50,100.5],xlim=[-2,500],title='(Shuffled) variance explained')\n","\n","\n","\n","## plot the \"effective subspace dimensionality\" of each layer\n","axs3.plot(effectiveCompCount[:,1],'ks',markerfacecolor=[.9,.7,.7,.5],markersize=10,label='Shuffled tokens')\n","axs3.plot(effectiveCompCount[:,0],'ko',markerfacecolor=[.7,.9,.7],markersize=10,label='Real tokens')\n","axs3.legend()\n","axs3.set(xlabel='Hidden layer',ylabel='Numer of dimensions',title='\"Effective dimensionality\"',\n","             xlim=[-1,numHidden])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"N331K6QhSMbN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CPtUSmSKLQ_y"},"execution_count":null,"outputs":[]}]}