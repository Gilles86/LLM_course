{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPUW8Ux4HsNJL2Zzr89X+Xg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating layers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Logit Lens in BERT<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"4oPXepmbV8_D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib.gridspec import GridSpec\n","\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"WivjQFTfVZo7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Reminder of masked-token predictions in BERT"],"metadata":{"id":"3CSzp20AVZmK"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForMaskedLM\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# getting predicts in BERT (note: target word for [MASK] is \"way\")\n","text = 'The way you do anything is the [MASK] you do everything.'\n","tokens = tokenizer.encode(text, return_tensors='pt')\n","\n","for t in tokens[0]:\n","  print(f'Token {t:5} is \"{tokenizer.decode(t)}\"')\n","\n","# index of the [MASK] token\n","mask_token_idx = torch.where(tokens == tokenizer.mask_token_id)[1]\n","print(f'\\nMask token is {tokenizer.mask_token_id} and is in index {mask_token_idx.item()}')"],"metadata":{"id":"8y8iC98iCgG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass\n","with torch.no_grad():\n","  outputs = model(tokens.to(device))\n","\n","# logits for [MASK] token\n","mask_logits = outputs.logits[0,mask_token_idx,:].squeeze().cpu()\n","\n","# top predicted token\n","top_token_idx = torch.argmax(mask_logits, dim=-1)\n","\n","# and plot\n","plt.figure(figsize=(10,4))\n","\n","plt.plot(top_token_idx,outputs.logits[0,mask_token_idx,top_token_idx].cpu(),'go',markersize=9)\n","plt.plot(mask_logits,'k.',alpha=.3)\n","plt.gca().set(xlim=[-5,tokenizer.vocab_size+4],xlabel='Token index',ylabel='BERT output logits',\n","              title=f'Predicted token is \"{tokenizer.decode(top_token_idx)}\"')\n","plt.show()"],"metadata":{"id":"X0SC7q0pV_lb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3bSbj7KbcNSG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Calculate and visualize z-scores"],"metadata":{"id":"MhOJs-pvcNPs"}},{"cell_type":"code","source":["# calculate z\n","zLogits = (mask_logits-mask_logits.mean()) / mask_logits.std()\n","\n","# and plot\n","fig = plt.figure(figsize=(13,4))\n","gs = GridSpec(1,3,figure=fig)\n","\n","ax0 = fig.add_subplot(gs[0])\n","ax1 = fig.add_subplot(gs[1:])\n","\n","# histogram of final output layer logits\n","ax0.hist(mask_logits.numpy(),bins=80,edgecolor='k',facecolor='gray',label='All tokens')\n","ax0.axvline(mask_logits[top_token_idx],linestyle='--',color='g',label='Predicted token')\n","ax0.set(xlabel='Logit value',ylabel='Count',title='Histogram of logits')\n","ax0.legend(fontsize=9)\n","\n","# scatter plot of logits\n","ax1.plot(top_token_idx,zLogits[top_token_idx],'go',markersize=9)\n","ax1.plot(zLogits,'k.',alpha=.3)\n","ax1.set(xlim=[-5,tokenizer.vocab_size+4],xlabel='Token index',ylabel='Z-scored output logits',\n","              title=f'Predicted token is \"{tokenizer.decode(top_token_idx)}\"')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"hmfyEzwAcPjU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-fdIqAOPbdqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Z-score final output logits for each token"],"metadata":{"id":"lYDF08oFZoSW"}},{"cell_type":"code","source":["# full sentence\n","text = 'the way you do anything is the way you do everything'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","\n","# initialize\n","mask_zscores = np.zeros(len(tokens[0]))\n","predicted_text = []\n","\n","print(f'[MASK] token is {tokenizer.mask_token_id}\\n')\n","\n","# loop over tokens, replace with [MASK], and get logits\n","for idx,tok in enumerate(tokens[0]):\n","\n","  # make a copy and replace a token with mask\n","  masked_tokens = tokens.clone().to(device)\n","  masked_tokens[0,idx] = tokenizer.mask_token_id\n","  # confirmation (convert to list for visualizability):\n","  print(masked_tokens[0].tolist())\n","\n","  # forward pass through the model\n","  with torch.no_grad(): outputs = model(masked_tokens)\n","\n","  # get logits for the masked position\n","  mask_logits = outputs.logits[0,idx,:].squeeze().cpu()\n","\n","  # get the max masked prediction and its z-score\n","  predicted_token = torch.argmax(mask_logits,dim=-1)\n","  predicted_text.append( tokenizer.decode(predicted_token) )\n","  mask_zscores[idx] = (mask_logits[predicted_token]-mask_logits.mean()) / mask_logits.std()"],"metadata":{"id":"enGyLiSFC3k7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print a table of results\n","\n","print('    TARGET   |  PREDICTED |  Z-SCORE')\n","print('-'*38)\n","for idx in range(len(tokens[0])):\n","  print(f'{tokenizer.decode(tokens[0,idx]):>10}   |   {predicted_text[idx]:^6}   |   {mask_zscores[idx]:5.2f}')"],"metadata":{"id":"_ZRiTD9xhMT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HmEzFTgZtxFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Exercise 4: Proper internal lensing in BERT"],"metadata":{"id":"Lzwu3n_jtyjV"}},{"cell_type":"code","source":["# reminder\n","model"],"metadata":{"id":"gbPdOl1xhMRd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convenience variable (+1 for initial embedding layer)\n","numlayers = len(model.bert.encoder.layer) + 1"],"metadata":{"id":"zxxgHP0Swtwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# getting predicts in BERT (note: target word for [MASK] is \"way\")\n","text_withMask = 'The way you do anything is the [MASK] you do everything.'\n","masked_tokens = tokenizer.encode(text_withMask, return_tensors='pt').to(device)"],"metadata":{"id":"5bNAKy_vtygx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad(): outputs = model(masked_tokens,output_hidden_states=True)\n","print(f'There are {numlayers} layers')\n","outputs.hidden_states[4].shape"],"metadata":{"id":"jN9sygkOhI9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### THE WRONG WAY\n","\n","# unembedding matrix\n","unembeddingT = model.cls.predictions.decoder.weight.detach().T\n","\n","# activations from mask position in one layer\n","acts = outputs.hidden_states[-3][0,8,:].detach()\n","\n","# same as with GPT?\n","mask_logitsWrong = acts @ unembeddingT\n","\n","mask_logitsWrong = mask_logitsWrong.cpu()"],"metadata":{"id":"kGSGV7o1uLgW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### THE CORRECT WAY\n","mask_logitsCorrect = model.cls.predictions(acts).detach().cpu()"],"metadata":{"id":"3QI4KTOguhB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predictions\n","_,axs = plt.subplots(1,2,figsize=(14,4))\n","\n","# the WRONG way\n","top_token_idx_wrong = torch.argmax(mask_logitsWrong,dim=-1).cpu()\n","\n","axs[0].plot(top_token_idx_wrong,mask_logitsWrong[top_token_idx_wrong],'go',markersize=9)\n","axs[0].plot(mask_logitsWrong,'k.',alpha=.3)\n","axs[0].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Z-scored output logits',\n","              title=f'(WRONG WAY) Predicted token is \"{tokenizer.decode(top_token_idx_wrong)}\"')\n","\n","\n","# the CORRECT way\n","top_token_idx_correct = torch.argmax(mask_logitsCorrect,dim=-1)\n","\n","axs[1].plot(top_token_idx_correct,mask_logitsCorrect[top_token_idx_correct],'go',markersize=9)\n","axs[1].plot(mask_logitsCorrect,'k.',alpha=.3)\n","axs[1].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Z-scored output logits',\n","              title=f'(CORRECT WAY) Predicted token is \"{tokenizer.decode(top_token_idx_correct)}\"')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"k_JyAO2Qug_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BA26PFNvtxCU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Repeat for all layers (logit-lens)"],"metadata":{"id":"tz7zMAC8hMOn"}},{"cell_type":"code","source":["# initialize\n","mask_zscores = np.zeros((numlayers,len(tokens[0])))\n","predictedTokens = np.zeros((numlayers,len(tokens[0])),dtype=int)\n","\n","# loop over tokens, replace with [MASK], and get logits\n","for idx,tok in enumerate(tokens[0]):\n","\n","  # make a copy and replace a token with mask\n","  masked_tokens = tokens.clone()\n","  masked_tokens[0,idx] = tokenizer.mask_token_id\n","  # confirmation:\n","  #print(masked_tokens[0])\n","\n","  # forward pass through the model (with hidden states exported!)\n","  with torch.no_grad():\n","    outputs = model(masked_tokens.to(device),output_hidden_states=True)\n","\n","\n","  ### loop over layers\n","  for layeri in range(numlayers):\n","\n","    # get internal logits for the masked position\n","    acts = outputs.hidden_states[layeri][0,idx,:]\n","\n","    # logit lens\n","    mask_logits = model.cls.predictions(acts).cpu()\n","\n","    # get the masked prediction and its z-score\n","    predicted_token = torch.argmax(mask_logits,dim=-1)\n","    predictedTokens[layeri,idx] = predicted_token\n","    mask_zscores[layeri,idx] = (mask_logits[predicted_token]-mask_logits.mean()) / mask_logits.std()"],"metadata":{"id":"UEVExLx-gq4g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Original text:\\n', text, '\\n')\n","print('Predictions at first transformer block:\\n', tokenizer.decode(predictedTokens[1,1:-1]), '\\n')\n","print('Predictions at final transformer block:\\n', tokenizer.decode(predictedTokens[-1,1:-1]))"],"metadata":{"id":"-PHo0gG5WDva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lines visualization\n","plt.figure(figsize=(12,4))\n","\n","for layeri in range(numlayers):\n","  plt.plot(range(len(tokens[0])-2),mask_zscores[layeri,1:-1].T,'o-',color=mpl.cm.plasma((layeri+1)/numlayers),label=f'L{layeri}')\n","\n","plt.gca().set(xticks=range(len(tokens[0,1:-1])),xticklabels=[tokenizer.decode(t) for t in tokens[0,1:-1]],\n","              ylabel='Z-score',title='Z-scores of predicted tokens')\n","\n","plt.legend(fontsize=11.3,bbox_to_anchor=(1,1.03),ncol=2)\n","plt.show()"],"metadata":{"id":"ZiifQLErgq1w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and as an image\n","plt.figure(figsize=(13,5))\n","plt.imshow(mask_zscores[:,1:-1],aspect='auto',origin='lower',cmap='plasma')\n","plt.colorbar(pad=.01,label='Z-score')\n","\n","plt.gca().set(xlabel='Target word',ylabel='Layer',\n","              xticks=range(len(tokens[0,1:-1])),xticklabels=[tokenizer.decode(t) for t in tokens[0,1:-1]])\n","plt.show()"],"metadata":{"id":"Hrjwd3n2hUwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v8MwGcEd7FFv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 6: Logit Lens text heatmap"],"metadata":{"id":"hhnmDbihhUtk"}},{"cell_type":"code","source":["# scale z-scores\n","scaled_zscores = (mask_zscores-mask_zscores.min()) / (mask_zscores.max()-mask_zscores.min())\n","np.sort(scaled_zscores.flatten())"],"metadata":{"id":"I3Kv6kUSzSUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(1,figsize=(13,10))\n","\n","# original text (separated into a list of decoded tokens)\n","target = [tokenizer.decode(t) for t in tokens[0]]\n","numTokens = len(target)\n","\n","# loop over layers\n","for layeri in range(numlayers):\n","\n","  # y-axis coordinate for this layer\n","  yCoord = 1-layeri/numlayers\n","\n","  # print the layer number in the left margin\n","  ax.text(-.1,yCoord,f'Layer {layeri}:',ha='right')\n","\n","  # loop over the predicted tokens in this layer\n","  for xi,tok in enumerate(predictedTokens[layeri]):\n","    ax.text(xi/numTokens,yCoord,tokenizer.decode(tok),ha='center',\n","            bbox=dict(boxstyle='round,pad=0.3', facecolor=mpl.cm.Reds(scaled_zscores[layeri,xi]), edgecolor='none',alpha=.4))\n","\n","ax.axis('off')\n","\n","# finally, draw the target tokens at the bottom\n","ax.text(-.1,yCoord-.05,f'Target:',ha='right',fontweight='bold')\n","for xi,tok in enumerate(target):\n","  ax.text(xi/numTokens,yCoord-.05,tok,ha='center',fontsize=12,fontweight='bold')\n","\n","plt.show()"],"metadata":{"id":"oaH9ZTWKhUqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9WmZDvrPcKvU"},"execution_count":null,"outputs":[]}]}