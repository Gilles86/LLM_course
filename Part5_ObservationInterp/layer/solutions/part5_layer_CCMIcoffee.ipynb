{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNebFs/YLrg3uGzey5WjoPa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating layers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Attention to coffee: MI and token distances<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"8UrqMO28-9ZL"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","# stats library for kendall correlation (when one variable is ordinal [sorted categorical])\n","import scipy.stats as stats\n","from statsmodels.stats.multitest import fdrcorrection\n","\n","from sklearn.feature_selection import mutual_info_regression\n","\n","import torch\n","\n","# vector matplotlib\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BJR9NR3dr4pZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Model, hooks, tokens, & activations"],"metadata":{"id":"ZRyQ1mlor4mm"}},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2-xl')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# variable for the number of transformer layers\n","nLayers = len(gpt2.transformer.h)\n","\n","gpt2.eval()"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook function to store attention vectors (see also part5_neuron_hookVsHiddenStates.ipynb)\n","activations = {}\n","\n","def implant_hook_attn(layer_number):\n","  def hook(module, input, output):\n","    activations[f'att_proj_{layer_number}'] = output.detach().numpy()\n","  return hook\n","\n","# and mlp layers\n","def implant_hook_mlp(layer_number):\n","  def hook(module, input, output):\n","    activations[f'mlp_proj_{layer_number}'] = output.detach().numpy()\n","  return hook\n","\n","# implant hooks\n","for layeri in range(nLayers):\n","  gpt2.transformer.h[layeri].attn.c_proj.register_forward_hook(implant_hook_attn(layeri))\n","  gpt2.transformer.h[layeri].mlp.c_proj.register_forward_hook(implant_hook_mlp(layeri))"],"metadata":{"id":"KcOSag-kHWH-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MIpgFqvuHWFH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from https://en.wikipedia.org/wiki/Turkish_coffee\n","text = 'Turkish coffee is very finely ground coffee brewed by boiling. Any coffee bean may be used; arabica varieties are considered best, but robusta or a blend is also used.[1] The coffee grounds are left in the coffee when served.[2][3] The coffee may be ground at home in a manual grinder made for the very fine grind, ground to order by coffee merchants in most parts of the world, or bought ready-ground from many shops.'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","print(f'There are {len(tokens[0])} tokens, {len(set(tokens[0].tolist()))} of which are unique.')"],"metadata":{"id":"L_5xIvYA--eH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find all the \"coffee\" target indices\n","target = ' coffee'\n","target_idxs = torch.where(tokens==tokenizer.encode(target)[0])[1]\n","target_idxs"],"metadata":{"id":"NsXA8Z9g6ezn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass to get activations\n","with torch.no_grad():\n","  output = gpt2(tokens,output_hidden_states=True) # hs not analyzed, but extracted for inspiration ;)"],"metadata":{"id":"hp-mwh6u--MZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations['att_proj_3'].shape"],"metadata":{"id":"7m1kp-CvIUnx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K-a-LsggLofq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Trimmed-MI function"],"metadata":{"id":"cvKA1ylWVFfs"}},{"cell_type":"code","source":["# a function for mutual information\n","def mutInfo(x,y,outlierThresh=0):\n","\n","  # remove outliers based on a z-score threshold\n","  if outlierThresh>0:\n","\n","    # z-standardize the variables\n","    zx = (x-x.mean()) / x.std(ddof=1)\n","    zy = (y-y.mean()) / y.std(ddof=1)\n","\n","    # remove data points based on threshold exceedances\n","    outlier = (abs(zx)>outlierThresh) | (abs(zy)>outlierThresh)\n","    x = x[~outlier]\n","    y = y[~outlier]\n","\n","\n","  # histogram and convert to proportion (estimate of probability)\n","  Z  = np.histogram2d(x,y,bins=15)[0]\n","  pZ = Z / Z.sum()\n","  px = pZ.sum(axis=1)\n","  py = pZ.sum(axis=0)\n","\n","  # calculate entropy\n","  eps = 1e-12\n","  Hx = -np.sum( px * np.log2(px+eps) )\n","  Hy = -np.sum( py * np.log2(py+eps) )\n","  HZ = -np.sum( pZ * np.log2(pZ+eps) )\n","\n","  return Hx+Hy - HZ"],"metadata":{"id":"GAptRmI0A1GZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract some data\n","x = activations['att_proj_3'][0,target_idxs[0],:]\n","y = activations['att_proj_3'][0,target_idxs[1],:]\n","\n","# z-transform\n","zx = (x-x.mean()) / x.std(ddof=1)\n","zy = (y-y.mean()) / y.std(ddof=1)\n","\n","# identify outliers\n","threshold = 4\n","outlier = (abs(zx)>threshold) | (abs(zy)>threshold)\n","\n","# mutual information with and without\n","miAll  = mutInfo(x,y)\n","miTrim = mutInfo(x,y,outlierThresh=threshold) # trimmed\n","\n","miSkAll = mutual_info_regression(x.reshape(-1,1),y)[0]\n","miSkTrim = mutual_info_regression(x[~outlier].reshape(-1,1),y[~outlier])[0]\n","\n","# plot\n","plt.figure(figsize=(6,5))\n","plt.plot(x[~outlier],y[~outlier],'ko',markerfacecolor=[.7,.9,.7,.3],label='Trimmed')\n","plt.plot(x[outlier],y[outlier],'ks',markerfacecolor=[.9,.7,.7],label='Outliers')\n","plt.gca().set(xlabel='Target word 1',ylabel='Target word 2')\n","\n","plt.title(f'MI of all data: {miAll:.2f}\\nMI of trimmed data: {miTrim:.2f}')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"PXFREa6Qb1Uz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# comparisons\n","print(f'Manual, all data : {miAll:.3f}')\n","print(f'Manual, trimmed  : {miTrim:.3f}')\n","print(f'Difference:      : {miAll-miTrim:.3f}\\n')\n","\n","print(f'Sklearn, all data: {miSkAll:.3f}')\n","print(f'Sklearn, trimmed : {miSkTrim:.3f}')\n","print(f'Difference:      : {miSkAll-miSkTrim:.3f}')"],"metadata":{"id":"tf-q9D8Hdx4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Qi7NEb57bsuA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: MI and token distances in one layer"],"metadata":{"id":"0PFIoJ0UbsnO"}},{"cell_type":"code","source":["mi = np.zeros((len(target_idxs),len(target_idxs)))\n","tokdists = np.zeros((len(target_idxs),len(target_idxs)))\n","\n","# double-loop over the word pairs\n","for toki in range(len(target_idxs)):\n","  for tokj in range(toki+1,len(target_idxs)):\n","\n","    # extract the data\n","    x = activations['att_proj_3'][0,target_idxs[toki],:]\n","    y = activations['att_proj_3'][0,target_idxs[tokj],:]\n","\n","    # pairwise mutual information\n","    mi[toki,tokj] = mutInfo(x,y,4)\n","    tokdists[toki,tokj] = target_idxs[tokj]-target_idxs[toki]\n","\n","    # next line is for exercise 5\n","    # mi[toki,tokj] = mutual_info_regression(x.reshape(-1,1),y)[0]"],"metadata":{"id":"prYk4R9QtWLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(12,4))\n","\n","h = axs[0].imshow(mi,origin='lower',vmin=0,vmax=.6)\n","axs[0].set(xlabel='Target token index',ylabel='Target token index',title='Mutual information')\n","fig.colorbar(h,ax=axs[0],pad=.02,fraction=.047)\n","\n","# distances\n","h = axs[1].imshow(tokdists,origin='lower',vmin=0,vmax=60)\n","axs[1].set(xlabel='Target token index',ylabel='Target token index',title='Inter-token distances')\n","fig.colorbar(h,ax=axs[1],pad=.02,fraction=.047)\n","\n","# correlate MI with token distance\n","r = stats.kendalltau(tokdists[np.nonzero(tokdists)],mi[np.nonzero(mi)])\n","\n","axs[2].plot(tokdists[np.nonzero(tokdists)],mi[np.nonzero(mi)],'ks',markersize=10,markerfacecolor=[.7,.7,.9])\n","axs[2].set(xlabel='Inter-token distance',ylabel='Mutual information',\n","           title=f\"Kendall's $\\\\tau: {r.statistic:.2f}$ ($p={r.pvalue:.4f}$)\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"6z8ckh7abskZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZLTgqkbuVIfL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: MI metrics over layers"],"metadata":{"id":"Q8awOQ9MVIcc"}},{"cell_type":"code","source":["MIresults = np.zeros((2,nLayers,2))\n","\n","# initialize temp matrices (overwritten in each layer)\n","miA = np.zeros((len(target_idxs),len(target_idxs)))\n","miM = np.zeros((len(target_idxs),len(target_idxs)))\n","\n","sublayerComps = np.zeros((nLayers,2,2))\n","\n","\n","\n","# loop over layers\n","for layeri in range(nLayers):\n","\n","\n","  # double-loop over the word pairs\n","  for toki in range(len(target_idxs)):\n","    for tokj in range(toki+1,len(target_idxs)):\n","\n","      ### ATTENTION block\n","      # extract the data\n","      x = activations[f'att_proj_{layeri}'][0,target_idxs[toki],:]\n","      y = activations[f'att_proj_{layeri}'][0,target_idxs[tokj],:]\n","\n","      # trimmed manual MI implementation\n","      miA[toki,tokj] = mutInfo(x,y,4)\n","\n","      # Exercise 5: pairwise mutual information using sklearn\n","      # miA[toki,tokj] = mutual_info_regression(x.reshape(-1,1),y)[0]\n","\n","\n","      ### MLP block\n","      # extract the data\n","      x = activations[f'mlp_proj_{layeri}'][0,target_idxs[toki],:]\n","      y = activations[f'mlp_proj_{layeri}'][0,target_idxs[tokj],:]\n","\n","      # pairwise mutual information (second line is for Exercise 5)\n","      miM[toki,tokj] = mutInfo(x,y,4)\n","      # miM[toki,tokj] = mutual_info_regression(x.reshape(-1,1),y)[0]\n","\n","\n","  # ATTENTION summary statistics\n","  uMIa = miA[np.triu_indices(miA.shape[0],1)] # (uMIa = unique mutual information attn)\n","  uDi  = tokdists[np.triu_indices(tokdists.shape[0],1)] # doesn't change for attn-vs-mlp\n","  MIresults[0,layeri,0] = np.mean(uMIa)\n","  MIresults[0,layeri,1] = stats.kendalltau(uMIa,uDi).statistic\n","\n","  # MLP summary statistics\n","  uMIm = miM[np.triu_indices(miM.shape[0],1)]\n","  MIresults[1,layeri,0] = np.mean(uMIm)\n","  MIresults[1,layeri,1] = stats.kendalltau(uMIm,uDi).statistic\n","\n","  # t-test to compare MI\n","  t = stats.ttest_ind(uMIm,uMIa)\n","  sublayerComps[layeri,0,0] = t.statistic\n","  sublayerComps[layeri,0,1] = t.pvalue\n","\n","  # z-test to compare correlations\n","  ra = np.atanh(MIresults[0,layeri,1]) # fisher-transformed correlation\n","  rm = np.atanh(MIresults[1,layeri,1])\n","  z = (ra-rm) / np.sqrt( 2/(len(uMIa)-3) ) # diff/ste\n","  p = stats.norm.cdf(-abs(z)) # p-value\n","\n","  sublayerComps[layeri,1,0] = z\n","  sublayerComps[layeri,1,1] = p\n"],"metadata":{"id":"YqHor3r_--Jb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(2,3,figsize=(12,5.5))\n","\n","sublayer_labels = [ 'ATT','MLP' ]\n","\n","for i in range(2):\n","\n","  # plot the average MI\n","  axs[0,i].plot(MIresults[i,:,0],'ko',markerfacecolor=[.9,.7,.7,.7],markersize=8)\n","  axs[0,i].set(xlabel='Transformer block',ylabel='Mutual information',\n","               title=f'{sublayer_labels[i]}: Mutual information by layer',\n","               ylim=[0,MIresults[:,:,0].max()*1.1])\n","\n","  # plot the MI correlation with token distance\n","  axs[1,i].plot(MIresults[i,:,1],'ks',markerfacecolor=[.7,.7,.9,.7],markersize=8)\n","  axs[1,i].set(xlabel='Transformer block',ylabel='Kendall $\\\\tau$',\n","               title=f'{sublayer_labels[i]}: MI-distance correlation by layer',\n","               ylim=[MIresults[:,:,1].min()*1.1,.1])\n","\n","  # comparing att vs mlp\n","  fdr_ps = fdrcorrection(sublayerComps[:,i,1])[0]\n","  for li in range(nLayers):\n","    marker = 'kh' if fdr_ps[li] else 'rx'\n","    axs[i,2].plot(li,sublayerComps[li,i,0],marker,markerfacecolor=[.7,.9,.7,.7],markersize=8)\n","\n","  # legend (kinda hacky)\n","  axs[i,2].plot(100,0,'rx',label='Non-sig.') # out of bounds\n","  axs[i,2].plot(100,0,'kh',markerfacecolor=[.7,.9,.7],markersize=8,label='Significant')\n","  axs[i,2].legend()\n","  axs[i,2].set(xlim=[-2,nLayers+1],xlabel='Transformer block',ylabel=f\"{'tz'[i]}-statistic\",title='ATT vs. MLP by layer')\n","  axs[i,2].axhline(0,linestyle='--',color=[.7,.7,.7])\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"lngKLs3Z1gnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"APAmECiAVAqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Compare manual and sklearn implementations"],"metadata":{"id":"A5H-AXETi1w5"}},{"cell_type":"code","source":[],"metadata":{"id":"bXDjufPHi1uB"},"execution_count":null,"outputs":[]}]}