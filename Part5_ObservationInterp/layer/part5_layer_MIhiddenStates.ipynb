{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMQUJh1j18Yx8y9Yu9I4KXf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating layers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Pairwise mutual information through the LLM<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"8UrqMO28-9ZL"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","# MI\n","from sklearn.feature_selection import mutual_info_regression\n","\n","import torch\n","\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","gpt2.eval()"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from https://en.wikipedia.org/wiki/Turkish_coffee\n","text = 'Turkish coffee is very finely ground coffee brewed by boiling. Any coffee bean may be used; arabica varieties are considered best, but robusta or a blend is also used.[1] The coffee grounds are left in the coffee when served.[2][3] The coffee may be ground at home in a manual grinder made for the very fine grind, ground to order by coffee merchants in most parts of the world, or bought ready-ground from many shops.'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","print(f'There are {len(tokens[0])} tokens, {len(set(tokens[0].tolist()))} of which are unique.')"],"metadata":{"id":"L_5xIvYA--eH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  output = gpt2(tokens,output_hidden_states=True)"],"metadata":{"id":"hp-mwh6u--MZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nLayers = len(output.hidden_states)\n","output.hidden_states[3].shape"],"metadata":{"id":"dOFc8S1KAQ_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f26YBYHrQg16"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# A function to calculate mutual information based on entropy"],"metadata":{"id":"Q1hNTOv0Qgyi"}},{"cell_type":"code","source":["# a function for mutual information\n","def mutInfo_manual(x,y):\n","\n","  # histogram and convert to proportion (estimate of probability)\n","  Z  = np.histogram2d(x,y,bins=15)[0]\n","  pZ = Z / Z.sum()\n","  px = pZ.sum(axis=1)\n","  py = pZ.sum(axis=0)\n","\n","  # calculate entropy\n","  eps = 1e-12\n","  Hx = -np.sum( px * np.log2(px+eps) )\n","  Hy = -np.sum( py * np.log2(py+eps) )\n","  HZ = -np.sum( pZ * np.log2(pZ+eps) )\n","\n","  return Hx+Hy - HZ"],"metadata":{"id":"ikw5BdWVQgvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NsXA8Z9g6ezn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Example MI between one pair of dimensions across all tokens"],"metadata":{"id":"cvKA1ylWVFfs"}},{"cell_type":"code","source":["# example MI\n","\n","dim1 = 4\n","dim2 = 5\n","\n","# extract some data\n","x = output.hidden_states[3][0,:,dim1].detach()\n","y = output.hidden_states[3][0,:,dim2].detach()\n","\n","# 2D histogram\n","Z,xx,yy = np.histogram2d(x,y,bins=15)\n","\n","# mutual information\n","mi = mutual_info_regression(x.reshape(-1,1),y)[0]\n","\n","\n","# plotting\n","_,axs = plt.subplots(1,2,figsize=(11,5))\n","\n","minmax = 1.1 * abs(torch.cat((x,y),dim=0)).max() # extreme value for axis\n","axs[0].plot(x,y,'bo',markerfacecolor=[.4,.4,.9])\n","axs[0].set(xlabel=f'Dimension {dim1}',ylabel=f'Dimension {dim2}',xlim=[-minmax,minmax],ylim=[-minmax,minmax],\n","           title='Full resolution data\\nEach dot is a token')\n","\n","h = axs[1].imshow(Z.T,extent=[xx[0],xx[-1],yy[0],yy[-1]],vmin=0,vmax=Z.max()*.5,origin='lower',aspect='auto')\n","axs[1].set(xlabel='x',ylabel='y',title=f'Discretized data (MI = {mi:.3f})')\n","axs[1].plot(x,y,'wo',markerfacecolor=[.4,.4,.4],alpha=.7)\n","plt.colorbar(h,ax=axs[1])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"NJsWBTN8PypE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZLTgqkbuVIfL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# All-to-all MI in each layer"],"metadata":{"id":"Q8awOQ9MVIcc"}},{"cell_type":"code","source":["# this cell takes waaayyyy too long\n","# initialize\n","mi = np.zeros((nLayers,gpt2.config.n_embd,gpt2.config.n_embd))\n","\n","# loop over layers\n","for layeri in range(nLayers):\n","\n","  # extract matrices to detach only once\n","  hidden_states = output.hidden_states[layeri].detach().squeeze()\n","\n","  # double-loop over dimension pairs within this layer (skipping every second dimension)\n","  for dimi in range(0,gpt2.config.n_embd,2):\n","\n","    mi[layeri,dimi,:] = mutual_info_regression(hidden_states,hidden_states[:,dimi])\n","\n","  print(f'Finished {layeri+1:2}/{nLayers} layers')"],"metadata":{"id":"NU40QwbdNLQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layeri,dimi"],"metadata":{"id":"-zQ9JCms-OKX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this cell takes ~6 mins\n","# initialize\n","mi = np.zeros((nLayers,gpt2.config.n_embd,gpt2.config.n_embd))\n","\n","# loop over layers\n","for layeri in range(nLayers):\n","\n","  # extract matrices to detach only once\n","  hidden_states = output.hidden_states[layeri].detach().squeeze()\n","\n","  # double-loop over dimension pairs within this layer (skipping every second dimension)\n","  for dimi in range(0,gpt2.config.n_embd,2):\n","    for dimj in range(dimi+1,gpt2.config.n_embd,2):\n","\n","      # pairwise mutual information\n","      mi[layeri,dimi,dimj] = mutInfo_manual( hidden_states[:,dimi],hidden_states[:,dimj] )\n","\n","  print(f'Finished {layeri+1:2}/{nLayers} layers')"],"metadata":{"id":"YqHor3r_--Jb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NK6iXwUyKwH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize all pairwise MI from one layer\n","\n","layer2show = 2\n","\n","_,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","# submatrix for this layer\n","mitmp = mi[layer2show,:,:]\n","\n","axs[0].hist(mitmp[np.nonzero(mitmp)],bins=60,color='gray',edgecolor='k')\n","axs[0].set(xlabel='Mutual information',ylabel='Count',title='Distribution of MI values')\n","\n","h = axs[1].imshow(mitmp[::2,1::2],vmin=.5,vmax=1.2,origin='lower')\n","plt.colorbar(h,ax=axs[1])\n","axs[1].set(xlabel='Embedding dimension',ylabel='Embedding dimension',title='Pairwise mutual information')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"eu3jvvdH--Gr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# histograms from all layers\n","plt.figure(figsize=(10,4))\n","\n","# bin boundaries\n","edges = np.linspace(.2,1.9,64)\n","\n","\n","for layeri in range(nLayers):\n","\n","  # get distribution\n","  mitmp = mi[layeri,:,:]\n","  yy,_ = np.histogram(mitmp[np.nonzero(mitmp)],bins=edges,density=True)\n","\n","  # plot\n","  plt.plot(edges[:-1],yy,color=mpl.cm.plasma(layeri/gpt2.config.n_layer),label=f'L{layeri}')\n","\n","\n","plt.legend()\n","plt.gca().set(xlim=[.2,2],xlabel='Mutual information value',ylabel='Density (pdf estimate)',title='Distribution of MI over layers')\n","plt.show()"],"metadata":{"id":"LykGov10EMfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"APAmECiAVAqQ"},"execution_count":null,"outputs":[]}]}