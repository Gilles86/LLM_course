{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMFufkzS/DIYCzdEI0G8tNN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddingss<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Byte-pair encoding<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"Q8_-VKZNDmzB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"APFQtXa0brAf"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","source":[],"metadata":{"id":"TrCWDXUdTS_N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialize the vocabulary"],"metadata":{"id":"bRwQVVLT6ZNP"}},{"cell_type":"code","source":["# some text with lots of repetitions\n","text = 'like liker love lovely hug hugs hugging hearts'\n","\n","chars = list(set(text))\n","chars.sort() # initial vocab is sorted\n","\n","for l in chars:\n","  print(f'\"{l}\" appears {text.count(l)} times.')"],"metadata":{"id":"6M7HN5nLTmaq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make a vocabulary\n","vocab = { word:i for i,word in enumerate(chars) }\n","vocab"],"metadata":{"id":"mXFIcoLk7Q8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the text needs to be a list, not a string\n","# each element in the list is a token\n","origtext = list(text)\n","print(text)\n","print(origtext)"],"metadata":{"id":"k70QKgFkBCr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4NW4pLnVUgcd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find character pairs and merge the most frequent"],"metadata":{"id":"K_wH9S9WUgZF"}},{"cell_type":"code","source":["token_pairs = dict()\n","\n","# loop over tokens\n","for i in range(len(origtext)-1):\n","\n","  # create a pair\n","  pair = origtext[i] + origtext[i+1]\n","\n","  # increase pair frequencies\n","  if pair in token_pairs:\n","    token_pairs[pair] += 1\n","  else:\n","    token_pairs[pair] = 1\n","\n","token_pairs"],"metadata":{"id":"porxIPE4cqrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the most frequent pair\n","mostFreqPair_idx = np.argmax(list(token_pairs.values()))\n","mostFreqPair_char = list(token_pairs.keys())[mostFreqPair_idx]\n","print(f'The most frequent character pair is \"{mostFreqPair_char}\" with {list(token_pairs.values())[mostFreqPair_idx]} appearances')"],"metadata":{"id":"uFaRRbskcqn-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# update the vocab\n","vocab[mostFreqPair_char] = max(vocab.values())+1\n","vocab"],"metadata":{"id":"RWWoz2T-FWcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"md3EG369FgmL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Replace the token pair with one token"],"metadata":{"id":"um8UTajPFght"}},{"cell_type":"code","source":["# initialize a new text list\n","newtext = []\n","\n","# loop through the list\n","i = 0\n","while i<(len(origtext)-1):\n","\n","  # test whether the pair of this and the following elements match the newly-created pair\n","  if (origtext[i]+origtext[i+1]) == mostFreqPair_char:\n","\n","    # append to the new version of the text\n","    newtext.append(mostFreqPair_char)\n","    print(f'added \"{mostFreqPair_char}\"')\n","\n","    # skip the next character\n","    i += 2\n","\n","  # this isn't a merged pair, so add this token to the list\n","  else:\n","    newtext.append(origtext[i])\n","\n","    # move to the next character\n","    i += 1\n","\n","\n","print('\\n')\n","print(f'Original text: {origtext}')\n","print(f'Updated text:  {newtext}')\n","\n","print(f'\\n\\nOriginal text had {len(origtext)} tokens; new text has {len(newtext)} tokens.')"],"metadata":{"id":"9C4p87b66hMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"T7oJ4dgs6hPe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find the most common letter pairs (again!)"],"metadata":{"id":"axWcmSB76hSR"}},{"cell_type":"code","source":["token_pairs = dict()\n","\n","# loop over the newtext tokens (not the original!)\n","for i in range(len(newtext)-1):\n","\n","  # create a pair\n","  pair = newtext[i] + newtext[i+1]\n","\n","  # increase pair frequencies\n","  if pair in token_pairs:\n","    token_pairs[pair] += 1\n","  else:\n","    token_pairs[pair] = 1\n","\n","token_pairs"],"metadata":{"id":"OTmE2Axj6hWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ky9njLdZUgVR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now using functions"],"metadata":{"id":"4Smkdx0MUgRx"}},{"cell_type":"code","source":["# > -----------------------------------\n","def get_pair_stats(text2pair):\n","  token_pairs = dict()\n","\n","  # loop over tokens\n","  for i in range(len(text2pair)-1):\n","\n","    # create a pair\n","    pair = text2pair[i] + text2pair[i+1]\n","\n","    # increase pair frequencies\n","    if pair in token_pairs:\n","      token_pairs[pair] += 1\n","    else:\n","      token_pairs[pair] = 1\n","\n","  return token_pairs\n","# ----------------------------------- <\n","\n","\n","\n","# > -----------------------------------\n","def update_vocab(token_pairs,vocab):\n","\n","  # find the most frequent pair\n","  idx = np.argmax(list(token_pairs.values()))\n","  newtok = list(token_pairs.keys())[idx]\n","\n","  # update the vocab\n","  vocab[newtok] = max(vocab.values())+1\n","  return vocab,newtok\n","# ----------------------------------- <\n","\n","\n","\n","# -----------------------------------\n","def generate_new_token_seq(prevtext,newtoken):\n","\n","  # initialize a new text list\n","  newtext = []\n","\n","  # loop through the list\n","  i = 0\n","  while i<(len(prevtext)-1):\n","\n","    # test whether the pair of this and the following elements match the newly-created pair\n","    if (prevtext[i]+prevtext[i+1]) == newtoken:\n","      newtext.append(newtoken)\n","      i += 2 # skip the next character\n","\n","    # not a pair\n","    else:\n","      newtext.append(prevtext[i])\n","      i += 1 # move to the next character\n","\n","  return newtext\n","# ----------------------------------- <"],"metadata":{"id":"DrX7ThhHcqk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GaBv1eDocqhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# re-initialize the vocab\n","vocab = { word:i for i,word in enumerate(chars) }\n","print(f'Vocab has {len(vocab)} tokens.')"],"metadata":{"id":"jVnWU2qMcqe4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r4vd-Pn1KjpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## do one iteration\n","\n","# find and count pairs\n","pairs = get_pair_stats(origtext)\n","\n","# update the dictionary\n","vocab,newtoken = update_vocab(pairs,vocab)\n","\n","# get a new list of tokens\n","updated_text = generate_new_token_seq(origtext,newtoken)\n","print(f'Vocab has {len(vocab)} tokens.')"],"metadata":{"id":"AJJoz6Y8KcPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## do a second iteration\n","pairs = get_pair_stats(updated_text)\n","\n","# update the dictionary\n","vocab,newtoken = update_vocab(pairs,vocab)\n","\n","# get a new list of tokens\n","updated_text = generate_new_token_seq(updated_text,newtoken)\n","print(f'Vocab has {len(vocab)} tokens.')"],"metadata":{"id":"72G7ZJ1fcqbw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab"],"metadata":{"id":"8fzm86CAcqYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8zwgt0JTNw0y"},"execution_count":null,"outputs":[]}]}