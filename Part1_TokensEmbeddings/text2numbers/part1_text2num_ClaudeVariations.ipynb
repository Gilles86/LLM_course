{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNUFPQMR5Xu6joIiBF59cEW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Word variations in Claude tokenizer<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"SxB6dAb3nx6s"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jASJ6qhCZYG2"},"outputs":[],"source":["from transformers import GPT2TokenizerFast\n","\n","tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/claude-tokenizer')"]},{"cell_type":"code","source":[],"metadata":{"id":"48RA3TSLZZTD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exploring the tokenizer"],"metadata":{"id":"5uiG7eQzcEFg"}},{"cell_type":"code","source":["tokenizer.vocab_size"],"metadata":{"id":"bkYuR2svaL3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.encode('this is some text')"],"metadata":{"id":"EuUYPzxUaL0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer('this is some text')"],"metadata":{"id":"JfwdmW5PaYwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer('this is some text',return_tensors='pt')"],"metadata":{"id":"_ZhDYMSVaLxr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ADTrqWtwafGn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Some examples of multitoken words"],"metadata":{"id":"5Md7b5Ewae_2"}},{"cell_type":"code","source":["words = [ 'hypothetical',\n","          ' hypothetical',\n","          'hypothetical ',\n","          ' hypothteical'  ]\n","\n","for word in words:\n","  toks = tokenizer.encode(word)\n","  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"],"metadata":{"id":"U2YaY0IAZZQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = [ \"can't\",\n","          \" can't\",\n","          \"cant\" ]\n","\n","for word in words:\n","  toks = tokenizer.encode(word)\n","  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"],"metadata":{"id":"92n9vHOMZZNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = [ ' straight forward',\n","          ' straightforward',\n","          ' straight-forward'  ]\n","\n","for word in words:\n","  toks = tokenizer.encode(word)\n","  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"],"metadata":{"id":"G94YpqWIZZKN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = [ 'parttime',\n","          ' part-time',\n","          'part-time' ]\n","\n","for word in words:\n","  toks = tokenizer.encode(word)\n","  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"],"metadata":{"id":"VlEjISXDZZHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G6AKcZX7ZZEj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Common misspellings"],"metadata":{"id":"3NIbDxr3ZZBx"}},{"cell_type":"code","source":["words = [ ' accommodate acommodate',\n","          ' a lot alot',\n","          ' definately definitely',\n","          'occurrence occurence occurance']\n","\n","for word in words:\n","  toks = tokenizer.encode(word)\n","  print(f'\"{word}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"],"metadata":{"id":"hNSgpVEzZY-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3Gtu-vdCdGF2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Punctuation"],"metadata":{"id":"ydb8aKwVdGDZ"}},{"cell_type":"code","source":["sentences = [ \"People say, you shouldn't do that.\",\n","              \"People say; you shouldn't do that.\",\n","              \"People say -- you shouldn't do that.\",  ]\n","\n","for senten in sentences:\n","  toks = tokenizer.encode(senten)\n","  print(f'\"{senten}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"],"metadata":{"id":"isfwgyfBdGAq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = '“Curiouser and curiouser!” cried Alice (she was so much surprised, that for the moment she quite forgot how to speak good English).'\n","\n","toks = tokenizer.encode(text)\n","print(f'\"{text}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"],"metadata":{"id":"_1hnloRddF9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences = [ '“No, no!” said the Queen. “Sentence first—verdict afterwards.”',\n","              '“No, no!” said the Queen. “Sentence first - verdict afterwards.”']\n","\n","for senten in sentences:\n","  toks = tokenizer.encode(senten)\n","  print(f'\"{senten}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"],"metadata":{"id":"ECXuCbPxZY8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VagisaHOemjL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Code"],"metadata":{"id":"Q4KcsfaCe1KY"}},{"cell_type":"code","source":["sentences = [ '\\\\textbf{Leibniz} & $\\\\frac{dy}{dx}$', # latex\n","              '&= \\lim_{h\\to0}\\alpha \\left[\\frac{f(x+h)-f(x)}{h}\\right] \\;+\\; \\lim_{h\\to0} \\beta \\left[\\frac{g(x+h)-g(x)}{h}\\right]', # latex\n","              'diffVects = targetActs[layeri,:,:,1] - targetActs[layeri-1,:,:,1]; diffNorms[layeri,1] = np.linalg.norm(diffVects,axis=1).mean()', # python\n","              '[Y,X] = meshgrid(linspace(-4,4,21)); G = exp( -(X.^2+Y.^2)/10 );' # MATLAB\n","              ]\n","\n","for senten in sentences:\n","  toks = tokenizer.encode(senten)\n","  print(f'\"{senten}\" has {len(toks)} tokens ({toks}):\\n   {[tokenizer.decode(t) for t in toks]}\\n')"],"metadata":{"id":"GHi-Nx91e1H6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"buiTri6se1AO"},"execution_count":null,"outputs":[]}]}