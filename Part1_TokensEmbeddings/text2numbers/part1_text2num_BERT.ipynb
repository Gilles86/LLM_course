{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNrF0moQjxlYfOwvTAJKp57"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Tokenization in BERT<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"9B8BpAieRqI5"}},{"cell_type":"code","source":[],"metadata":{"id":"_4NUr5hhRqFc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import BERT tokenizer"],"metadata":{"id":"4v1cyOt23Bqe"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# load BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NVlg8p0x3KPP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inspect the tokenizer properties"],"metadata":{"id":"em5ykH6t3KMg"}},{"cell_type":"code","source":["# inspect the tokenizer info\n","dir(tokenizer)"],"metadata":{"id":"YFklMtfeCnJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gQnoY3PP3NZN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Check out some tokens"],"metadata":{"id":"KuWK93BA3NWd"}},{"cell_type":"code","source":["all_tokens = list(tokenizer.get_vocab().keys())\n","all_tokens[20000:20100]"],"metadata":{"id":"BVLQWI8s3RVJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.vocab_size)\n","tokenizer.get_vocab()['science']"],"metadata":{"id":"2vUAJjP0CnHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_oB_uEPlCnEj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizing a word"],"metadata":{"id":"O1r6BXyv3ci4"}},{"cell_type":"code","source":["word = 'science'\n","\n","res1 = tokenizer.convert_tokens_to_ids(word)\n","res2 = tokenizer.get_vocab()[word]\n","\n","print(res1)\n","print(res2)"],"metadata":{"id":"q-DdiCr23fBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wIZvZnNy3e-A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Encoding a text"],"metadata":{"id":"sq2bgkEq3e7U"}},{"cell_type":"code","source":["text = 'science is great'\n","\n","res1 = tokenizer.convert_tokens_to_ids(text)\n","res2 = tokenizer.get_vocab()[text]\n","\n","print(res1)\n","print(res2)"],"metadata":{"id":"IWjew3aOCnB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# better method:\n","res3 = tokenizer.encode(text)\n","\n","for i in res3:\n","  print(f'Token {i} is \"{tokenizer.decode(i)}\"')\n","\n","# [CLS] = classification\n","# [SEP] = sentence separation\n","\n","print('')\n","print(tokenizer.decode(res3,skip_special_tokens=True))\n","print(tokenizer.decode(res3,skip_special_tokens=False))"],"metadata":{"id":"rXXAfvB8GkdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT adds [CLS]...[SEP] with each encode\n","tokenizer.decode(tokenizer.encode(tokenizer.decode(tokenizer.encode( text ))))"],"metadata":{"id":"hhbmvRHvDVdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BhYhUdH2DLyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Calling the class directly"],"metadata":{"id":"5rM1zNMeDLvX"}},{"cell_type":"code","source":["tokenizer(text)"],"metadata":{"id":"jZZpL7xqDOWL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"W-qJeimW3s2P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# More on tokenizing"],"metadata":{"id":"0JHKZRrk3szm"}},{"cell_type":"code","source":["sentence = 'AI is both exciting and terrifying.'\n","\n","print('Original sentence:')\n","print(f'  {sentence}\\n')\n","\n","# segment the text into tokens\n","tokenized = tokenizer.tokenize(sentence)\n","print('Tokenized (segmented) sentence:')\n","print(f'  {tokenized}')\n","\n","# encode the tokenized sentence\n","ids_from_tokens = tokenizer.convert_tokens_to_ids(tokenized)\n","print(f'  {ids_from_tokens}\\n')\n","\n","# and finally, encode from the original sentence\n","encodedText = tokenizer.encode(sentence)\n","print('Encoded from the original text:')\n","print(f'  {encodedText}\\n\\n')\n","\n","# now for decoding\n","print('Decoded from token-wise encoding:')\n","print(f'  {tokenizer.decode(ids_from_tokens)}\\n')\n","\n","print('Decoded from text encoding:')\n","print(f'  {tokenizer.decode(encodedText)}')"],"metadata":{"id":"bbJ7u8mrHrrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5tN-vdsOJAMb"},"execution_count":null,"outputs":[]}]}