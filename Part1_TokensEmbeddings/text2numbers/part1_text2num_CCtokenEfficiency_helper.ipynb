{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNWYPgiMJpH/V9qU+Ud4AoS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Token count by subword length<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"MbkzUrDcI3VW"}},{"cell_type":"code","source":[],"metadata":{"id":"aX83gXxSI5cy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8JzcByJ_dVt"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","!pip install tiktoken\n","import tiktoken"]},{"cell_type":"code","source":[],"metadata":{"id":"Vll_zH5t_gHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPT-4's tokenizer\n","tokenizer = tiktoken.get_encoding('cl100k_base')"],"metadata":{"id":"f34QAbA-_gD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E9WjkvGzDa65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Token count by word length"],"metadata":{"id":"AHHbrPnS_f2F"}},{"cell_type":"code","source":["import requests\n","import re\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","tmTokens ="],"metadata":{"id":"b-oWizBu_fzO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split by punctuation\n","words = re.split(r'([,.:;—?_!\"“()\\']|--|\\s)',text)\n","words = [item.strip() for item in words if item.strip()]\n","\n","tokenCount = np.zeros\n","\n","for idx,w in enumerate(words):\n","  tokenCount[idx,0] =  # first column is the length of the word\n","  tokenCount[idx,1] =  # second column is the number of tokens"],"metadata":{"id":"sCLJkrvUE-Lu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","\n","offsetsX = # appropriately scaled random scatter\n","offsetsY =\n","\n","plt.plot(,,'k.',alpha=.5)\n","plt.gca().set(xlabel='Word lengths',ylabel='Encoded token count',xticks=\n","\n","plt.show()"],"metadata":{"id":"hobD0KBTFWpC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FRpUUIqKqKbR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Encoding of 14-character words"],"metadata":{"id":"eg_R9W9fqKYf"}},{"cell_type":"code","source":["# find words with characters\n","wordsWith14Chars =\n","\n","# print their tokens\n","for idx in wordsWith14Chars:\n","  this_decode = # list comprehension to decode the tokens for this word\n","  print(f'\"{idx}\" comprises {this_decode}')"],"metadata":{"id":"2hCNuNz1FWmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FWXJa9spm8cD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Token efficiency"],"metadata":{"id":"XmenZMcnm8SN"}},{"cell_type":"code","source":["# \"more efficient\" word: lots of letters and few tokens\n","# \"less efficient\" word: few letters and many tokens\n","moreEfficient =\n","lessEfficient =\n","\n","print(f'A very efficient word:\\n  \"{words[moreEfficient[0]]}\" has {tokenCount[moreEfficient[0],0]} letters and {tokenCount[moreEfficient[0],1]} tokens.\\n')\n","print(f'An inefficient word:\\n"],"metadata":{"id":"A2nFEm8RNeRv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# \"efficiency\" as characters/tokens\n","efficiency = something / somethingElse\n","\n","# show a historgram\n","plt.figure(figsize=(10,4))\n","plt.hist(efficiency,color=[.9,.7,.7],edgecolor='k',linewidth=.4,bins='fd')\n","plt.gca().set(xlabel=,ylabel=,title=)\n","plt.show()"],"metadata":{"id":"ONqFv41bOhte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the most and least efficiently tokenized words\n","max_efficiency =\n","min_efficiency =\n","\n","# find all the words with those efficiency values\n","most_efficient_words =\n","least_efficient_words =\n","\n","# find and print the unique words with max-efficiency score\n","most_efficient_words = # overwriting the previously defined variable\n","\n","print('MOST EFFICIENT WORDS:')\n","for w in most_efficient_words:\n","  print(f'\"{w}\" has {max_efficiency} characters per token')\n","\n","\n","# repeat for min-efficiency score\n","print('\\n\\nLEAST EFFICIENT WORDS:')\n"],"metadata":{"id":"K1u0jY0ZPSr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZhaTodvQFWjm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Tokens in separated words vs. text"],"metadata":{"id":"PcgnouSAMpBR"}},{"cell_type":"code","source":["# unique set of words as we've split them up\n","uniqueWords =\n","print(f'There are {} unique words in The Time Machine according to our split.')"],"metadata":{"id":"ZLUQzwEgMqMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# random token\n","token_idx =\n","\n","# non-random tokens to try\n","# token_idx = 1879#,5030#,716\n","\n","# find the words (from our split) in The Time Machine that contain that token\n","words_with_token = [w for w in uniqueWords if\n","\n","# find the context (from the full encoding) surrounding each token appearance\n","seqs_with_token = ==token_idx\n","\n","# print the token\n","print(f'Token {token_idx} is \"{tokenizer.decode([token_idx])}\"\\n\\n')\n","\n","# its occurance in our manually split words\n","print(f'*** Our manual word split: Token appears {len(words_with_token)} times, including:\\n----------------')\n","for w in words_with_token:\n","\n","\n","# its occurances in the GPT encoding\n","print(f'\\n\\n*** From encoding the full text: This token appears {sum(seqs_with_token)} times, including:\\n----------------')\n","for s in np.where(seqs_with_token)[0]:\n"],"metadata":{"id":"ItwqBsyhFWgh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3Xv93Niy4Di6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Do more frequent words have fewer tokens?"],"metadata":{"id":"PSOkxEW2-emX"}},{"cell_type":"code","source":["# the unique words, as a list instead of a set (for indexing)\n","uniqueWords = # OK to overwrite the variable...\n","\n","# initialize results vectors\n","wordFreq = np.zeros(,dtype=int)\n","numTokens = np.zeros(len(uniqueWords),)\n","\n","\n","# loop through all unique words\n","for i,uword in enumerate(uniqueWords):\n","\n","  # count the number of times that word appears\n","\n","\n","  # count the number of tokens in that word\n",""],"metadata":{"id":"HVBdiZC_4DgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","plt.plot(\n","plt.gca().set(xlabel='Word frequency (log)',ylabel='Encoding length',xscale='log')\n","plt.show()"],"metadata":{"id":"SCDq0AOo9xFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wordFreqidx = np.argsort(wordFreq)[::-1]\n","for i in wordFreqidx[:20]:\n","  print(f'{} appearances of \"{}\"')"],"metadata":{"id":"rTOmNEBH7N-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PzIjDOyY4DSI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 6: Fake words from real tokens"],"metadata":{"id":"PzXEaMx4FWYp"}},{"cell_type":"code","source":["nTokens = .00003 # hmm...\n","\n","# choose some random tokens\n","randtokens = np.random.choice\n","\n","# print the individual tokens\n","for t in randtokens:\n","  print(f'Token {t} is \"{tokenizer.decode([t])}\"')\n","\n","# fake word without whitespace\n","fakeword = re.sub\n","\n","print(f'\\nThe fake word is \"{fakeword}\"')"],"metadata":{"id":"2fAPp7SIFWV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WE4HmJ8JBXCB"},"execution_count":null,"outputs":[]}]}