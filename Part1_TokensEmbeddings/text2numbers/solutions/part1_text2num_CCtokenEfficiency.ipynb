{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOkYXtRBVRdmmP2acPFhshI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Token count by subword length<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"MbkzUrDcI3VW"}},{"cell_type":"code","source":[],"metadata":{"id":"aX83gXxSI5cy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8JzcByJ_dVt"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","!pip install tiktoken\n","import tiktoken"]},{"cell_type":"code","source":[],"metadata":{"id":"Vll_zH5t_gHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPT-4's tokenizer\n","tokenizer = tiktoken.get_encoding('cl100k_base')"],"metadata":{"id":"f34QAbA-_gD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E9WjkvGzDa65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Token count by word length"],"metadata":{"id":"AHHbrPnS_f2F"}},{"cell_type":"code","source":["import requests\n","import re\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","tmTokens = tokenizer.encode(text)"],"metadata":{"id":"b-oWizBu_fzO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split by punctuation\n","words = re.split(r'([,.:;—?_!\"“()\\']|--|\\s)',text)\n","words = [item.strip() for item in words if item.strip()]\n","\n","tokenCount = np.zeros((len(words),2),dtype=int)\n","\n","for idx,w in enumerate(words):\n","  tokenCount[idx,0] = len(w) # first column is the length of the word\n","  tokenCount[idx,1] = len(tokenizer.encode(w)) # second column is the number of tokens"],"metadata":{"id":"sCLJkrvUE-Lu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","\n","offsetsX = np.random.randn(len(words))/20\n","offsetsY = np.random.randn(len(words))/20\n","\n","plt.plot(tokenCount[:,0]+offsetsX,tokenCount[:,1]+offsetsY,'k.',alpha=.5)\n","plt.gca().set(xlabel='Word lengths',ylabel='Encoded token count',xticks=np.arange(1,np.max(tokenCount[:,0])+1))\n","\n","plt.show()"],"metadata":{"id":"hobD0KBTFWpC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FRpUUIqKqKbR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Encoding of 14-character words"],"metadata":{"id":"eg_R9W9fqKYf"}},{"cell_type":"code","source":["# find words with characters\n","wordsWith14Chars = np.where(tokenCount[:,0]==14)[0]\n","\n","# print their tokens\n","for idx in wordsWith14Chars:\n","  this_decode = [ tokenizer.decode([t]) for t in tokenizer.encode(words[idx]) ]\n","  print(f'\"{words[idx]}\" comprises {this_decode}')"],"metadata":{"id":"2hCNuNz1FWmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FWXJa9spm8cD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Token efficiency"],"metadata":{"id":"XmenZMcnm8SN"}},{"cell_type":"code","source":["# \"more efficient\" word: lots of letters and few tokens\n","# \"less efficient\" word: few letters and many tokens\n","moreEfficient = np.where( (tokenCount[:,0]==17) & (tokenCount[:,1]==2) )[0]\n","lessEfficient = np.where( (tokenCount[:,0]==10) & (tokenCount[:,1]==6) )[0]\n","\n","print(f'A very efficient word:\\n  \"{words[moreEfficient[0]]}\" has {tokenCount[moreEfficient[0],0]} letters and {tokenCount[moreEfficient[0],1]} tokens.\\n')\n","print(f'An inefficient word:\\n  \"{words[lessEfficient[0]]}\" has {tokenCount[lessEfficient[0],0]} letters and {tokenCount[lessEfficient[0],1]} tokens.')"],"metadata":{"id":"A2nFEm8RNeRv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# \"efficiency\" as characters/tokens\n","efficiency = tokenCount[:,0]/tokenCount[:,1]\n","\n","# show a historgram\n","plt.figure(figsize=(10,4))\n","plt.hist(efficiency,color=[.9,.7,.7],edgecolor='k',linewidth=.4,bins='fd')\n","plt.gca().set(xlabel='Efficiency score',ylabel='Frequency',title='Word efficiency (characters/tokens)')\n","plt.show()"],"metadata":{"id":"ONqFv41bOhte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the most and least efficiently tokenized words\n","max_efficiency = np.max(efficiency)\n","min_efficiency = np.min(efficiency)\n","\n","# find all the words with those efficiency values\n","most_efficient_words = np.where(efficiency==max_efficiency)[0]\n","least_efficient_words = np.where(efficiency==min_efficiency)[0]\n","\n","# find and print the unique words with max-efficiency score\n","most_efficient_words = list(set([ words[i] for i in most_efficient_words ]))\n","\n","print('MOST EFFICIENT WORDS:')\n","for w in most_efficient_words:\n","  print(f'\"{w}\" has {max_efficiency} characters per token')\n","\n","\n","# repeat for min-efficiency score\n","print('\\n\\nLEAST EFFICIENT WORDS:')\n","for w in list(set([ words[i] for i in least_efficient_words ])):\n","  print(f'\"{w}\" has {min_efficiency:.2f} characters per token')"],"metadata":{"id":"K1u0jY0ZPSr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZhaTodvQFWjm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Tokens in separated words vs. text"],"metadata":{"id":"PcgnouSAMpBR"}},{"cell_type":"code","source":["# unique set of words as we've split them up\n","uniqueWords = set(words)\n","print(f'There are {len(uniqueWords)} unique words in The Time Machine according to our split.')"],"metadata":{"id":"ZLUQzwEgMqMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# random token\n","token_idx = np.random.randint(0,tokenizer.n_vocab//10)\n","\n","# non-random tokens to try\n","# token_idx = 1879#,5030#,716\n","\n","# find the words (from our split) in The Time Machine that contain that token\n","words_with_token = [w for w in uniqueWords if token_idx in tokenizer.encode(w)]\n","\n","# find the context (from the full encoding) surrounding each token appearance\n","seqs_with_token = np.array(tmTokens)==token_idx\n","\n","# print the token\n","print(f'Token {token_idx} is \"{tokenizer.decode([token_idx])}\"\\n\\n')\n","\n","# its occurance in our manually split words\n","print(f'*** Our manual word split: Token appears {len(words_with_token)} times, including:\\n----------------')\n","for w in words_with_token:\n","  print(f'{w}')\n","\n","# its occurances in the GPT encoding\n","print(f'\\n\\n*** From encoding the full text: This token appears {sum(seqs_with_token)} times, including:\\n----------------')\n","for s in np.where(seqs_with_token)[0]:\n","  print(f'{tokenizer.decode( np.array(tmTokens[s-5:s+5]) )}\\n----------------')"],"metadata":{"id":"ItwqBsyhFWgh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3Xv93Niy4Di6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Do more frequent words have fewer tokens?"],"metadata":{"id":"PSOkxEW2-emX"}},{"cell_type":"code","source":["# the unique words, as a list instead of a set (for indexing)\n","uniqueWords = list(uniqueWords)\n","\n","# initialize results vectors\n","wordFreq = np.zeros(len(uniqueWords),dtype=int)\n","numTokens = np.zeros(len(uniqueWords),dtype=int)\n","\n","\n","# loop through all unique words\n","for i,uword in enumerate(uniqueWords):\n","\n","  # count the number of times that word appears\n","  wordFreq[i] = words.count(uword)\n","\n","  # count the number of tokens in that word\n","  numTokens[i] = len(tokenizer.encode(uword))"],"metadata":{"id":"HVBdiZC_4DgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","plt.plot(wordFreq,numTokens,'ko',markerfacecolor=[.7,.9,.7],alpha=.6)\n","plt.gca().set(xlabel='Word frequency (log)',ylabel='Encoding length',xscale='log')\n","plt.show()"],"metadata":{"id":"SCDq0AOo9xFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wordFreqidx = np.argsort(wordFreq)[::-1]\n","for i in wordFreqidx[:20]:\n","  print(f'{wordFreq[i]:>5,} appearances of \"{uniqueWords[i]}\"')"],"metadata":{"id":"rTOmNEBH7N-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PzIjDOyY4DSI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 6: Fake words from real tokens"],"metadata":{"id":"PzXEaMx4FWYp"}},{"cell_type":"code","source":["nTokens = 3\n","\n","# choose some random tokens\n","randtokens = np.random.choice(range(tokenizer.n_vocab),nTokens)\n","\n","# print the individual tokens\n","for t in randtokens:\n","  print(f'Token {t} is \"{tokenizer.decode([t])}\"')\n","\n","# fake word without whitespace\n","fakeword = re.sub(r'\\s+','',tokenizer.decode(randtokens))\n","\n","print(f'\\nThe fake word is \"{fakeword}\"')"],"metadata":{"id":"2fAPp7SIFWV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WE4HmJ8JBXCB"},"execution_count":null,"outputs":[]}]}