{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOwkR6oDp9YABqkQNi37psi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Zipf's law in characters and tokens<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"s47798zFeK02"}},{"cell_type":"code","source":[],"metadata":{"id":"bFz0fvDKeKxe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8JzcByJ_dVt"},"outputs":[],"source":["# for getting text data off the web\n","import requests\n","import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","!pip install tiktoken\n","import tiktoken"]},{"cell_type":"code","source":["# GPT-4's tokenizer\n","tokenizer = tiktoken.get_encoding('cl100k_base')"],"metadata":{"id":"f34QAbA-_gD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vll_zH5t_gHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Counting occurances in numbers and letters"],"metadata":{"id":"3wBnp89CoU78"}},{"cell_type":"code","source":["# a quick intro to np.unique\n","\n","nums = np.array([ 1,1,3,2,2,2,9,9,9,-1,-1,-1,-1,-1 ])\n","uniq,counts = np.unique(nums,return_counts=True)\n","\n","for n,c in zip(uniq,counts):\n","  print(f'The number {n:2} appears {c} times.')"],"metadata":{"id":"CQAQdEH3olRU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# also works for strings?\n","text = 'Hello, my name is Mike and I like to each chocolate.'\n","np.unique(text) # nope :("],"metadata":{"id":"4kPqTt4BoU5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# but this works:\n","uniq = set(text)\n","counts = np.zeros(len(uniq),dtype=int)\n","for i,u in enumerate(uniq):\n","  counts[i] = text.count(u)\n","\n","# or using list-comprehension\n","#counts = [ text.count(u) for u in uniq ]\n","\n","# print out\n","for n,c in zip(uniq,counts):\n","  print(f'The letter \"{n}\" appears {c:2} times.')"],"metadata":{"id":"CafKz1kMeh69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gHoOT8EyoUvL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Explore Zipf's law in books"],"metadata":{"id":"a6e_f2fDoUsN"}},{"cell_type":"code","source":["# all books have the same url format; they are unique by numerical code\n","baseurl = 'https://www.gutenberg.org/cache/epub/'\n","\n","bookurls = [\n","    # code       title\n","    ['84',    'Frankenstein'    ],\n","    ['64317', 'GreatGatsby'     ],\n","    ['11',    'AliceWonderland' ],\n","    ['1513',  'RomeoJuliet'     ],\n","    ['76',    'HuckFinn'        ],\n","    ['219',   'HeartDarkness'   ],\n","    ['2591',  'GrimmsTales'     ],\n","    ['2148',  'EdgarAllenPoe'   ],\n","    ['36',    'WarOfTheWorlds'  ],\n","    ['829',   'GulliversTravels']\n","]"],"metadata":{"id":"fX_ZgpveSYPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","for code,title in bookurls:\n","\n","  # get the text\n","  fullurl = baseurl + code + '/pg' + code + '.txt'\n","  text = requests.get(fullurl).text\n","\n","  # Zipf's law for characters\n","  counts = [ text.count(u) for u in set(text) ]\n","  axs[0].plot(np.sort(counts)[::-1],'.',markersize=4,alpha=.6,label=title)\n","\n","  # and for tokens\n","  tokens = tokenizer.encode(text)\n","  unitokens,counts = np.unique(tokens,return_counts=True)\n","  axs[1].plot(np.sort(counts)[::-1],'.',markersize=4,alpha=.3,label=title)\n","\n","\n","# axis adjustments\n","for a in axs:\n","  a.legend(fontsize=9)\n","  a.set(xscale='log',yscale='log',xlabel='Sorted token index (log)',ylabel='Frequency in text (log)')\n","\n","axs[0].set_title('Frequency of characters')\n","axs[1].set_title('Frequency of GPT tokens')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"hC9TN_2SPEuT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i_3jPwujPEpV"},"execution_count":null,"outputs":[]}]}