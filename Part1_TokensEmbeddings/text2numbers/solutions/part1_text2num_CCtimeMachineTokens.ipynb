{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMzHThNeV1G6urTXvEEJxVz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Tokenizing The Time Machine<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"1pxg6ywI6EEw"}},{"cell_type":"code","source":[],"metadata":{"id":"7k510WhK6Dpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwgCppyRKogB"},"outputs":[],"source":["# typical libraries...\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# for importing and working with texts\n","import requests\n","import re\n","import string\n","\n","# adjust matplotlib defaults to personal preferences\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"3xNZfqxm6m4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Get and prepare the text"],"metadata":{"id":"fEFbCZ8FLEu8"}},{"cell_type":"code","source":["# get raw text from internet\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","\n","# character strings to replace with space\n","strings2replace = [ '\\r\\n\\r\\nâ\\x80\\x9c','â\\x80\\x9c','â\\x80\\x9d','\\r\\n','â\\x80\\x94','â\\x80\\x99','â\\x80\\x98','_', ]\n","\n","# use regular expression (re) to replace those strings with space\n","for str2match in strings2replace:\n","  text = re.compile(r'%s'%str2match).sub(' ',text)\n","\n","# remove non-ASCII characters and numbers, and make lower-case\n","text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n","text = re.sub('\\d+','',text).lower()"],"metadata":{"id":"EPRfkKgHLEsE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split into words that contain >1 character\n","words = re.split(f'[{string.punctuation}\\s]+',text)\n","words = [item.strip() for item in words if item.strip()]\n","words = [item for item in words if len(item)>1]\n","\n","# create the vocab / lexicon\n","vocab = sorted(set(words))\n","nWords = len(words)\n","nLex = len(vocab)"],"metadata":{"id":"YM_OBLi7zptg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the encoder/decoding mapping dictionaries\n","word2idx = {w:i for i,w in enumerate(vocab)}\n","idx2word = {i:w for i,w in enumerate(vocab)}"],"metadata":{"id":"Belr9eoCLTcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create encoder and decoder functions\n","def encoder(words,encode_dict):\n","\n","  # loop through the words and find their token in the vocab\n","  idxs = np.zeros(len(words),dtype=int)\n","  for i,w in enumerate(words):\n","    idxs[i] = encode_dict[w]\n","  return idxs\n","\n","# and the decoder function\n","def decoder(idxs,decode_dict):\n","  return ' '.join([decode_dict[i] for i in idxs])"],"metadata":{"id":"2HYTeIawLEOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7R9AB9nXY3Ru"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: A random walk through the Time Machine"],"metadata":{"id":"O59NxW2pY3U2"}},{"cell_type":"code","source":["# random tokens\n","randomTokens = np.random.randint(0,len(vocab),10)\n","\n","# test with random token indices\n","print(f'Random tokens: \\n\\t{randomTokens}\\n')\n","print(f'Decoded text: \\n\\t{decoder(randomTokens,idx2word)}')"],"metadata":{"id":"MpnKCGlNYyKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A brief aside on Brownian noise\n","brownNoise = np.cumsum(np.random.choice([-1,1],3000))\n","\n","plt.figure(figsize=(10,3))\n","plt.plot(brownNoise,'k')\n","plt.gca().set(xlim=[0,len(brownNoise)],xlabel='\"Time\" (?)',ylabel='Signal amplitude',title='Brownian noise')\n","plt.show()"],"metadata":{"id":"dLNWiOh5V2zo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Brownian noise\n","brownNoise = np.cumsum(np.random.choice([-1,1],30))\n","print(brownNoise)\n","\n","BrownianRandomTokens = brownNoise + np.random.choice(nLex,1)\n","print(BrownianRandomTokens)\n","print('')\n","\n","# test with random token indices\n","print(f'Brownian random tokens: \\n\\t{BrownianRandomTokens}\\n')\n","print(f'Decoded text: \\n\\t{decoder(BrownianRandomTokens,idx2word)}')"],"metadata":{"id":"cIpqrE5rKGbe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gEzmF7q5LXeD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Distribution of word lengths"],"metadata":{"id":"NXFIvKbFLXa1"}},{"cell_type":"code","source":["# loop through the words and count the characters per word\n","numChars = np.zeros(nWords)\n","for i,w in enumerate(words):\n","  numChars[i] = len(w)\n","\n","# now count the number of words with those characters\n","charCounts = np.zeros(int(np.max(numChars)))\n","for i in range(len(charCounts)):\n","  charCounts[i] = np.sum(numChars==i)\n","\n","\n","# and plot\n","_,axs = plt.subplots(2,1,figsize=(10,7))\n","axs[0].scatter(range(nWords),numChars,marker='.',s=10,c=np.linspace(.1,.9,len(numChars)),alpha=.4)\n","axs[0].set(yticks=range(1,int(np.max(numChars))),xlabel='Token index',xlim=[-15,nWords+15],\n","           ylabel='Number of characters',title='Character count by token index')\n","\n","axs[1].bar(range(len(charCounts)),charCounts,edgecolor='k',color=[.9,.7,.9])\n","axs[1].set(xticks=range(1,len(charCounts)),xlim=[0,len(charCounts)],xlabel='Number of characters',\n","           ylabel='Token count',title='Histogram of character count frequencies')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"YFZwHX25MQuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Gcbcj7JsLXX6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Encode a novel sentence"],"metadata":{"id":"SJacP80VRwbV"}},{"cell_type":"code","source":["# the text to decode\n","sentence = 'The space aliens came to Earth to steal watermelons and staplers.'\n","\n","# preprocess (remove punctuation, make lower-case, split into words)\n","words_new = re.split(f'[,.\\s]+',sentence.lower())\n","\n","# remove empty items\n","words_new = [item.strip() for item in words_new if item.strip()]\n","words_new"],"metadata":{"id":"unIc2KwtRaGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize (uh oh...)\n","encoder(words_new,word2idx)"],"metadata":{"id":"nVrNpUxeVQgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pkZSVYVbgJE6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Create a new encoder"],"metadata":{"id":"GzV3-uvcgJCh"}},{"cell_type":"code","source":["# need to update the vocab\n","word2idx_new = word2idx.copy()\n","idx2word_new = idx2word.copy()\n","\n","# add an entry for unknown words\n","word2idx_new['<|unk|>'] = len(word2idx)+1\n","idx2word_new[len(idx2word)+1] = '<|unk|>'"],"metadata":{"id":"z_8CpZRLRaEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# need a new encoder function\n","def encoder_new(words,encode_dict):\n","\n","  # initialize a vector of numerical indices\n","  idxs = np.zeros(len(words),dtype=int)\n","\n","  # loop through the words and find their token in the vocab\n","  for i,w in enumerate(words):\n","    if w in encode_dict:\n","      idxs[i] = encode_dict[w]\n","    else:\n","      idxs[i] = encode_dict['<|unk|>']\n","\n","  # return the results!\n","  return idxs\n","\n","  # note: could use list-comp:\n","  #return np.array([ encode_dict[w] if w in encode_dict else encode_dict['<|unk|>'] for w in words ])"],"metadata":{"id":"1u6Vb4_SdDiN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try again\n","tokenidx = encoder_new(words_new,word2idx_new)\n","tokenidx"],"metadata":{"id":"H6TZbghCRaBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# need a new decoder function?\n","decoder(tokenidx,idx2word_new)"],"metadata":{"id":"dKDT_rlPRZ8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AFEeR0URLXSd"},"execution_count":null,"outputs":[]}]}