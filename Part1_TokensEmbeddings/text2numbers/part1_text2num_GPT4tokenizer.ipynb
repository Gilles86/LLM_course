{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP5pyA9+pTsYfvQ6VmRqRSO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Exploring ChatGPT4's tokenizer<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"UnNdFahh7-8z"}},{"cell_type":"code","source":[],"metadata":{"id":"K1UFBQ9o7-6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8JzcByJ_dVt"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# matplotlib defaults\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# need to install the tiktoken library to get OpenAI's tokenizer\n","# note: it's tik-token, not tiktok-en :P\n","!pip install tiktoken\n","import tiktoken"],"metadata":{"id":"O-rylt4G8MWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vll_zH5t_gHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPT-4's tokenizer\n","tokenizer = tiktoken.get_encoding('cl100k_base')\n","dir(tokenizer)"],"metadata":{"id":"f34QAbA-_gD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get help\n","tokenizer??"],"metadata":{"id":"D-ehv3aujvgh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vocab size\n","tokenizer.n_vocab"],"metadata":{"id":"y3wREWipBBH0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.decode([tokenizer.eot_token])"],"metadata":{"id":"pIox41g_BBFS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# but not all tokens are valid, e.g.,\n","print(tokenizer.n_vocab)\n","tokenizer.decode([100261])"],"metadata":{"id":"aQex4QV7ZC8D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of all tokens:\n","# https://github.com/vnglst/gpt4-tokens/blob/main/decode-tokens.ipynb"],"metadata":{"id":"UaGtXMdoZDAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sTBxIVVoBBCw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Explore some tokens"],"metadata":{"id":"-2UiNQED81ig"}},{"cell_type":"code","source":["for i in range(1000,1050):\n","  print(f'{i} = {tokenizer.decode([i])}')"],"metadata":{"id":"aKLYdHUiLalD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"c-0XBTjgLahX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenization!"],"metadata":{"id":"W54bWoRcF6fd"}},{"cell_type":"code","source":["text = \"My name is Mike and I like toothpaste-flavored chocolate.\"\n","tokens = tokenizer.encode(text)\n","print(tokens)"],"metadata":{"id":"EQoW6wxE_gBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text.split()"],"metadata":{"id":"SVemggKqXyQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for word in text.split():\n","  print(f'\"{word}\" comprises token(s) {tokenizer.encode(word)}')"],"metadata":{"id":"9wt9WuiI_f-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for t in tokens:\n","  print(f'Token {t:>6} is \"{tokenizer.decode([t])}\"')"],"metadata":{"id":"OwUd-Jry_f7o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# with special (non-ASCII) characters\n","tokenizer.encode('â')"],"metadata":{"id":"V86VfNz3_f4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"73ZpWc09F2yT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# How long are the tokens?"],"metadata":{"id":"v28C8R_gF4XK"}},{"cell_type":"code","source":["# initialize lengths vector\n","token_lengths = np.zeros(tokenizer.n_vocab)\n","\n","# get the number of characters in each token\n","for idx in range(tokenizer.n_vocab):\n","  try:\n","    token_lengths[idx] = len(tokenizer.decode([idx]))\n","  except:\n","    token_lengths[idx] = np.nan\n","\n","# count unique lengths\n","uniqueLengths,tokenCount = np.unique(token_lengths,return_counts=True)\n","\n","\n","\n","# visualize\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","axs[0].plot(token_lengths,'k.',markersize=3,alpha=.4)\n","axs[0].set(xlim=[0,tokenizer.n_vocab],xlabel='Token index',ylabel='Token length (characters)',\n","           title='GPT4 token lengths')\n","\n","axs[1].bar(uniqueLengths,tokenCount,color='k',edgecolor='gray')\n","axs[1].set(xlim=[0,max(uniqueLengths)],xlabel='Token length (chars)',ylabel='Token count (log scale)',\n","           title='Distribution of token lengths')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"HN2EMOYi6ouB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E9WjkvGzDa65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Many word-tokens start with spaces"],"metadata":{"id":"shSBSrGSoHL3"}},{"cell_type":"code","source":["# single-token words with vs. without spaces\n","print( tokenizer.encode(' Michael') )\n","print( tokenizer.encode('Michael') )"],"metadata":{"id":"4Q18PZz3oK1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# multi-token words without a space\n","print( tokenizer.encode(' Peach') )\n","print( tokenizer.encode('Peach') )"],"metadata":{"id":"ul4qzL0VoUqP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["peach = tokenizer.encode('Peach')\n","[tokenizer.decode([p]) for p in peach]"],"metadata":{"id":"Y-YLajcooHJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"41SHGKIsoHGF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The Time Machine book encoded"],"metadata":{"id":"nsfMaYxen8oU"}},{"cell_type":"code","source":["import requests\n","import re\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","\n","# split by punctuation\n","words = re.split(r'([,.:;—?_!\"“()\\']|--|\\s)',text)\n","words = [item.strip() for item in words if item.strip()]\n","print(f'There are {len(words)} words.')\n","words[10000:10050]"],"metadata":{"id":"b-oWizBu_fzO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokens of a random word in the text\n","someRandomWord = np.random.choice(words)\n","print(f'\"{someRandomWord}\" has token {tokenizer.encode(someRandomWord)}')"],"metadata":{"id":"aLR0jdSclljl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for t in words[:20]:\n","  print(f'\"{t}\" has {len(tokenizer.encode(t))} tokens')"],"metadata":{"id":"D39CEFi__fwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for spelling in ['book','Book','bOok']:\n","  print(f'\"{spelling}\" has tokens {tokenizer.encode(spelling)}')"],"metadata":{"id":"BkP7Fdcg_ftn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v_8A_ey6r8qL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# But do we need to separate the text into words?"],"metadata":{"id":"BVvhTKdDHISD"}},{"cell_type":"code","source":["# what happens if we just tokenize the raw (unprocessed) text?\n","tmTokens = tokenizer.encode(text)\n","print(f'The text has {len(tmTokens):,} tokens and {len(words):,} words.')"],"metadata":{"id":"ntdIA6qMr8lk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check out some tokens\n","\n","for t in tmTokens[9990:10020]:\n","  print(f'Token {t:>6}: \"{tokenizer.decode([t])}\"')"],"metadata":{"id":"3xQRRWpfHQG2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode(tmTokens[9990:10020]))"],"metadata":{"id":"WE4HmJ8JBXCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zZdw1sP8IVuz"},"execution_count":null,"outputs":[]}]}