{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPmDafzaGUTDQyd44z4n9E9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Parsing text to numbered tokens<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"dBsNRoJ8HpGk"}},{"cell_type":"code","source":[],"metadata":{"id":"KDVo9UpRHuaY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Parsing text into words"],"metadata":{"id":"mcl6mvSJ0z6H"}},{"cell_type":"code","source":["# the text\n","text = [ 'All that we are is the result of what we have thought',\n","         'To be or not to be that is the question',\n","         'Be yourself everyone else is already taken' ]\n","\n","text"],"metadata":{"id":"RmFWYsyFHuXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# separate into words by splitting by spaces\n","import re\n","re.split('\\s',text[0])"],"metadata":{"id":"83q3ZHR6HuSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# can recombine into a text\n","' '.join( re.split('\\s',text[0]) )"],"metadata":{"id":"F3xIl0TZHuPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# also make lower-case\n","allwords = re.split('\\s',' '.join(text).lower())\n","allwords"],"metadata":{"id":"5jMgUhoFJFCo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xWjXaGF40-Y2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create a vocabulary (lexicon)"],"metadata":{"id":"p3vIFNqe0-V7"}},{"cell_type":"code","source":["# find the unique words\n","vocab = sorted(set(allwords))\n","vocab"],"metadata":{"id":"Xt6qDnGvHuNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'There are {len(allwords)} words in the text, and {len(vocab)} words in the vocabulary')"],"metadata":{"id":"bJIoqHSLHuHL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1LNdk--11DH9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create an encoder and decoder"],"metadata":{"id":"SQOK9BE31DDd"}},{"cell_type":"code","source":["# the encoder is a python dictionary type\n","word2idx = {}\n","for i,word in enumerate(vocab):\n","  word2idx[word] = i\n","word2idx"],"metadata":{"id":"RQk8dGyBHuEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and a decoder\n","idx2word = {}\n","for i,word in enumerate(vocab):\n","  idx2word[i] = word\n","idx2word"],"metadata":{"id":"PqKALTBaHuBa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'The word \"to\" has index {word2idx[\"to\"]}')\n","print(f'The index \"7\" maps to the word \"{idx2word[7]}\"')"],"metadata":{"id":"NTR0BMxdHt-l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P1VfwvVt1NUz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make fake quotes, just for fun :P"],"metadata":{"id":"GULhYnOP1NSA"}},{"cell_type":"code","source":["# select random words from the dictionary\n","import numpy as np\n","randidx = np.random.randint(0,len(vocab),size=5)\n","\n","# words of wisdom as a list of tokens\n","[ idx2word[i] for i in randidx ]"],"metadata":{"id":"QGCwhK-GHt8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# does it sound more wise as text??\n","' '.join([ idx2word[i] for i in randidx ])"],"metadata":{"id":"d0QzTDjSUU-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0StRY37I1dwU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# A peak at tokenization"],"metadata":{"id":"_X2k966q1dpW"}},{"cell_type":"code","source":["# translate the text into numbers\n","text_as_int = [ word2idx[word] for word in allwords ]\n","text_as_int"],"metadata":{"id":"rLBJ8ijaL5um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and numbers back into text\n","for tokeni in text_as_int:\n","  print(f'Token {tokeni:2}: {idx2word[tokeni]}')"],"metadata":{"id":"D9VtmaEb15kX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"beWoY-qYOyLn"},"execution_count":null,"outputs":[]}]}