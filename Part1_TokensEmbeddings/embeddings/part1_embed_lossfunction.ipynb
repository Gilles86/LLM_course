{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPIpVixOXuDkkHUkEliE1pT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Embedding spaces<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Loss function to train the embeddings<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"SnHUa7KXeu73"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"jjBeo5PWsofE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3KdT0HWlsoa7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the figure in the slides\n","p = np.linspace(.0000001,.9,500)\n","\n","plt.figure(figsize=(8,3))\n","plt.plot(p,p,label='p',linewidth=2)\n","plt.plot(p,np.log(p),label='log(p)',linewidth=2)\n","plt.gca().set(xlabel='Probability',ylabel='Loss value',xlim=[0,p[-1]])\n","\n","plt.legend(fontsize=13)\n","plt.show()"],"metadata":{"id":"ZKRc-9aIGBh5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rh0J8UVSJesG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HU06kyWmre5Q"},"outputs":[],"source":["# create a loss function instance\n","loss_function = nn.NLLLoss()\n","dir(loss_function)"]},{"cell_type":"code","source":["# start with three outputs (raw model outputs for three tokens in the vocab)\n","model_output = torch.tensor([[ -1, 2.3, .1 ]],dtype=torch.float64)\n","print('Raw model outputs:')\n","print(f'  {model_output[0].tolist()}\\n')\n","\n","# NLLLoss expects log-softmax inputs!\n","logsoftmax_output = F.log_softmax(model_output,dim=-1)\n","print('Log-softmax model outputs:')\n","print('  ',[round(o.item(),2) for o in logsoftmax_output[0] ],'\\n')\n","\n","\n","# check the loss for different targets\n","for target in range(len(model_output[0])):\n","\n","  # which output is the target (correct response)?\n","  target = torch.tensor([target])\n","\n","  # calculate the loss\n","  theloss = loss_function(logsoftmax_output,target)\n","\n","  # and print\n","  print(f'When the correct output is index \"{target.item()}\", the loss is {theloss.item():.2f}')"],"metadata":{"id":"0Uh9evVlsrP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ch8K3UpF5gYy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multi-sample losses (for batches)"],"metadata":{"id":"p9il2PU7srdU"}},{"cell_type":"code","source":["# let's create four batches\n","batch_output = model_output.repeat(4,1)\n","print(batch_output,'\\n')\n","\n","# but of course we need logsoftmax\n","logsoftmax_output = F.log_softmax(batch_output,dim=1)\n","print(logsoftmax_output)"],"metadata":{"id":"PD1Sb7Vi35Ru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# target changes for each batch\n","targets = torch.tensor([0,1,2,0])"],"metadata":{"id":"qoiDN7wh35Ma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = loss_function(logsoftmax_output,targets)\n","print(loss)"],"metadata":{"id":"wTvLk_v-35I2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# again, manual calculation\n","-torch.tensor([ -3.4377 + -0.1377 + -2.3377 + -3.4377 ]) / len(targets)"],"metadata":{"id":"yr2ymFDi35Fs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss.backward()"],"metadata":{"id":"NYuOiOea35AL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"utzbRHU68xpB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Simple example in pytorch"],"metadata":{"id":"4pscg2mW8xmM"}},{"cell_type":"code","source":["# define a weight matrix (requires_grad=True to track gradients)\n","w = torch.tensor([[-1,.2]], requires_grad=True)\n","\n","# target category\n","target = torch.tensor([0])\n","\n","# optimizer\n","optimizer = torch.optim.SGD([w],lr=.5)\n","\n","# training iterations\n","numTrainingIters = 10\n","\n","# initialize some variables\n","allWeights = torch.zeros((numTrainingIters+1,2))\n","allWeights[0,:] = w.detach()\n","allLosses = torch.zeros(numTrainingIters)\n","\n","# training loop\n","for i in range(numTrainingIters):\n","\n","  # reset gradients\n","  optimizer.zero_grad()\n","\n","  # model outputs (simulating a full model forward pass ;)  )\n","  modeloutput = F.log_softmax(w,dim=1)\n","\n","  # loss\n","  loss = loss_function(modeloutput,target)\n","  allLosses[i] = loss.item()\n","\n","  # gradient descent\n","  loss.backward()  # calculate gradient of loss wrt w\n","  optimizer.step() # adjust w using SGD\n","\n","  # store the new weights\n","  allWeights[i+1,:] = w.detach()\n","\n","  # and print out some results\n","  print(f\"Step {i+1:2d}: loss = {loss.item():.3f}, weights = {[round(o.item(),2) for o in w[0] ]}\")"],"metadata":{"id":"9q02adnB8xjn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's see the weights!\n","_,axs = plt.subplots(1,2,figsize=(10,3))\n","\n","\n","axs[0].plot(allLosses,'ks-',linewidth=1,markerfacecolor=[.9,.7,.7])\n","axs[0].set(xlabel='Training epochs',ylabel='Loss value',title='Losses during training')\n","\n","axs[1].plot(allWeights[:,0],'ks-',markerfacecolor=[.7,.9,.7],linewidth=1,label='Weight 0 (target)')\n","axs[1].plot(allWeights[:,1],'ko-',markerfacecolor=[.7,.7,.9],linewidth=1,label='Weight 1 (non-target)')\n","axs[1].set(xlabel='Training epochs',ylabel='Weight value',title='Weight values')\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"owOAhjsa8xg3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bp6LiJM18xYm"},"execution_count":null,"outputs":[]}]}