{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNUhb92D+su3/7gG7dToi2M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Embedding spaces<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Position embeddings<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"3UI_HsUuYM7E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qliPUVxwuvWm"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# higres plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"mmR6IaAwvI_T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import GPT-2 model and extract its position embedding matrix"],"metadata":{"id":"P9S0TmkovI8n"}},{"cell_type":"code","source":["# load the model\n","from transformers import GPT2Model\n","gpt2 = GPT2Model.from_pretrained('gpt2')\n","gpt2"],"metadata":{"id":"B7PftXi1u5nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# position embeddings\n","positions = gpt2.wpe.weight.detach().numpy()"],"metadata":{"id":"LQQmg-2nHoFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the size of this matrix\n","positions.shape"],"metadata":{"id":"Kvm9IuxQTAEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize the matrix\n","plt.figure(figsize=(10,4))\n","plt.imshow(positions.T,aspect='auto',vmin=-.2,vmax=.2)\n","plt.gca().set(xlabel='Token position',ylabel='Dimensions',title='GPT-2 position embedding matrix')\n","plt.show()"],"metadata":{"id":"vBlk2abVLSkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zmreAbQIB5eG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize some position vectors"],"metadata":{"id":"efyz7oI4B5bZ"}},{"cell_type":"code","source":["_,axs = plt.subplots(3,4,figsize=(16,6))\n","\n","# pick random vectors\n","for a in axs.flatten():\n","\n","  # a random position embedding vector\n","  randidx = np.random.randint(positions.shape[1])\n","\n","  # and plot it\n","  a.plot(positions[:,randidx],'k',label=f'Position index {randidx}')\n","  a.axhline(0,linestyle='--',color='gray',zorder=-3)\n","\n","  a.set(xticks=[],yticks=[0],xlim=[0,positions.shape[0]])\n","  a.legend(fontsize=10)\n","\n","\n","# x-axis label on one plot\n","a.set_xlabel('Start of text <---> end of context window',fontsize=12)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"LWoe9X68LXyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_TRPFIc1alGb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Similarities across vectors"],"metadata":{"id":"wqgrX2S0CLRN"}},{"cell_type":"code","source":["# cosine similarities for \"time series\" (token index)\n","Pnorm1 = positions / np.linalg.norm(positions,axis=1,keepdims=True)\n","cossim_tokens = Pnorm1 @ Pnorm1.T\n","\n","# cosine similarities across embedding dimensions\n","Pnorm0 = positions / np.linalg.norm(positions,axis=0,keepdims=True)\n","cossim_embeds = Pnorm0.T @ Pnorm0\n","\n","\n","# draw the images\n","fig,axs = plt.subplots(1,2,figsize=(12,5))\n","\n","h = axs[0].imshow(cossim_tokens,vmin=-1,vmax=1)\n","axs[0].set(xlabel='Token index (\"time\")',ylabel='Token index (\"time\")',title='$S_c$ over \"time\"')\n","ch = fig.colorbar(h,ax=axs[0],pad=.02,fraction=.046)\n","ch.ax.tick_params(labelsize=10)\n","ch.ax.set_yticks(np.arange(-1,1.1,.5))\n","\n","h = axs[1].imshow(cossim_embeds,vmin=-1,vmax=1)\n","axs[1].set(xlabel='Embedding index',ylabel='Embedding index',title='$S_c$ across embeddings')\n","ch = fig.colorbar(h,ax=axs[1],pad=.02,fraction=.046)\n","ch.ax.tick_params(labelsize=10)\n","ch.ax.set_yticks(np.arange(-1,1.1,.5))\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"70q51F4ealD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"On3w83Bpl1MJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sinusoidal embeddings as defined in \"Attention\" paper"],"metadata":{"id":"JFhdDmfjM2Q3"}},{"cell_type":"code","source":["positionsFormula = np.zeros_like(gpt2.wpe.weight.data)\n","d = positionsFormula.shape[1]\n","\n","# token position (\"time\")\n","th = np.arange(positionsFormula.shape[0])\n","\n","# create the vectors\n","for i in range(0,positionsFormula.shape[1],2):\n","\n","  # denominator scaling factor\n","  denom = 10000 ** (2*i//2 / d)\n","\n","  # define the embeddings\n","  positionsFormula[:,i]   = np.sin(th / denom)\n","  positionsFormula[:,i+1] = np.cos(th / denom)\n","\n","\n","\n","#### and visualize\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","axs[0].imshow(positionsFormula.T,vmin=-1,vmax=1)\n","axs[0].set(ylabel='Embedding dimensions',xlabel='Token order (\"time\")',title='All position embeddings')\n","\n","pos2show = np.linspace(200,600,4,dtype=int)\n","h = axs[1].plot(positionsFormula[:,pos2show])\n","axs[1].set(ylabel='Weight value',xlabel='Token order (\"time\")',xlim=[0,len(th)],title='A few position embeddings')\n","\n","for i,p in enumerate(pos2show):\n","  axs[0].axhline(p,linestyle='--',color=h[i].get_color(),linewidth=1.8)\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"FXHJ2ayhPomD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample plot as earlier with the learned embeddings\n","\n","_,axs = plt.subplots(3,4,figsize=(16,6))\n","\n","# pick random vectors\n","for a in axs.flatten():\n","\n","  # a random position embedding vector\n","  randidx = np.random.randint(positions.shape[1])\n","\n","  # and plot it\n","  a.plot(positionsFormula[:,randidx],'k',label=f'Position index {randidx}')\n","  a.axhline(0,linestyle='--',color='gray',zorder=-3)\n","\n","  a.set(xticks=[],yticks=[0],xlim=[0,positions.shape[0]])\n","  a.legend(fontsize=10)\n","\n","\n","# x-axis label on one plot\n","a.set(xlabel='<-- present     ...     past -->')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"LBV_dwYJM2AA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"90koL_4kJvEi"},"execution_count":null,"outputs":[]}]}