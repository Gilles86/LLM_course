{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPuBCjkAR2Ubybvp99oTmiS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Embedding spaces<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Math with tokens and embeddings<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"En26hadeHQh8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qliPUVxwuvWm"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","# highres plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"mmR6IaAwvI_T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import GPT-2 model and extract its embeddings matrix"],"metadata":{"id":"P9S0TmkovI8n"}},{"cell_type":"code","source":["from transformers import GPT2Model,GPT2Tokenizer\n","\n","# pretrained GPT-2 model and tokenizer\n","gpt2 = GPT2Model.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","embeddings = gpt2.wte.weight.detach().numpy()"],"metadata":{"id":"B7PftXi1u5nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"woV0vKYvxEp0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Numbers to tokens"],"metadata":{"id":"nS7tgWrXK_UX"}},{"cell_type":"code","source":["# create some numbers\n","numbers = np.arange(11)\n","numbers = np.concatenate( (numbers,10*numbers[2:],100*numbers[2:]), axis=0)\n","\n","# initialize token vector\n","numTokenLabels = np.zeros(len(numbers))\n","\n","# get and report the tokens\n","for i,n in enumerate(numbers):\n","\n","  # get the first token for this number\n","  numTokenLabels[i] = tokenizer.encode(str(n))[0]\n","\n","  # try /2\n","  print(f'The number {n:5} is token(s) {tokenizer.encode(str(n))}')"],"metadata":{"id":"aBRNtefzHarZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","plt.plot(numbers,numTokenLabels,color=[.5,.5,.5],linewidth=.5)\n","plt.scatter(numbers,numTokenLabels,c=np.arange(len(numbers)),s=100,marker='s',cmap='plasma_r',zorder=10)\n","\n","plt.gca().set(xlabel='Number (as string)',ylabel='Token value',xticks=numbers,xlim=[numbers[0]-15,numbers[-1]+15])\n","# plt.gca().set(xscale='log')\n","plt.show()"],"metadata":{"id":"etpI6w9iG5SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ercDYIu34eRy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: How long are numbers?"],"metadata":{"id":"TKEw1dR54eOm"}},{"cell_type":"code","source":["# tokenize integers and floating-point numbers\n","\n","# initialize\n","numnums = 99_999\n","int_toklens = np.zeros(numnums,dtype=int)\n","float_toklens = np.zeros(numnums,dtype=int)\n","\n","# random numbers\n","ra = 5*np.random.randn(numnums)\n","\n","for i in range(numnums):\n","\n","  # integers\n","  int_toklens[i] = len(tokenizer.encode(str(i)))\n","\n","  # and the random numbers\n","  float_toklens[i] = len(tokenizer.encode(str(ra[i])))"],"metadata":{"id":"R-yuiGMLxEhI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","axs[0].plot(int_toklens+np.random.randn(numnums)/30,'s',markerfacecolor=[.7,.7,.9],alpha=.4)\n","axs[1].plot(ra,float_toklens+np.random.randn(numnums)/50,'o',markerfacecolor=[.7,.9,.7],alpha=.4)\n","\n","axs[0].set(xlabel='Number',ylabel='Token length',yticks=range(int_toklens.max()+2),title='Token lengths of integers')\n","axs[1].set(xlabel='Number',ylabel='Token length',title='Token lengths of floating-point numbers')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"V3-dT-hZxEeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cjyf950ZHUV8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Does math work in token conversions?"],"metadata":{"id":"Mzr_9_SUM40Z"}},{"cell_type":"code","source":["# the equation and its tokens\n","eq = '5 x 3 ='\n","tokens = tokenizer.encode(eq)\n","\n","# try the math\n","print(f'{eq} -> {tokens}')\n","print(f'Product of tokens = {np.prod(tokens)}')\n","print(f'   which is \"{tokenizer.decode(np.prod(tokens))}\"')"],"metadata":{"id":"mHBWoVIVHVXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# maybe just the numbers?\n","t5 = tokenizer.encode('5')\n","t3 = tokenizer.encode('3')\n","\n","# let's see...\n","print(f'\"5\" and \"3\" have tokens {t5} and {t3}.')\n","print(f'Their product is {t5[0]*t3[0]}, which is \"{tokenizer.decode(t5[0]*t3[0])}\"')"],"metadata":{"id":"xZmtvC1MHVTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8EHF-nvzHVQm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: How about embeddings?"],"metadata":{"id":"qbt1bUuRHVNa"}},{"cell_type":"code","source":["# isolate the embedding vectors\n","e5 = embeddings[t5,:].squeeze()\n","e3 = embeddings[t3,:].squeeze()\n","\n","# math\n","theirSum  = e3+e5\n","theirProd = e3*e5\n","\n","# plot the vectors\n","plt.figure(figsize=(12,4))\n","\n","plt.plot(e3,label='3')\n","plt.plot(e5,label='5')\n","plt.plot(theirSum,label='3+5')\n","plt.plot(theirProd,label='3x5')\n","\n","plt.gca().set(xlabel='Embeddings dimension',ylabel='Value',xlim=[0,len(e3)])\n","plt.legend()\n","plt.show()"],"metadata":{"id":"R1MuPIGfHVIG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# unembedding as transpose of embeddings\n","sumUnembedding  = theirSum @ embeddings.T\n","prodUnembedding = theirProd @ embeddings.T\n","\n","# print sizes\n","print(f'Summed vector X embeddings matrix = unembeddings vector')\n","print(f'    {theirSum.shape}    X    {embeddings.shape}   =  {sumUnembedding.shape}\\n')\n","\n","# find the argmax output\n","print(f'Max embedding of 5+3 = \"{tokenizer.decode(np.argmax(sumUnembedding))}\"')\n","print(f'Max embedding of 5x3 = \"{tokenizer.decode(np.argmax(prodUnembedding))}\"')"],"metadata":{"id":"4QyaeAsQHVFR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# curious to see how the embeddings for '3' and '5' compare\n","plt.plot(e5,e3,'.')\n","plt.show()"],"metadata":{"id":"FrbndR-pHU6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XlWJy3ESk6Yk"},"execution_count":null,"outputs":[]}]}