{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOafhOeDeo7I0Hc/BpLBSkq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Embedding spaces<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Wikipedia vs. Twitter embeddings<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"8Z06zLxr6vIl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2l85Wel2qa04"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","# svg figure format\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"KsGto4rz1zvZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Download and inspect the models"],"metadata":{"id":"5onBwtyv1zsU"}},{"cell_type":"code","source":["# NOTE: If you get errors importing, run the following !pip... line,\n","# then restart your session (from Runtime menu) and comment out the pip line.\n","# !pip install gensim\n","\n","import gensim.downloader as api\n","\n","# download the wikipedia and twitter models\n","wiki = api.load('glove-wiki-gigaword-50')\n","twit = api.load('glove-twitter-50')"],"metadata":{"id":"xh3PJNtPqnPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir(twit)"],"metadata":{"id":"_ac3JrC4qnMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"A-FwRwHW1387"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embedding matrix dimensions\n","print(f'Wikipedia model has {wiki.vectors.shape[0]:,} words and {wiki.vectors.shape[1]} embedding dimensions.')\n","print(f'Twitter model has {twit.vectors.shape[0]:,} words and {twit.vectors.shape[1]} embedding dimensions.')"],"metadata":{"id":"I6z0HLDKq67A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LtCRoY2mqnJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Visualize the embeddings for one word"],"metadata":{"id":"nsIJFUOgqnGG"}},{"cell_type":"code","source":["targetword = 'table'\n","\n","_,axs = plt.subplots(1,2,figsize=(12,4.5))\n","axs[0].plot(wiki[targetword],'ks',markerfacecolor=[.7,.7,.9],markersize=8,label='Wikipedia')\n","axs[0].plot(twit[targetword],'ko',markerfacecolor=[.7,.9,.7],markersize=8,label='Twitter')\n","axs[0].set(xlabel='Dimension',ylabel='Value',title=f'Embeddings for \"{targetword}\"')\n","axs[0].legend()\n","\n","axs[1].plot(wiki[targetword],twit[targetword],'k^',markerfacecolor=[.9,.7,.7],markersize=8)\n","axs[1].set(xlabel='Wiki embedding',ylabel='Twitter embedding',title=f'Embeddings for \"{targetword}\"')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"rJQ8cnIeyAKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nqCB1EAmqnDS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Embeddings for word pairs within each model"],"metadata":{"id":"-Qs7-sWLqnAV"}},{"cell_type":"code","source":["# word pair\n","word1 = 'table'\n","word2 = 'chair'\n","\n","# scatter plot for wiki\n","_,axs = plt.subplots(1,2,figsize=(12,4.5))\n","axs[0].plot(wiki[word1],wiki[word2],'ks',markersize=9,markerfacecolor=[.9,.7,.7])\n","axs[0].set(xlabel=f'Embedding for \"{word1}\"',ylabel=f'Embedding for \"{word2}\"',\n","           title=f'WIKI (Cosine similarity: {wiki.similarity(word1,word2):.3f})')\n","\n","\n","# scatter plot for twitter\n","axs[1].plot(twit[word1],twit[word2],'ko',markersize=9,markerfacecolor=[.7,.9,.7])\n","axs[1].set(xlabel=f'Embedding for \"{word1}\"',ylabel=f'Embedding for \"{word2}\"',\n","           title=f'TWITTER (Cosine similarity: {twit.similarity(word1,word2):.3f})')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"0FchbjGsy5XG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pSNWY7BMy5Z3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Similar words within each model"],"metadata":{"id":"zD0NV-mA2DAb"}},{"cell_type":"code","source":["print('10 words most similar to \"battery\" in wiki:')\n","for w,cs in wiki.most_similar('battery'):\n","  print(f' {w:>15} with similarity {cs:.4f}')\n","\n","print('\\nAnd in twitter:')\n","for w,cs in twit.most_similar('battery'):\n","  print(f' {w:>15} with similarity {cs:.4f}')"],"metadata":{"id":"0H2lzMaEy5ce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mnMsFM1Hqm9d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: foxes and dogs"],"metadata":{"id":"_pAAvm46NLVl"}},{"cell_type":"code","source":["text = 'The quick brown fox jumps over the lazy dog'\n","\n","import re\n","words = re.split('\\s',text)#.lower())\n","\n","# index sequence in the two embeddings\n","wiki_idx = [wiki.key_to_index[w] if w in wiki.key_to_index else np.Inf for w in words ]\n","twit_idx = [twit.key_to_index[w] if w in twit.key_to_index else np.Inf for w in words ]\n","\n","print(' Word |  Wiki | Twitter')\n","print('-'*23)\n","for o,w,t in zip(words,wiki_idx,twit_idx):\n","  print(f'{o:>5} | {w:>5} | {t:>5}')"],"metadata":{"id":"u5ed8-opIM-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get all unique inter-word similarities\n","\n","plt.figure(figsize=(9,7))\n","\n","# start range at 0 or 1?\n","for i in range(0,len(words)):\n","  for j in range(i+1,len(words)):\n","\n","    # skip identity\n","    if words[i]==words[j]: continue\n","\n","    # calculate the cosine similarities for the two embeddings\n","    cs_wiki = wiki.similarity(wiki_idx[i],wiki_idx[j])\n","    cs_twit = twit.similarity(twit_idx[i],twit_idx[j])\n","\n","    # calculate the distance to the unity line\n","    v = np.array([cs_wiki,cs_twit])\n","    u = np.array([1,1])\n","    dist = np.linalg.norm(v - (sum(v*u))/(np.linalg.norm(u)**2)*u)\n","\n","    # draw the results at the coordinates\n","    plt.plot(cs_wiki,cs_twit,'ks',markersize=9,markerfacecolor=mpl.cm.plasma(dist*5))\n","\n","    # and write the word pair\n","    plt.text(cs_wiki,cs_twit+.02,f'{words[i]}-{words[j]}',va='bottom',ha='center')\n","\n","\n","\n","# plot the unity line\n","xylims = [.05,.95]\n","plt.plot(xylims,xylims,'--',color=[.4,.4,.4],zorder=-30)\n","\n","# final adjustments\n","plt.gca().set(xlim=xylims,ylim=xylims,xlabel='Wiki inter-word similarities',\n","              ylabel='Twitter inter-word similarities',title='Inter-word similarities')\n","plt.show()"],"metadata":{"id":"FmDChKOzIM7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xKPvbC9_IM48"},"execution_count":null,"outputs":[]}]}