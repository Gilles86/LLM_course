{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPY7GrhDZ2Brz1AyZBJy0sY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Embedding spaces<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: GPT2 cosine similarities<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"N_qTdA1avZMb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qliPUVxwuvWm"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"mmR6IaAwvI_T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import GPT-2 model and extract its embedding matrix"],"metadata":{"id":"P9S0TmkovI8n"}},{"cell_type":"code","source":["from transformers import GPT2Model,GPT2Tokenizer\n","\n","# pretrained GPT-2 model and tokenizer\n","gpt2 = GPT2Model.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"B7PftXi1u5nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the Word Token Embeddings matrix\n","embeddings = gpt2.wte.weight.detach().numpy()"],"metadata":{"id":"Kvm9IuxQTAEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2MiHny290HfH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: The cosmic banana"],"metadata":{"id":"tB_x28mzGpAo"}},{"cell_type":"code","source":["# words of interest\n","word1 = 'banana'\n","word2 = 'apple'\n","word3 = 'cosmic'\n","\n","# decomposition\n","for w in [word1,word2,word3]:\n","  t = tokenizer.encode(w)\n","  print(f'\"{w}\" comprises {len(t)} tokens:\\n   {[tokenizer.decode(i) for i in t]}\\n')"],"metadata":{"id":"1fBkwC1b0HaH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup the figure subplot geometry\n","fig = plt.figure(figsize=(10,7))\n","gs = GridSpec(2,2)\n","ax0 = fig.add_subplot(gs[0,:])\n","ax1 = fig.add_subplot(gs[1,0])\n","ax2 = fig.add_subplot(gs[1,1])\n","\n","\n","# plot the embeddings by dimension\n","colors = 'krb'\n","linestyles = ['-','--','-.',':']\n","for idx,word in enumerate([word1,word2,word3]):\n","  wordidx = tokenizer.encode(word)\n","  for j in range(len(wordidx)):\n","    ax0.plot(embeddings[wordidx[j],:],linestyle=linestyles[j],color=colors[idx],label=tokenizer.decode(wordidx[j]))\n","\n","ax0.set(xlabel='Dimension',title='Embeddings',xlim=[-1,embeddings.shape[1]+1])\n","ax0.legend()\n","\n","\n","\n","# cosine similarity between 'ban' and 'ana'\n","v1 = embeddings[tokenizer.encode('ban')]\n","v2 = embeddings[tokenizer.encode('ana')]\n","cossim = np.sum(v1*v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n","\n","# plot the embeddings by each other\n","ax1.plot(v1,v2,'ko',markerfacecolor=[.7,.7,.7,.6])\n","ax1.set(xlim=[-.8,.8],ylim=[-.8,.8],xlabel='\"ban\" embedding',ylabel='\"ana\" embedding',\n","        title=f'Cosine similarity = {cossim:.3f}')\n","\n","\n","\n","# plot the \"cos\" for two endings\n","for s in ['mic','ine']:\n","\n","  # cosine similarity\n","  v1 = embeddings[tokenizer.encode('cos')].squeeze()\n","  v2 = embeddings[tokenizer.encode(s)].squeeze()\n","  cossim = np.sum(v1*v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n","\n","  # plot\n","  ax2.plot(v1,v2,marker=f'${s[0]}$',linestyle='none',alpha=.6,label=f'cos-{s} ($s_c$  = {cossim:.3f})')\n","\n","ax2.legend(fontsize=10)\n","ax2.set(xlim=[-.8,.8],ylim=[-.8,.8],xlabel='\"cos\" embedding',ylabel='\"mic\" or \"ine\" embedding')\n","\n","\n","# final touches\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"lLj1r8mEyVCV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"74XVBpgo0HXH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Find tokens with similar embeddings"],"metadata":{"id":"aIzrOtawVHOs"}},{"cell_type":"code","source":["# get seed index and embedding vector\n","seed = tokenizer.encode('Mike')\n","seedvect = embeddings[seed].squeeze()\n","seedvectNorm = np.linalg.norm(seedvect)\n","\n","# calculate cosine similarity to all other tokens in a for-loop\n","cossims = np.zeros(embeddings.shape[0])\n","for idx,v in enumerate(embeddings):\n","  cossims[idx] = np.sum(seedvect*v) / (seedvectNorm*np.linalg.norm(v))\n","\n","\n","\n","# and visualize\n","plt.figure(figsize=(10,4))\n","plt.plot(cossims,'k.',alpha=.6,markerfacecolor='w')\n","plt.gca().set(xlim=[-5,len(cossims)+5],xlabel='Token index',ylabel='Cosine similarity',\n","              title='Cosine similarity to \"Mike\"')\n","\n","\n","# top 20 largest cosine-similarities\n","top20 = np.argsort(cossims)[-20:]\n","for i in top20:\n","  plt.plot(i,cossims[i],'ms',markersize=4,alpha=.6)\n","\n","plt.show()"],"metadata":{"id":"2dQRyf1p0-o7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokens closest to \"Mike\"\n","for n in top20[::-1]:\n","  print(f'\"{tokenizer.decode(n)}\" with cosine similarity {cossims[n]:.3f}')"],"metadata":{"id":"PqMYDxkMIEY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"06bLEtYKu50_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: One-to-all cosine similarity via linear algebra"],"metadata":{"id":"R8n6aqlERGa1"}},{"cell_type":"code","source":["# normalize entire embedding matrix (in a new variable!)\n","Enorm = embeddings / np.linalg.norm(embeddings,axis=1,keepdims=True)\n","\n","# cosine similarity matrices\n","cossims2 = Enorm[seed] @ Enorm.T"],"metadata":{"id":"K3q1MuhtUdKq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Size of for-loop version:   {cossims.shape}')\n","print(f'Size of vectorized version: {cossims2.shape}')\n","\n","# remove singleton dimension\n","cossims2 = np.squeeze(cossims2)\n","\n","# inspect again\n","print('\\n** After squeezing:')\n","print(f'Size of for-loop version:   {cossims.shape}')\n","print(f'Size of vectorized version: {cossims2.shape}')"],"metadata":{"id":"SODhRj6WUqy0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","plt.plot(range(0,len(cossims),100),cossims[::100],'o',label='for-loop')\n","plt.plot(range(0,len(cossims),100),cossims2[::100],'+',label='vectorized')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Token idx (plotting every 100)',ylabel='Cossine similarity',title=f'Correlation = {np.corrcoef(cossims,cossims2)[0,1]:.3f}')\n","plt.show()"],"metadata":{"id":"jAw1vAKPU81h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8VEPQQ2Vu59z"},"execution_count":null,"outputs":[]}]}