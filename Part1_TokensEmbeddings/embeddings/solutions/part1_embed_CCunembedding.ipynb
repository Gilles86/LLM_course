{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPNwE+6apts+KrmNXBsvBKO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Embedding spaces<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Unembeddings (vectors to tokens)<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"cJ56VUGP2nQa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDFppSRRopBP"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"iRAzf_6L1ZY0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import GPT2 model and tokenizer, and get dimensions"],"metadata":{"id":"Gz9ZTkp51ZSy"}},{"cell_type":"code","source":["from transformers import GPT2Model,GPT2Tokenizer\n","\n","# pretrained GPT-2 model and tokenizer\n","gpt2 = GPT2Model.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"B7PftXi1u5nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embeddings matrix\n","embeddings = gpt2.wte.weight.detach().numpy()"],"metadata":{"id":"_DTHcukv6kO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the size parameters in .config\n","gpt2.config"],"metadata":{"id":"R5GMjGhH1f5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the properties we'll use later\n","print(f'Embedding dimensions: {gpt2.config.n_embd}')\n","print(f'Vocab size: {gpt2.config.vocab_size}')\n","print(f'Size of embeddings matrix: {embeddings.shape}')"],"metadata":{"id":"59Xe6als6kiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0i7XoGE91wKu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Real and random unembeddings"],"metadata":{"id":"c0gmlLRz6kVG"}},{"cell_type":"code","source":["# unembeddings matrix as the transpose of the (real) embeddings\n","unembeddings = embeddings.T\n","\n","# confirm that transposing matrix a copy\n","print('id of embeddings:  ',id(embeddings))\n","print('id of unembeddings:',id(unembeddings))"],"metadata":{"id":"bV0ppdoQ6kIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a random unembeddings matrix\n","unembeddingsRand = np.random.randn(gpt2.config.n_embd,gpt2.config.vocab_size)\n","\n","print(f'         Size of embeddings matrix: {embeddings.shape}')\n","print(f'Size of random unembeddings matrix: {unembeddingsRand.shape}')\n","print(f'  Size of real unembeddings matrix: {unembeddings.shape}')"],"metadata":{"id":"5rI2UTJE1zRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"94k69u0w2MFm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: California embedding"],"metadata":{"id":"TToyw0eL2MAh"}},{"cell_type":"code","source":["# pick a word\n","seedword = ' California'\n","\n","# its token index\n","seed_idx = tokenizer.encode(seedword)\n","\n","# make sure it's one token\n","seed_idx"],"metadata":{"id":"NV2cynlbJepg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find its embedding vector\n","embed_vector = embeddings[seed_idx,:]\n","\n","# plot it!\n","plt.figure(figsize=(10,3))\n","plt.scatter(range(gpt2.config.n_embd),embed_vector.squeeze(),s=30,c=abs(embed_vector),cmap='RdPu')\n","plt.gca().set(xlabel='Embedding dimension',ylabel='Embedding weight',xlim=[-3,gpt2.config.n_embd+2],\n","              title=f'Embedding (GPT2) of \"{tokenizer.decode(seed_idx)}\"')\n","plt.show()"],"metadata":{"id":"nWqt1Ks71Dpx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# project the embedding vector onto the unembedding matrix\n","dpRand = embed_vector @ unembeddingsRand\n","\n","# next token is the maximum dot product (unscaled cosine similarity)!\n","nextTokenRand_idx = np.argmax(dpRand)\n","nextTokenRand = tokenizer.decode(nextTokenRand_idx)\n","\n","# check the sizes\n","print('embed_vector  X  unembeddings  =  dotproducts')\n","print(f'  {embed_vector.shape}       {unembeddingsRand.shape}      {dpRand.shape}')"],"metadata":{"id":"DQRw7aceLdpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat for the real unembeddings matrix\n","dpReal = embed_vector @ unembeddings\n","nextTokenReal_idx = np.argmax(dpReal)\n","nextTokenReal = tokenizer.decode(nextTokenReal_idx)"],"metadata":{"id":"dg49K8OQ3ONM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('** Random unembeddings matrix:')\n","print(f'   \"{tokenizer.decode(seed_idx)}\" has largest dot product with token \"{nextTokenRand}\"\\n')\n","\n","print('** Real unembeddings matrix:')\n","print(f'   \"{tokenizer.decode(seed_idx)}\" has largest dot product with token \"{nextTokenReal}\"')"],"metadata":{"id":"oaRvqT0LJucr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot it!\n","_,axs = plt.subplots(1,2,figsize=(12,3))\n","axs[0].scatter(range(tokenizer.vocab_size),dpRand,s=30,c=abs(dpRand),cmap='RdPu',alpha=.4)\n","axs[0].axvline(nextTokenRand_idx,linestyle='--',color='k',alpha=1/3)\n","axs[0].plot(nextTokenRand_idx,dpRand[0,nextTokenRand_idx],'gv')\n","axs[0].set(xlabel='Unembedding dimension',ylabel='Dot product',xlim=[-11,tokenizer.vocab_size+10],\n","              title=f'(Random) dot products with \"{tokenizer.decode(seed_idx)}\"')\n","\n","axs[1].scatter(range(tokenizer.vocab_size),dpReal,s=30,c=abs(dpReal),cmap='RdPu',alpha=.4)\n","axs[1].axvline(nextTokenReal_idx,linestyle='--',color='k',alpha=1/3)\n","axs[1].plot(nextTokenReal_idx,dpReal[0,nextTokenReal_idx],'gv')\n","axs[1].set(xlabel='Unembedding dimension',ylabel='Dot product',xlim=[-11,tokenizer.vocab_size+10],\n","              title=f'(Real) dot products with \"{tokenizer.decode(seed_idx)}\"')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"pLDQ-RwGJejA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d3Bq4TwK6kYO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Find top-10 unembeddings"],"metadata":{"id":"c9PGqM69-8jf"}},{"cell_type":"code","source":["top10 = np.argsort(dpReal[0])[::-1][:10]\n","\n","for i in top10:\n","  print(f'Dot product {dpReal[0,i]:6.3f} for token \"{tokenizer.decode(i)}\"')"],"metadata":{"id":"UHQnvorp-8gn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tSQ7KDhm-8dw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Generate a token sequence"],"metadata":{"id":"5WdhW9Kh-8bI"}},{"cell_type":"code","source":["# sequence length\n","seq_len = 10\n","\n","# initial seed\n","nextword = 'budget'\n","\n","# initializing a list that will contain the text\n","text = nextword\n","\n","\n","# loop to create the sequence\n","for i in range(seq_len-1):\n","\n","  # step 1: tokenize\n","  token = tokenizer.encode(nextword)\n","\n","  # step 2: get embedding vector\n","  embed_vector = embeddings[token,:]\n","\n","  # step 3: project onto unembedding matrix (dot products)\n","  dp = embed_vector @ unembeddings\n","\n","  # step 4: find top10 projections\n","  top10 = np.argsort(dp[0])[::-1][:10]\n","\n","  # step 5: randomly pick one for next token\n","  aRandomToken = np.random.choice(top10)\n","  nextword = tokenizer.decode(aRandomToken)\n","\n","  # step 6: append the text\n","  text += nextword\n","\n","# print the final result!\n","print('Our very philosophically meaningful text:\\n',text)"],"metadata":{"id":"U6dH96C_-8YB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"itNx26ENxJOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Repeat with random unembeddings"],"metadata":{"id":"nmRfT3gJ3w4l"}},{"cell_type":"code","source":["# initialize\n","nextword = 'budget'\n","text = nextword\n","\n","# loop\n","for i in range(seq_len-1):\n","\n","  # step 1: tokenize\n","  token = tokenizer.encode(nextword)\n","\n","  # step 2: get embedding vector\n","  embed_vector = embeddings[token,:]\n","\n","  # step 3: project onto RANDOM unembedding matrix\n","  dp = embed_vector @ unembeddingsRand\n","\n","  # step 4: find top10 projections\n","  top10 = np.argsort(dp[0])[::-1][:10]\n","\n","  # step 5: randomly pick one for next token\n","  aRandomToken = np.random.choice(top10)\n","  nextword = tokenizer.decode(aRandomToken)\n","\n","  # step 6: append the text\n","  text += nextword\n","\n","print('Our randomly meanderingful text:\\n',text)"],"metadata":{"id":"Z6Yal2r9xJLZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1umxbFabzMg3"},"execution_count":null,"outputs":[]}]}