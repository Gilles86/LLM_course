{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oDaogKfz9gQYSAyQT-uF9xRI3EdrtokO","timestamp":1745863540476}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bhWV8oes-wKR"},"source":["|<h2>Course:</h2>|<h1><b><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></b></h1>|\n","|-|:-:|\n","|<h2>Part 8:</h2>|<h1>Deep learning intro<h1>|\n","|<h2>Section:</h2>|<h1>Math of deep learning<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Softmax<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"]},{"cell_type":"code","metadata":{"id":"2TD8IyfBGXiY"},"source":["# import libraries\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmjUxlEqGbDu"},"source":["# \"manually\" in numpy\n","\n","# the list of numbers\n","z = [1,2,3]\n","\n","# compute the softmax result\n","num = np.exp(z)\n","den = np.sum( np.exp(z) )\n","sigma = num / den\n","\n","print(sigma)\n","print(np.sum(sigma))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOug_tPzHY1y"},"source":["# repeat with some random integers\n","z = np.random.randint(-5,high=15,size=25)\n","print(z)\n","\n","# compute the softmax result\n","num = np.exp(z)\n","den = np.sum( num )\n","sigma = num / den\n","\n","# compare\n","plt.plot(z,sigma,'ko')\n","plt.xlabel('Original number (z)')\n","plt.ylabel('Softmaxified $\\sigma$')\n","plt.yscale('log')\n","plt.title('$\\sum\\sigma$ = %g' %np.sum(sigma))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jDIYW2bKHxXB"},"source":["# Using pytorch"]},{"cell_type":"code","metadata":{"id":"TxerJIwvIREg"},"source":["# slightly more involved using torch.nn\n","\n","# create an instance of the softmax activation class\n","softfun = nn.Softmax(dim=0)\n","\n","# then apply the data to that function\n","sigmaT = softfun( torch.Tensor(z) )\n","\n","# now we get the results\n","print(sigmaT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WY4hMa2BIRLQ"},"source":["# show that they are the same\n","plt.plot(sigma,sigmaT,'ko')\n","plt.xlabel('\"Manual\" softmax')\n","plt.ylabel('Pytorch nn.Softmax')\n","plt.title(f'The two methods correlate at r={np.corrcoef(sigma,sigmaT)[0,1]}')\n","plt.show()"],"execution_count":null,"outputs":[]}]}