{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNo/ZBBRp1BM0BrK6K026PJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Understanding nn.Embedding and nn.Linear<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"cmKCtBT1SL5n"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zt-FYu_LeI7l"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"8eJ94J2jeN_3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the two tensor objects"],"metadata":{"id":"Xh5CSDnheYW2"}},{"cell_type":"code","source":["vocab_size = 5000\n","embed_dim = int(vocab_size**(1/2))\n","print(f'{vocab_size:,} tokens in the vocab, and {embed_dim} embedding dimensions.')"],"metadata":{"id":"8GCgRfA3eODI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["E = nn.Embedding(vocab_size,embed_dim ) # input [in  X out]\n","L = nn.Linear(    embed_dim,vocab_size) # input [out X in]\n","\n","print(E)\n","print(L)\n","print(f'\\nEmbedding matrix size: {E.weight.shape}')\n","print(f'Linear matrix size:    {L.weight.shape}')"],"metadata":{"id":"6YqlqCh1gRPZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z6yWX0TniLpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Explore their attributes"],"metadata":{"id":"roLNEOBOiLmb"}},{"cell_type":"code","source":["len(dir(E)), len(dir(L))"],"metadata":{"id":"1arv-HWjhWqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir(E)"],"metadata":{"id":"a5jWSNLaiOWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# unique attributes of each type\n","attrE = dir(E)\n","attrL = dir(L)\n","\n","print('Attributes in Embedding that are not in Linear:')\n","for e in attrE:\n","  if e not in attrL:\n","    print('  ' + e)\n","\n","\n","print('\\n\\nAttributes in Linear that are not in Embedding:')\n","for l in attrL:\n","  if l not in attrE:\n","    print('  ' + l)"],"metadata":{"id":"rwDkamylhllB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Yxm8ZIXQhYkv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Indexing their vectors"],"metadata":{"id":"0j_sSRGxiQyA"}},{"cell_type":"code","source":["print(E.weight)\n","print('')\n","print(L.weight)"],"metadata":{"id":"DRk_lEbfichl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenidx = torch.tensor([14])\n","E(tokenidx) # emulates one-hot encoding"],"metadata":{"id":"xNxAlb9ZiSj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# L(tokenidx)\n","L.weight[tokenidx,:]"],"metadata":{"id":"3OV9sa7LiQu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# explicit one-hot encoding with vector-matrix multiplication\n","oneHotVect = torch.zeros(vocab_size,1)\n","oneHotVect[tokenidx] = 1\n","\n","# as vector-matrix multiplication\n","oneHotVect.t() @ L.weight"],"metadata":{"id":"V6shduNUgsYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y1Rpt2-6eOGS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Distributions of weights at initialization"],"metadata":{"id":"o9k-reBPeOJa"}},{"cell_type":"code","source":["# extract histograms to get empirical distributions\n","Ey,Ex = np.histogram(E.weight.flatten().detach(),100)\n","Ly,Lx = np.histogram(L.weight.flatten().detach(),100)\n","\n","plt.figure(figsize=(10,4))\n","plt.plot(Ex[:-1],Ey,label='nn.Embedding')\n","plt.plot(Lx[:-1],Ly,label='nn.Linear')\n","\n","plt.gca().set(xlim=[np.min(Ex),np.max(Ex)],xlabel='Weight value',ylabel='Count')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"iFexumWxkW1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: More on weights initialization in video\n","# \"Weight initializations\" in section \"Pretrain LLMs\"\n","\n","print('*** Statistics of nn.Linear weights ***\\n')\n","print(f'Kaiming expected std: { np.sqrt(1/embed_dim)/np.sqrt(3) :.4f}')\n","print(f'Observed std of L: {torch.std(L.weight.detach()) :.4f}')\n","\n","print(f'\\nKaiming expected bounds: {np.sqrt(1/embed_dim):.4f}')\n","print(f'Observed min/max vals: [{torch.min(L.weight):.4f},{torch.max(L.weight):.4f}]')"],"metadata":{"id":"a7ms3e2WeOMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat the above for Xavier\n","print('*** Statistics of nn.Embeddings weights ***\\n')\n","print(f'Xavier expected std: { np.sqrt(2/(vocab_size+embed_dim)) :.4f}')\n","print(f'Observed std of E: {torch.std(E.weight.detach()) :.4f}') # it's normal, not Xavier"],"metadata":{"id":"TTPPi66HeOPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IIHqvYH1eOS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize L to match properties of E\n","Ln = nn.Linear(embed_dim,vocab_size) # new matrix to avoid overwriting\n","torch.nn.init.normal_(Ln.weight)"],"metadata":{"id":"2Ew2pK8_eOV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Lny,Lnx = np.histogram(Ln.weight.flatten().detach(),100)\n","\n","plt.figure(figsize=(10,4))\n","plt.plot(Ex[:-1],Ey,label='nn.Embedding')\n","plt.plot(Lx[:-1],Ly,label='nn.Linear orig')\n","plt.plot(Lnx[:-1],Lny,label='nn.Linear weight init')\n","\n","plt.gca().set(xlim=[np.min(Ex),np.max(Ex)],xlabel='Weight value',ylabel='Count')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"pb42nyICeOYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# both created via nn.Parameter\n","??nn.Embedding\n","??nn.Linear"],"metadata":{"id":"LT6_-rc0UoiT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Conclusions:\n","#   1) Embeddings weights are initialized as standard normal, not Xavier normal.\n","#   2) they are \"different\" for practical reasons but reflect the same thing (trainable random weights)"],"metadata":{"id":"Yh_MheiiVvlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SJJAulPmBL73"},"execution_count":null,"outputs":[]}]}