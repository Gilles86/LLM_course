{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyObgpe3Q5mGoRXkAirqrBUN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>The Transformer block (code)<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"kgR8zCwdGAz2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGAUA24VOllp"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"xbl8WcFvSiSm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo of linear separation after dimensionality expansion"],"metadata":{"id":"gh6JZcOUSiP-"}},{"cell_type":"code","source":["# angles\n","n = 100\n","theta = np.linspace(0,2*np.pi-1/n,n)\n","\n","# coordinates in 2D\n","x_inner = 1*np.cos(theta) + np.random.randn(n)/10\n","y_inner = 1*np.sin(theta) + np.random.randn(n)/10\n","x_outer = 2*np.cos(theta) + np.random.randn(n)/10\n","y_outer = 2*np.sin(theta) + np.random.randn(n)/10\n","\n","# dimensionality-expansion via nonlinear transform\n","z_inner = np.sqrt(x_inner**2 + y_inner**2)\n","z_outer = np.sqrt(x_outer**2 + y_outer**2)\n","\n","\n","\n","### 2D scatter plot\n","fig = plt.figure(figsize=(12,5))\n","ax0 = fig.add_subplot(121)\n","\n","ax0.plot(x_inner,y_inner,'ko',markerfacecolor=[.7,.9,.7],markersize=9)\n","ax0.plot(x_outer,y_outer,'ks',markerfacecolor=[.9,.7,.7],markersize=9)\n","ax0.axis('square')\n","ax0.set(title='Non-linearly separable in 2D',xlabel='x',ylabel='y',\n","        xticklabels=[],yticklabels=[])\n","\n","### 3D scatter plot\n","ax1 = fig.add_subplot(122, projection='3d')\n","ax1.plot(x_inner,y_inner,z_inner,'ko',markerfacecolor=[.7,.9,.7],markersize=9)\n","ax1.plot(x_outer,y_outer,z_outer,'ks',markerfacecolor=[.9,.7,.7],markersize=9)\n","ax1.set(title='Linearly separable in 3D',xlabel='x',ylabel='y',zlabel='Radius',\n","        xticklabels=[],yticklabels=[])\n","ax1.view_init(20,20)\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"zIWdbUyvOp-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZI7VrNZZ4o5q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# And now to the main part of the code :)"],"metadata":{"id":"WGPg3oGM4ozm"}},{"cell_type":"code","source":[],"metadata":{"id":"PByf5MOR4ow9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model hyperparameters"],"metadata":{"id":"Gi-0zIzafVUo"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8\n","\n","# model hyperparameters\n","embed_dim = 128\n","\n","# training hyperparameters\n","batch_size = 5"],"metadata":{"id":"u0j1AmufJjg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LAp4k3HzfXCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# One attention head"],"metadata":{"id":"GjK4KWn9jQXO"}},{"cell_type":"code","source":["# create one attention head\n","class OneAttentionHead(nn.Module):\n","  def __init__(self,embed_dim):\n","    super().__init__()\n","\n","    # create the k/q/v matrices\n","    self.key   = nn.Linear(embed_dim,embed_dim,bias=False)\n","    self.query = nn.Linear(embed_dim,embed_dim,bias=False)\n","    self.value = nn.Linear(embed_dim,embed_dim,bias=False)\n","    self.W0    = nn.Linear(embed_dim,embed_dim,bias=False)\n","\n","  def forward(self,x):\n","\n","    # run the token embeddings vectors through attention\n","    k = self.key(x)\n","    q = self.query(x)\n","    v = self.value(x)\n","    y = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n","    y = self.W0(y) # linear weightings post-attention\n","\n","    return y"],"metadata":{"id":"o0Vq7bFElmwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# explore the attention head\n","onehead = OneAttentionHead(embed_dim)\n","\n","print(onehead)\n","\n","# run some fake data through\n","tokenEmbeds = torch.randn(batch_size, seq_len, embed_dim)\n","out = onehead(tokenEmbeds)\n","print(f'\\nOutput ({out.shape}): \\n{out}')"],"metadata":{"id":"M2a0F3D11q0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lGT67ohh8Cfr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer block"],"metadata":{"id":"E2-qzdxn8FiX"}},{"cell_type":"code","source":["#\n","class TransformerBlock(nn.Module):\n","  def __init__(self,embed_dim):\n","    super().__init__()\n","\n","    # attention sublayer\n","    self.layerNormAttn = nn.LayerNorm(embed_dim)\n","    self.attn = OneAttentionHead(embed_dim)\n","\n","    # feedforward (MLP) sublayer\n","    self.layerNormMLP  = nn.LayerNorm(embed_dim)\n","    self.W1   = nn.Linear(embed_dim,4*embed_dim) # 4x expansion\n","    self.gelu = nn.GELU()                        # nonlinearity\n","    self.W2   = nn.Linear(4*embed_dim,embed_dim) # 4x contraction\n","\n","\n","  def forward(self,x):\n","\n","    ## --- attention sublayer --- ##\n","    # save a copy of pre-attention data\n","    residual = x\n","\n","    # layernorm -> attention\n","    h        = self.layerNormAttn(x)\n","    attn_out = self.attn(h)\n","\n","    # combine pre-attention copy + attention adjustments\n","    x        = residual + attn_out\n","\n","    # note: could do this in one line:\n","    #x = x + self.attn(self.layerNormAttn(x))\n","    ## -------------------------- ##\n","\n","\n","\n","    ## ------ MLP sublayer ------ ##\n","    # copy of pre-MLP data\n","    residual2 = x\n","\n","    # layernorm before MLP\n","    h2        = self.layerNormMLP(x)\n","\n","    # expansion-nonlinearity-contraction\n","    mlp_out   = self.W2(self.gelu(self.W1(h2)))\n","\n","    # combine pre-MLP copy + MLP-adjustment\n","    y         = residual2 + mlp_out\n","    ## -------------------------- ##\n","\n","\n","    return y"],"metadata":{"id":"8c8TIZzvK51S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create and explore an instance\n","transblock = TransformerBlock(embed_dim)\n","print(transblock)"],"metadata":{"id":"LCQb5JCG8H1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# again, pushing data through\n","out = transblock(tokenEmbeds)\n","print(f'\\nOutput ({out.shape}): \\n{out}')"],"metadata":{"id":"HgvAnJX2_fgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AzDNP4o14ond"},"execution_count":null,"outputs":[]}]}