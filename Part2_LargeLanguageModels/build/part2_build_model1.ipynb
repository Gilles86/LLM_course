{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO43RjzeMi8lisutfUS86CE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Model 1: embedding (input) and unembedding (output)<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"ytd2fC5rSNwt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import requests\n","import matplotlib.pyplot as plt\n","\n","# pytorch stuff\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","source":["# GPT-4's tokenizer\n","!pip install tiktoken\n","import tiktoken\n","tokenizer = tiktoken.get_encoding('cl100k_base')"],"metadata":{"id":"4txp8aK9K_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B3ap39h0Jjdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"Gi-0zIzafVUo"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8 # aka context length\n","stride = 2\n","n_vocab = tokenizer.n_vocab\n","\n","# model hyperparameters\n","embed_dim = 2**6 # 64\n","\n","batch_size = 5"],"metadata":{"id":"u0j1AmufJjg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LAp4k3HzfXCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get data"],"metadata":{"id":"frpSLHaPJpeq"}},{"cell_type":"code","source":["# tokenize the text\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","\n","# text needs to be pytorch tensors\n","tokens = tokenizer.encode(text)\n","print(f'Variable \"tokens\" is type {type(tokens)}')\n","\n","# convert to pytorch\n","tmTokens = torch.tensor( tokens )\n","print(f'Variable \"tmTokens\" is type {type(tmTokens)} and has {len(tmTokens)}')"],"metadata":{"id":"4ROffDzOJqh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DataLoader"],"metadata":{"id":"8BDcTdUxJjbM"}},{"cell_type":"code","source":["# create a class for a dataset\n","class TokenDataset(Dataset):\n","  def __init__(self, tokens, seq_len=8, stride=4):\n","\n","    # initialize\n","    self.inputs  = []\n","    self.targets = []\n","\n","    # overlapping sequences of seq_len\n","    for i in range(0,len(tokens)-seq_len,stride):\n","\n","      # get c tokens and append to the lists\n","      self.inputs.append( tokens[i   : i+seq_len])\n","      self.targets.append(tokens[i+1 : i+seq_len+1])\n","\n","  def __len__(self):\n","    return len(self.inputs)\n","\n","  def __getitem__(self, idx):\n","    return self.inputs[idx], self.targets[idx]\n","\n","# create an instance!\n","token_dataset = TokenDataset(tmTokens,seq_len,stride)\n","\n","token_dataset[4]"],"metadata":{"id":"-d-Yg-XgJjYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UoVVoVwcJjQM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the model"],"metadata":{"id":"GjK4KWn9jQXO"}},{"cell_type":"code","source":["class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # embedding matrix\n","    self.embedding = nn.Embedding(n_vocab,embed_dim)\n","\n","    # unembedding (linear layer)\n","    self.gelu = nn.GELU()\n","    self.finalLinear = nn.Linear(embed_dim,n_vocab,bias=False)\n","\n","\n","\n","  def forward(self,tokx):\n","\n","    # forward pass\n","    x = self.embedding(tokx) # [batch, token, embed_dim]\n","    x = self.gelu(x)\n","    x = self.finalLinear(x)  # [batch, token, vocab_size]\n","\n","    # note: no softmax here!\n","    return x # logits\n","\n","  def generate(self,tokx,n_new_tokens=30):\n","\n","    # tokx is [batch, tokens]\n","\n","    for _ in range(n_new_tokens):\n","\n","      # get predictions\n","      x = self(tokx)\n","\n","      # extract the final token to predict the next\n","      x = x[:,-1,:] # [batch, vocab_size]\n","\n","      # apply softmax to get probability values over all tokens in the vocab\n","      probs = F.softmax(x,dim=-1)\n","\n","      # probabilistically sample from the distribution\n","      tokx_next = torch.multinomial(probs,num_samples=1) # [batch, 1]\n","\n","      # append\n","      tokx = torch.cat( (tokx,tokx_next),dim=1) # [batch, (tokens+1)]\n","    return tokx\n"],"metadata":{"id":"lpG1Af9RjQUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wlafU5oKIw8n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run the model!"],"metadata":{"id":"fCq05DDcbc-t"}},{"cell_type":"code","source":["# new instance of the model\n","model = Model()\n","\n","# get some data\n","X,y = token_dataset[12345]\n","\n","# process the tokens (forward pass)\n","out = model(X)\n","\n","print(X.shape)\n","print(y.shape)\n","print(out.shape) # [tokens, vocab_size]"],"metadata":{"id":"SQoC6PTqbcRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZZ3CpB_UIbk4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize the model output"],"metadata":{"id":"uJZ4Rv8dIbhz"}},{"cell_type":"code","source":["# visualize the softmax output\n","final = out[-1,:].detach()\n","softmaxFinal = torch.exp(final) / torch.exp(final).sum()\n","\n","\n","# create a figure\n","_,axs = plt.subplots(1,3,figsize=(12,3.3))\n","\n","# show the logits (raw logit coloring throughout)\n","axs[0].scatter(range(len(final)),final,s=5,marker='o',c=final,alpha=.4)\n","axs[0].set(title='Raw model outputs',xlabel='Token index',ylabel='Value',xlim=[0,len(final)])\n","\n","# the softmaxified logits (probabilities)\n","axs[1].scatter(range(len(final)),softmaxFinal,s=5,marker='o',c=final,alpha=.4)\n","axs[1].set(title='Softmax outputs',xlabel='Token index',ylabel='Probability',xlim=[0,len(final)])\n","\n","# their relation\n","axs[2].scatter(final,softmaxFinal,s=10,marker='o',c=final,alpha=.4,label='Data')\n","axs[2].plot([torch.min(final),torch.max(final)],[torch.min(softmaxFinal),torch.max(softmaxFinal)],\n","            '--',color=[.7,.7,.7],linewidth=1,label='Unity',zorder=-10)\n","axs[2].set(xlabel='Raw outputs',ylabel='Probability')\n","axs[2].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"47Hf5f9ScoTl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ODWHQSD1binT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate text"],"metadata":{"id":"kBe4yoPMbikq"}},{"cell_type":"code","source":["# some text :)\n","generated_tokens = model.generate(X.unsqueeze(dim=0),10)\n","\n","tokenizer.decode(generated_tokens[0].tolist())"],"metadata":{"id":"6i9qZepuJjNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat multiple times from the same starting point\n","for i in range(5):\n","\n","  # new tokens\n","  tokz = model.generate(X.unsqueeze(dim=0),10)\n","  tokz = tokz[0].tolist()\n","\n","  # print our lovely poem ;)\n","  print(f'\\n\\n--- Run {i+1} ---')\n","  print(tokenizer.decode(tokz))"],"metadata":{"id":"Abr_ndLZJjKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"H9UP4E0mVBxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate text in batches"],"metadata":{"id":"i9tSvFLRVBz5"}},{"cell_type":"code","source":["# also need a dataloader\n","dataloader = DataLoader(\n","                token_dataset,\n","                batch_size = batch_size,\n","                shuffle    = True,\n","                drop_last  = True\n","            )\n","\n","# let's have a look at the indices\n","X,y = next(iter(dataloader))\n","print(f'Inputs ({batch_size} batches X {seq_len} tokens):')\n","print(X)"],"metadata":{"id":"ZncvA4AAVANZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get model outputs (logits)\n","out = model(X)\n","print(out.shape) # [batch, tokens, vocab]\n","print('\\n',out)"],"metadata":{"id":"FBfqUvbAWnJ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sVj5fLt1I5B5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate some data\n","gen_tokens = model.generate(X)\n","print(gen_tokens.shape) # [batch, (tokens+n_new_tokens)]"],"metadata":{"id":"i5UpA0A3WiHR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat multiple times from the same starting point\n","for batchtok in gen_tokens:\n","  print('\\n--- NEXT SAMPLE: ---\\n')\n","  print(tokenizer.decode(batchtok.tolist()))"],"metadata":{"id":"XakghIo8WiEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XoyTa-GUWh-c"},"execution_count":null,"outputs":[]}]}