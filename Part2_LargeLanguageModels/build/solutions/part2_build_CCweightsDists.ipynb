{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMJP1kubP6rev4wgWF+wkpV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: GPT-2 trained weights distributions<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"cuMpzNCyK7JI"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from transformers import AutoModelForCausalLM\n","\n","# svg plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load SMALL pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VEMweSALLMid"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Distributions of token and position embeddings"],"metadata":{"id":"76jkzcB7Pwc4"}},{"cell_type":"code","source":["bins = np.linspace(-.6,.6,71)\n","\n","yE,xE = np.histogram(gpt2.transformer.wte.weight.detach(),bins,density=True)\n","yP,xP = np.histogram(gpt2.transformer.wpe.weight.detach(),bins,density=True)\n","\n","plt.figure(figsize=(8,4))\n","plt.plot(bins[:-1],yE,'.-',linewidth=2,label='Token')\n","plt.plot(bins[:-1],yP,'.-',linewidth=2,label='Position')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Parameter value',ylabel='Density',xlim=bins[[0,-1]],title='Embeddings weights')\n","plt.show()"],"metadata":{"id":"O95cjlGaLMnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bDBWtGYEP-_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Distributions of attention and MLP weights"],"metadata":{"id":"qGRX9HzMP-8f"}},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(13,4))\n","\n","\n","bins = np.linspace(-1,1,171)\n","colors = plt.cm.coolwarm(np.linspace(0,1,len(gpt2.transformer.h)))\n","\n","for hidx in range(len(gpt2.transformer.h)):\n","\n","  # attention weights\n","  y,x = np.histogram(gpt2.transformer.h[hidx].attn.c_attn.weight.detach(),bins=bins)\n","  axs[0].plot(x[:-1],y,color=colors[hidx],label=f'h{hidx}')\n","\n","  # MLP weights\n","  allMLP = torch.concatenate((gpt2.transformer.h[hidx].mlp.c_fc.weight.flatten(),gpt2.transformer.h[hidx].mlp.c_proj.weight.flatten()))\n","  y,x = np.histogram(allMLP.detach(),bins=bins)\n","  axs[1].plot(x[:-1],y,color=colors[hidx],label=f'h{hidx}')\n","\n","\n","for a in axs:\n","  a.legend(fontsize=10)\n","  a.set(xlim=bins[[0,-1]],xlabel='Weight value',ylabel='Counts')\n","\n","axs[0].set_title('Attention weights')\n","axs[1].set_title('MLP weights')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"P97lEBNnLMqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JzwD3M-cLMsv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Separately for Q, K, and V matrices"],"metadata":{"id":"SfA3aeVoLMvn"}},{"cell_type":"code","source":["# q,k,v are each of size n_emb X n_emb, and are concatenated into c_attn\n","gpt2.transformer.h[0].attn.c_attn.weight.shape#[1]/3"],"metadata":{"id":"fGB-Ww_ALMyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# example splitting the matrix into q, k, and v\n","n_emb = gpt2.transformer.wte.weight.shape[1]\n","q_weight, k_weight, v_weight = torch.split(gpt2.transformer.h[0].attn.c_attn.weight,n_emb,dim=1)"],"metadata":{"id":"NdWVtuDLLM1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","bins = np.linspace(-1,1,151)\n","\n","# initialize\n","allq = np.array([])\n","allk = np.array([])\n","allv = np.array([])\n","\n","n_emb = gpt2.transformer.wte.weight.shape[1]\n","\n","# loop over all transformers and get their weights\n","for hidx in range(len(gpt2.transformer.h)):\n","\n","  # split into matrices\n","  q,k,v = torch.split(gpt2.transformer.h[hidx].attn.c_attn.weight,n_emb,dim=1)\n","\n","  # add to the pile o' numbers\n","  allq = np.concatenate((allq,q.detach().flatten().numpy()),axis=0)\n","  allk = np.concatenate((allk,k.detach().flatten().numpy()),axis=0)\n","  allv = np.concatenate((allv,v.detach().flatten().numpy()),axis=0)\n","\n","\n","\n","# get histogram bins\n","yq,xq = np.histogram(allq,bins=bins,density=True)\n","yk,xk = np.histogram(allk,bins=bins,density=True)\n","yv,xv = np.histogram(allv,bins=bins,density=True)\n","\n","# and plot\n","plt.figure(figsize=(10,4))\n","plt.plot(xq[:-1],100*yq,linewidth=2,label='Q')\n","plt.plot(xk[:-1],100*yk,linewidth=2,label='K')\n","plt.plot(xv[:-1],100*yv,linewidth=2,label='V')\n","\n","plt.legend()\n","plt.gca().set(xlim=bins[[0,-1]],xlabel='Weight value',ylabel='Density',\n","              title=f'Distributions of attention matrices (n={len(allq):,} each)')\n","\n","plt.show()"],"metadata":{"id":"lq_CwRZDLM4P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aFI8g-xmLM63"},"execution_count":null,"outputs":[]}]}