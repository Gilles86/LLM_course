{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPmJV8Pmu274uMv7t/+jFg8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Code Attention manually and in Pytorch<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"4WWQV-8FSIhA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcXH_OwflxqX"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","import time"]},{"cell_type":"code","source":[],"metadata":{"id":"1t4sQjVgl4bp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Simulate data and attention matrices"],"metadata":{"id":"PcA4lzR3mdKa"}},{"cell_type":"code","source":["# parameters\n","n_batch = 4\n","n_embed = 10\n","context_length = 8\n","vocab_size = 40\n","\n","# input data\n","data = torch.randint(vocab_size,(n_batch,context_length)) # [batch,tokens]"],"metadata":{"id":"s-Qc7D0GmkGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embedding matrix\n","embeddings = nn.Embedding(vocab_size,n_embed)\n","\n","# create the q,k,v matrices\n","key   = nn.Linear(n_embed,n_embed,bias=False)\n","query = nn.Linear(n_embed,n_embed,bias=False)\n","value = nn.Linear(n_embed,n_embed,bias=False)"],"metadata":{"id":"T4JLDzZ7vsyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TQDn6X6lvy3T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Process the data"],"metadata":{"id":"eBoTH-CLvyzy"}},{"cell_type":"code","source":["# tokens to embeddings\n","x = embeddings(data)\n","\n","# weight the data pre-attention\n","k = key(x)\n","q = query(x)\n","v = value(x)"],"metadata":{"id":"GDIf9hmhvw5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print data sizes\n","print(f'      Data matrix: {data.shape}')\n","print(f'Embeddings matrix: {embeddings.weight.shape}')\n","print(f' Token embeddings: {x.shape}')\n","\n","# sizes of matrices\n","print('')\n","print(f'        Size of Q: {key.weight.shape}')\n","print(f'        Size of K: {key.weight.shape}')\n","print(f'        Size of V: {key.weight.shape}')\n","\n","# print attention matrices sizes\n","print('')\n","print(f'     Size of Q(x): {k.shape}')\n","print(f'     Size of K(x): {q.shape}')\n","print(f'     Size of V(x): {v.shape}')"],"metadata":{"id":"jcmywrX1aQGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OcS7BT54wEhj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Implement self-attention"],"metadata":{"id":"CeuO8wTcwEe5"}},{"cell_type":"code","source":["### manual implementation\n","\n","# \"cosine similarity\" between query and keys (note: would actually be cosine similarity if scaled by |q||k| )\n","qk = q@k.transpose(-2,-1) # transpose non-batch dimensions\n","\n","# variance-scale the QK\n","qk_scaled = qk * n_embed**-.5\n","\n","# apply mask for future tokens\n","pastmask = torch.tril(torch.ones(n_batch,context_length,context_length))\n","qk_scaled[pastmask==0] = -torch.inf # equivalent to adding a matrix of zeros/-infs\n","\n","# softmaxify\n","qk_softmax = F.softmax(qk_scaled,dim=-1)\n","\n","# and final attention mechanism\n","actsManual = qk_softmax @ v\n","\n","print(f'Shape of activations (manual): {actsManual.shape}') # [batch, context, n_embed]"],"metadata":{"id":"jRP3Ggqeshet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pytorch implementation\n","actsTorch = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n","print(f'Shape of activations (PyTorch): {actsTorch.shape}')"],"metadata":{"id":"kUxGutzGl4Tr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compare\n","print(actsManual[0,:,:])\n","print('')\n","print(actsTorch[0,:,:])\n","print('')\n","print(actsManual[0,:,:]-actsTorch[0,:,:])\n","\n","print(f'\\n\\nAre they _exactly_ equal? {torch.equal(actsTorch,actsManual)}')\n","print(f'Are they \"equal\"? {torch.allclose(actsTorch,actsManual)}')"],"metadata":{"id":"zwmi7HQBu2AO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oha23AE8Cemp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: CPU computation time"],"metadata":{"id":"Ok7NVrOQl4Qa"}},{"cell_type":"code","source":["numReps = 50_000\n","\n","# the manual version\n","start_time = time.time()\n","for _ in range(numReps):\n","  qk = q@k.transpose(-2,-1) * (n_embed**-.5)\n","  pastmask = torch.tril(torch.ones(n_batch,context_length,context_length))\n","  qk[pastmask==0] = -torch.inf\n","  qk = F.softmax(qk,dim=-1)\n","  activations = qk @ v\n","print(f'---    Manual: {time.time()-start_time:.3f} sec')\n","\n","# the optimized version\n","start_time = time.time()\n","for _ in range(numReps):\n","  activations = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n","print(f'--- Optimized: {time.time()-start_time:.3f} sec')"],"metadata":{"id":"_WTr9EEkl4Nj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"61iGjBgzl4K8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: GPU computation time"],"metadata":{"id":"wIk11i_Dl3mi"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"vWSlhOei0M3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZaVRB31U9vMR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Using bigger matrices"],"metadata":{"id":"8feQVJ439vhP"}},{"cell_type":"code","source":["# parameters\n","n_batch = 64\n","n_embed = 1000\n","context_length = 2048\n","vocab_size = 50257\n","\n","# create matrices\n","data = torch.randint(vocab_size,(n_batch,context_length),dtype=torch.long,device=device)\n","embedding = nn.Embedding(vocab_size,n_embed,device=device)\n","key   = nn.Linear(n_embed,n_embed,bias=False,device=device)\n","query = nn.Linear(n_embed,n_embed,bias=False,device=device)\n","value = nn.Linear(n_embed,n_embed,bias=False,device=device)\n","\n","x = embedding(data)\n","k = key(x)\n","q = query(x)\n","v = value(x)"],"metadata":{"id":"7OJ5NNtw00Yt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fU7yjXyy9zEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Now for the test!"],"metadata":{"id":"h14DG3NX9zB_"}},{"cell_type":"code","source":["numReps = 200\n","\n","torch.cuda.synchronize() # synchronize the GPU&CPU. good for time-testing, bad for overall performance\n","start_time = time.time()\n","for _ in range(numReps):\n","  qk = q@k.transpose(-2,-1) * (n_embed**-.5)\n","  pastmask = torch.tril(torch.ones(n_batch,context_length,context_length,device=device))\n","  qk[pastmask==0] = -torch.inf\n","  qk = F.softmax(qk,dim=-1)\n","  activationsM = qk @ v\n","print(f'--- Manual:  {time.time()-start_time:.3f} sec')\n","\n","\n","torch.cuda.synchronize()\n","start_time = time.time()\n","for _ in range(numReps):\n","  activationsP = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n","print(f'--- Pytorch: {time.time()-start_time:.3f} sec')"],"metadata":{"id":"Q1eFQn5RxvbZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GWWmle4aPHR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some additional optimizations\n","import torch._dynamo\n","SDPA_compiled = torch.compile(F.scaled_dot_product_attention)\n","torch.set_float32_matmul_precision('high')"],"metadata":{"id":"iWNOEO7ixvTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI, FlashAttention: https://github.com/Dao-AILab/flash-attention"],"metadata":{"id":"3aDlmDzsj0Rt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numReps = 200\n","\n","torch.cuda.synchronize() # synchronize the GPU&CPU. good for time-testing, bad for overall performance\n","start_time = time.time()\n","for _ in range(numReps):\n","  qk = q@k.transpose(-2,-1) * (n_embed**-.5)\n","  pastmask = torch.tril(torch.ones(n_batch,context_length,context_length,device=device))\n","  qk[pastmask==0] = -torch.inf\n","  qk = F.softmax(qk,dim=-1)\n","  activationsM = qk @ v\n","print(f'--- Manual:  {time.time()-start_time:.3f} sec')\n","\n","\n","torch.cuda.synchronize()\n","start_time = time.time()\n","for _ in range(numReps):\n","  activationsP = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n","print(f'--- Pytorch: {time.time()-start_time:.3f} sec')\n","\n","\n","torch.cuda.synchronize()\n","start_time = time.time()\n","for _ in range(numReps):\n","  activationsO = SDPA_compiled(q,k,v,is_causal=True)\n","print(f'--- Compiled: {time.time()-start_time:.3f} sec')"],"metadata":{"id":"7HRbGEmZPwys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o2U8L2e1Pwsc"},"execution_count":null,"outputs":[]}]}