{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNAQfkkP5YPcsMqZu+apgqd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Time model5 on CPU and GPU<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"ggclU75JnzrI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"659KlDz3lpkt"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import time"]},{"cell_type":"code","source":["# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"3ZXGg9FO9FfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameters for GPT2-124M\n","n_vocab    = 50257 # GPT2 vocab size\n","embed_dim  =   768 # embedding dimension\n","seq_len    =  1024 # max sequence length\n","n_heads    =    12 # attention heads\n","n_blocks   =    12 # transformer blocks\n","batch_size =     8"],"metadata":{"id":"sGjyf07DlqdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OICCCBmewJsj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Create (and time) two model instances"],"metadata":{"id":"7HU4tImfwJ7K"}},{"cell_type":"code","source":["### ----------- Class for multihead attention ----------- ###\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # number of attention heads\n","    self.num_heads = n_heads\n","    self.head_dim  = embed_dim // n_heads\n","\n","    # the three Q,K,V matrices are initialized as one, and are split inside forward()\n","    self.QKV = nn.Linear(embed_dim, 3*embed_dim, bias=True)\n","\n","    # linear projection after attention\n","    self.W0 = nn.Linear(embed_dim, embed_dim, bias=True)\n","\n","\n","  def forward(self,x):\n","\n","    # sizes for later use\n","    B, T, E = x.shape # [batch, seq_len, embed_dim]\n","\n","    # push data through Q, K, and V in one concatenated matrix\n","    qkv = self.QKV(x) # [batch, sequence, 3*embed]\n","    q,k,v = torch.split(qkv,E,dim=2) # each matrix is [B, T, E]\n","\n","    # reshape to [B, T, nHeads, head_dim]\n","    #  and then transpose to [B, nHeads, T, head_dim]\n","    q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2) # [B, nHeads, T, head_dim]\n","    k = k.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n","    v = v.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n","\n","    # Pytorch's dot-product attention function handles multi-head shapes\n","    out = F.scaled_dot_product_attention(q, k, v, is_causal=True) # [B, nHeads, T, head_dim]\n","\n","    # recombine heads: (B, nHeads, T, head_dim) -> [B, T, E]\n","    out = out.transpose(1,2).view(B, T, E)\n","\n","    # finally, linearly mix the attention heads\n","    out = self.W0(out)\n","\n","    return out\n","### --------------- multihead attention --------------- ###\n","\n","\n","\n","\n","\n","### ----------- Class for Transformer block ----------- ###\n","class TransformerBlock(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    ### attention subblock\n","    self.layernorm_1 = nn.LayerNorm(embed_dim, eps=1e-5)\n","    self.attn = MultiHeadAttention()\n","\n","\n","    ### linear feedforward (MLP) subblock\n","    self.layernorm_2 = nn.LayerNorm(embed_dim, eps=1e-5)\n","    # 4x expansion, then back to embedding size\n","    self.mlp_1 = nn.Linear(embed_dim, 4*embed_dim, bias=True)\n","    self.gelu  = nn.GELU()\n","    self.mlp_2 = nn.Linear(4*embed_dim, embed_dim, bias=True)\n","\n","  def forward(self, x):\n","\n","    # attention\n","    x_att = self.layernorm_1(x) # pre-attention normalization\n","    x_att = x + self.attn(x_att) # run through attention, then add pre-attention activation (\"residual\")\n","\n","\n","    # MLP\n","    x_ff = self.layernorm_2(x_att) # pre-MLP normalization\n","    x_ff = self.mlp_2(self.gelu( self.mlp_1(x_ff) )) # expansion-nonlinearity-contraction\n","    x_ff = x_att + x_ff # add back pre-MLP activation for adjustment\n","\n","    return x_ff\n","### --------------- Transformer block --------------- ###\n","\n","\n","\n","\n","\n","\n","### --------------- class for the language model --------------- ###\n","class LanguageModel(nn.Module):\n","  def __init__(self,device):\n","    super().__init__()\n","\n","    # token + position embeddings\n","    self.wte = nn.Embedding(n_vocab, embed_dim) # token embedding\n","    self.wpe = nn.Embedding(seq_len, embed_dim) # position embedding\n","\n","    # transformer blocks\n","    self.transformerBlocks = nn.Sequential(*[TransformerBlock() for _ in range(n_blocks)])\n","\n","    # final layernorm\n","    self.layernorm_final = nn.LayerNorm(embed_dim, eps=1e-5)\n","\n","    # lm head, with weights tied to token embedding\n","    self.final_head = nn.Linear(embed_dim, n_vocab, bias=False)\n","    self.final_head.weight = nn.Parameter(self.wte.weight)\n","\n","    self.device = device\n","\n","\n","  def forward(self, idx):\n","\n","    # token + position embeddings (note the device!)\n","    token_emb = self.wte(idx) # [B, T, E]\n","    posit_emb = self.wpe(torch.arange(idx.shape[-1],device=self.device)) # [T, E]\n","    x = token_emb + posit_emb # [B, T, E]\n","\n","    # pass through each transformer block\n","    x = self.transformerBlocks(x)\n","\n","    # final layernorm and unembeddings\n","    x = self.layernorm_final(x)\n","    logits = self.final_head(x)  # [B, T, n_vocab]\n","\n","    return logits\n","\n","\n","  def generate(self, idx, temperature=1., max_new_tokens=50):\n","\n","    for _ in range(max_new_tokens):\n","\n","      # forward pass\n","      logits = self(idx[:,-seq_len:])  # [B, T, n_vocab]\n","      logits = logits[:,-1,:]  # last token's logits: [B, n_vocab]\n","\n","      # apply temperature + softmax\n","      probs = F.softmax(logits/temperature, dim=-1) # [B, n_vocab]\n","\n","      # sample next token\n","      idx_next = torch.multinomial(probs, num_samples=1) # [B, 1]\n","\n","      # append\n","      idx = torch.cat((idx, idx_next), dim=1) # [B, T+1]\n","    return idx\n","### ------------------ language model ------------------ ###"],"metadata":{"id":"Osvbhri9jMjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2T2vRnY3jMgV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# time how long it takes to create each model instance on the CPU vs GPU\n","print('--- Creating the model ---')\n","torch.cuda.synchronize()\n","start_time = time.time()\n","model_gpu = LanguageModel(device=device).to(device)\n","print(f'--- GPU: {time.time()-start_time:.3f} sec')\n","\n","start_time = time.time()\n","model_cpu = LanguageModel(device='cpu')\n","print(f'--- CPU: {time.time()-start_time:.3f} sec')"],"metadata":{"id":"VK63bdKWoQbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm\n","print(model_gpu.wte.weight.device)\n","print(model_cpu.wte.weight.device)"],"metadata":{"id":"E3zTvr02kRmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rT218SJ1zDuU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Time a forward pass"],"metadata":{"id":"Mi3-wiGYwh1f"}},{"cell_type":"code","source":["numReps = 5\n","\n","\n","### test the GPU model\n","torch.cuda.synchronize()\n","start_time = time.time()\n","for _ in range(numReps):\n","\n","  # some fake data (one batch)\n","  data = torch.randint(0,n_vocab,size=(batch_size,seq_len),device=device)\n","  outG = model_gpu(data)\n","\n","print(f'--- GPU: {time.time()-start_time:6.3f} sec')\n","\n","\n","\n","### repeat for the CPU model\n","start_time = time.time()\n","for _ in range(numReps):\n","\n","  # some fake data (one batch)\n","  data = torch.randint(0,n_vocab,size=(batch_size,seq_len),device='cpu')\n","  outG = model_cpu(data)\n","\n","print(f'--- CPU: {time.time()-start_time:6.3f} sec')"],"metadata":{"id":"IUotTBFgoQYI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iE_ZOnnGxBiz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Time backprop"],"metadata":{"id":"E2rmpKSjxBf_"}},{"cell_type":"code","source":["### run this cell to create a loss function and optimizer\n","\n","# GPU: create the loss and optimizer functions\n","lossfun_gpu = nn.NLLLoss().to(device)\n","optimizer_gpu = torch.optim.AdamW(model_gpu.parameters(), lr=.001)\n","\n","# CPU: create the loss and optimizer functions\n","lossfun_cpu = nn.NLLLoss()\n","optimizer_cpu = torch.optim.AdamW(model_cpu.parameters(), lr=.001)"],"metadata":{"id":"zM0eZGbDxBdT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### copy these lines to put into a for-loop to run backprop\n","\n","data = torch.randint(0,n_vocab,size=(batch_size,seq_len)).to(device) # create data\n","model_gpu.zero_grad() # clear previous gradients\n","outG = model_gpu(data) # forward pass\n","lossG = lossfun_gpu(outG[:,-1,:],data[:,-1]) # calculate the loss\n","lossG.backward() # calculate gradients\n","optimizer_gpu.step() # implement backprop"],"metadata":{"id":"tuiyf39D0rJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### test the GPU model\n","torch.cuda.synchronize()\n","start_time = time.time()\n","for _ in range(numReps):\n","  data = torch.randint(0,n_vocab,size=(batch_size,seq_len)).to(device) # create data\n","  model_gpu.zero_grad() # clear previous gradients\n","  outG = model_gpu(data) # forward pass\n","  lossG = lossfun_gpu(outG[:,-1,:],data[:,-1]) # calculate the loss\n","  lossG.backward() # calculate gradients\n","  optimizer_gpu.step() # implement backprop\n","\n","print(f'--- GPU: {time.time()-start_time:6.3f} sec')\n","\n","\n","### test the CPU model\n","torch.cuda.synchronize()\n","start_time = time.time()\n","for _ in range(numReps):\n","  data = torch.randint(0,n_vocab,size=(batch_size,seq_len)) # create data\n","  model_cpu.zero_grad() # clear previous gradients\n","  outC = model_cpu(data) # forward pass\n","  lossC = lossfun_cpu(outC[:,-1,:],data[:,-1]) # calculate the loss\n","  lossC.backward() # calculate gradients\n","  optimizer_cpu.step() # implement backprop\n","\n","print(f'--- CPU: {time.time()-start_time:6.3f} sec')"],"metadata":{"id":"o9GE_Peb0Ont"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QmtcLTWKxBaW"},"execution_count":null,"outputs":[]}]}