{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNaVS6GQypfmkuPMWfGNNvt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Model 3: One attention head<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"heNBXBCtSO98"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# pytorch stuff\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","source":["#n load GPT2 tokenizer\n","from transformers import GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"4txp8aK9K_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B3ap39h0Jjdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"Gi-0zIzafVUo"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8 # aka context window\n","n_vocab = tokenizer.n_vocab #n\n","\n","# model hyperparameters\n","embed_dim = 2**6 # 64\n","\n","# training hyperparameters\n","batch_size = 5"],"metadata":{"id":"u0j1AmufJjg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LAp4k3HzfXCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The model"],"metadata":{"id":"GjK4KWn9jQXO"}},{"cell_type":"code","source":["class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # embedding matrices\n","    self.embedding = nn.Embedding(n_vocab,embed_dim)\n","    self.positions = nn.Embedding(seq_len,embed_dim)\n","\n","    # final output linear layer (unembeddings)\n","    self.finalLinear = nn.Linear(embed_dim,n_vocab,bias=False)\n","\n","    # initialize the k,q,v matrices for attention          #n\n","    self.layernormA = nn.LayerNorm(embed_dim)              #n\n","    self.key   = nn.Linear(embed_dim,embed_dim,bias=False) #n\n","    self.query = nn.Linear(embed_dim,embed_dim,bias=False) #n\n","    self.value = nn.Linear(embed_dim,embed_dim,bias=False) #n\n","    self.W0    = nn.Linear(embed_dim,embed_dim)            #n\n","\n","    # the final output layer is tied to the token embedding\n","    self.finalLinear.weight = nn.Parameter(self.embedding.weight)\n","\n","\n","\n","  def forward(self,tokx):\n","\n","    # create the token+position embedding\n","    token_embed = self.embedding(tokx)\n","    posit_embed = self.positions(torch.arange(tokx.shape[-1])) # [numtokens, embedding_dims]\n","\n","    # their sum is the output of the embeddings (the addition will broadcast for batch>1)\n","    x = token_embed + posit_embed # [batch, numtokens, embedding_dims]\n","\n","\n","    ##n --- attention sublayer begins here\n","    # layernorm before attention\n","    x = self.layernormA(x)\n","\n","    # attention algo\n","    k = self.key(x)\n","    q = self.query(x)\n","    v = self.value(x)\n","\n","    qk = q@k.transpose(-2,-1) # dot product between query and keys (would be cosine similiarity if normalized)\n","    qk_scaled = qk * embed_dim**-.5 # variance-scale the QK (for this model, embed_dim = head_size)\n","    pastmask = torch.tril(torch.ones(x.shape[0],seq_len,seq_len)) # apply mask for future tokens\n","    qk_scaled[pastmask==0] = -torch.inf\n","    qk_softmax = F.softmax(qk_scaled,dim=-1) # softmaxify\n","    y = qk_softmax @ v # final attention mechanism\n","\n","    y *= self.W0(y)\n","    #n --- end attention\n","\n","\n","    # -o-\n","    # MLP sublayer would be here\n","    # -o-\n","\n","\n","    # final output transformation (unembeddings)\n","    y = self.finalLinear(y) / np.sqrt(embed_dim)\n","\n","    return y, (pastmask,qk_scaled,qk_softmax) #n output some attention matrices for inspection (hooks are better in real models!)\n","\n","\n","  def generate(self,tokx,temperature=1,n_new_tokens=50):\n","\n","    # tokx is [batch, tokens]\n","\n","    for _ in range(n_new_tokens):\n","\n","      # get predictions, but only from the past seq_len tokens\n","      x = self(tokx[:,-seq_len:])[0] # [batch, seq_len, n_vocab]\n","\n","      # extract the final token to predict the next\n","      x = x[:,-1,:] # [batch, n_vocab]\n","\n","      # apply softmax to get probability values over all tokens in the vocab - with temperature\n","      probs = F.softmax(x/temperature,dim=-1) # [batch, n_vocab]\n","\n","      # probabilistically sample from the distribution\n","      tokx_next = torch.multinomial(probs,num_samples=1) # [batch, 1]\n","\n","      # append\n","      tokx = torch.cat( (tokx,tokx_next),dim=1) # [batch, (tokens+1)]\n","    return tokx\n"],"metadata":{"id":"lpG1Af9RjQUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZjB2dHKNhJFD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Calculate logits (model output)"],"metadata":{"id":"fCq05DDcbc-t"}},{"cell_type":"code","source":["# create data\n","tokens = tokenizer.encode('I prefer oat milk in my coffee.')\n","X = torch.tensor(tokens[:-1]).unsqueeze(0)\n","y = torch.tensor(tokens[1:]).unsqueeze(0)\n","\n","print(X.shape)\n","print(y.shape)"],"metadata":{"id":"heS6eE776A7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model()\n","out,attn = model(X)\n","\n","print(out.shape)"],"metadata":{"id":"SQoC6PTqbcRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Expected loss for random weights: {-np.log(1/tokenizer.vocab_size):.3f}')\n","print(f'Observed mean log-softmax output: {torch.mean(-F.log_softmax(out.detach(),dim=-1)):.3f}')\n","print(f'Cross-entropy loss from pytorch:  {F.cross_entropy(out.view(-1, out.shape[-1]), y.view(-1)):.3f}')"],"metadata":{"id":"0K2SA-rU5Br8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Time-causal mask:\\n',attn[0])\n","print('\\nqk_scaled:\\n',attn[1])\n","print('\\nqk_softmax:\\n',attn[2])"],"metadata":{"id":"UXJgaBsCeGOX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(attn[2].detach().squeeze(),vmin=0,vmax=.6,cmap='plasma')\n","plt.gca().set(xlabel='Token weighting',ylabel='Token being processed',title='Softmaxified QK$^T$')\n","plt.colorbar(pad=.02)\n","plt.show()"],"metadata":{"id":"urvDZHIo73aI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sanity-check ;)\n","attn[2].detach().squeeze().sum(dim=1)"],"metadata":{"id":"Sf47KGKXOHQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ODWHQSD1binT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate text"],"metadata":{"id":"kBe4yoPMbikq"}},{"cell_type":"code","source":["# start with some new tokens\n","text = 'When I grow up, I want to be a'\n","tokens = tokenizer.encode(text)\n","tokens = torch.tensor(tokens).unsqueeze(0)\n","\n","generated_tokens = model.generate(tokens,temperature=2,n_new_tokens=10)[0]\n","\n","# let's see how it looks!\n","tokenizer.decode(generated_tokens.tolist())"],"metadata":{"id":"6i9qZepuJjNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"O_InQQ9p9DP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat with different temperature values\n","\n","temps = [ .2, .7, 1, 2, 10 ] # outrageous values...\n","\n","for T in temps:\n","  tokz = model.generate(tokens,temperature=T,n_new_tokens=10)\n","  tokz = tokz[0].tolist()\n","  print(f'Temp = {T}:\\n  {tokenizer.decode(tokz)}\\n')"],"metadata":{"id":"Abr_ndLZJjKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XoyTa-GUWh-c"},"execution_count":null,"outputs":[]}]}