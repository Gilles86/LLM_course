{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPu9nvH98AIGCmkVfbYcyEN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Multihead attention: theory and implementation<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"28KbzPhwSQNP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","source":[],"metadata":{"id":"B3ap39h0Jjdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"Gi-0zIzafVUo"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8 # aka context window\n","\n","# model hyperparameters\n","embed_dim = 128\n","n_heads = 4 #n embed_dim/n_heads must be int\n","\n","# training hyperparameters\n","batch_size = 5"],"metadata":{"id":"u0j1AmufJjg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UoVVoVwcJjQM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Class for multihead attention"],"metadata":{"id":"wfe6PEy98Cc5"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, num_heads, embed_dim):\n","    super().__init__()\n","\n","    # head-dimensionality is embed_dim split across the heads\n","    self.num_heads = num_heads\n","    self.head_dim = embed_dim // num_heads\n","\n","    # num_heads Q, K, and V matrices, initialized as one \"super-head\"\n","    #    note: in model 5, these three matrices are combined into one\n","    self.query = nn.Linear(embed_dim, embed_dim, bias=False)\n","    self.key   = nn.Linear(embed_dim, embed_dim, bias=False)\n","    self.value = nn.Linear(embed_dim, embed_dim, bias=False)\n","\n","    # final linear projection merges the heads' outputs\n","    self.W0 = nn.Linear(embed_dim, embed_dim, bias=False)\n","\n","  def forward(self,x,track_sizes=False):\n","\n","    # extract the dimension sizes of the inputs (token embeddings)\n","    B, T, E = x.shape # [batch, tokens (sequence length), embed_dim]\n","    if track_sizes: print(f\"1){' Input data shape:':>28} {x.shape}\")\n","\n","    # push data through Q, K, and V (actually multiple heads still in the same matrix)\n","    q = self.query(x) # [batch, seq_len, embed_dim]\n","    k = self.key(x)\n","    v = self.value(x)\n","    if track_sizes: print(f\"2){'q/k/v pre-split shape:':>28} {q.shape}\")\n","\n","    # reshape to split up the heads (note: head-splitting is done after XW_Q)\n","    q = q.view(B, T, self.num_heads, self.head_dim)\n","    k = k.view(B, T, self.num_heads, self.head_dim)\n","    v = v.view(B, T, self.num_heads, self.head_dim)\n","\n","    # but pytorch's SDPA function needs the shape to be [B, num_heads, T, head_dim]\n","    q = q.transpose(1,2)\n","    k = k.transpose(1,2)\n","    v = v.transpose(1,2)\n","    if track_sizes: print(f\"3){'q/k/v post-split shape:':>28} {q.shape}\")\n","\n","    # now we can call SDPA\n","    out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n","    if track_sizes: print(f\"4){'Data post-attention shape:':>28} {out.shape}\")\n","\n","    # but our code still needs [B, T, num_heads, head_dim]\n","    out = out.transpose(1,2)\n","    if track_sizes: print(f\"5){'Post-attention data reshape:':>28} {out.shape}\")\n","\n","    # merge heads back into embed_dim\n","    out = out.reshape(B, T, E)\n","    if track_sizes: print(f\"6){'Data merged to size:':>28} {out.shape}\")\n","\n","    # finally, apply linear mixing matrix\n","    out = self.W0(out)\n","    if track_sizes: print(f\"7){'Post-MHA H0 linear mixing:':>28} {out.shape}\")\n","\n","    return out"],"metadata":{"id":"inaEfb9EK6hJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mha = MultiHeadAttention(n_heads,embed_dim)\n","mha"],"metadata":{"id":"5V7ytoWt6R-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run some fake data through\n","data = torch.randn(size=(batch_size,seq_len,embed_dim))\n","out = mha(data)\n","print(f'Input size:  {data.shape}')\n","print(f'Output size: {out.shape}')"],"metadata":{"id":"BOsT2wuxymNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'    Sequence length: {seq_len:2d}')\n","print(f'Embedding dimension: {embed_dim}')\n","print(f'    Number of heads: {n_heads:2d}')\n","print(f'Head dimensionality: {embed_dim // n_heads}')\n","\n","print('\\nDimensions of the data as it passes through the attention sublayer of one Transformer block:')\n","out = mha(data,track_sizes=True)"],"metadata":{"id":"-_9LoZDAymKS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BFfWqZij7kHr"},"execution_count":null,"outputs":[]}]}