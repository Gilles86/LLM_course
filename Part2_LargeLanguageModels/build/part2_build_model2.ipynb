{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNSrUhPJleW7dxiVWzOdhv+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Model 2: Position embedding, layernorm, tied output, temperature<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"ngqc0DJISOXC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import requests\n","import matplotlib.pyplot as plt\n","\n","# pytorch stuff\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","source":["# GPT-4's tokenizer\n","!pip install tiktoken\n","import tiktoken\n","tokenizer = tiktoken.get_encoding('cl100k_base')"],"metadata":{"id":"4txp8aK9K_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Dd0LYzOdyMxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"Gi-0zIzafVUo"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8 # aka context window\n","stride = 2\n","\n","# model hyperparameters\n","embed_dim = 2**6 # 64\n","\n","# training hyperparameters\n","batch_size = 5"],"metadata":{"id":"u0j1AmufJjg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LAp4k3HzfXCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get data"],"metadata":{"id":"frpSLHaPJpeq"}},{"cell_type":"code","source":["# tokenize the text\n","# note that we need torch tensors!\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","tmTokens = torch.tensor( tokenizer.encode(text) )\n","len(tmTokens)"],"metadata":{"id":"4ROffDzOJqh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DataLoader"],"metadata":{"id":"8BDcTdUxJjbM"}},{"cell_type":"code","source":["# create a class for a dataset (note: batching is done by the DataLoader, not in the dataset)\n","class tokenDataset(Dataset):\n","  def __init__(self, tokens, seq_len=8, stride=4):\n","\n","    # initialize\n","    self.inputs  = []\n","    self.targets = []\n","\n","    # overlapping sequences of seq_len\n","    for i in range(0,len(tokens)-seq_len,stride):\n","\n","      # get c tokens and append to the lists\n","      self.inputs.append( tokens[i   : i+seq_len])\n","      self.targets.append(tokens[i+1 : i+seq_len+1])\n","\n","  def __len__(self):\n","    return len(self.inputs)\n","\n","  def __getitem__(self, idx):\n","    return self.inputs[idx], self.targets[idx]\n","\n","# create an instance!\n","token_dataset = tokenDataset(tmTokens,seq_len,stride)\n","\n","token_dataset[4]"],"metadata":{"id":"-d-Yg-XgJjYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UoVVoVwcJjQM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The model"],"metadata":{"id":"GjK4KWn9jQXO"}},{"cell_type":"code","source":["class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # embeddings matrices\n","    self.embedding = nn.Embedding(tokenizer.n_vocab,embed_dim)\n","    self.positions = nn.Embedding(seq_len,embed_dim) #n\n","\n","    # nonlinearity and layernorm\n","    self.gelu = nn.GELU()\n","    self.layernorm  = nn.LayerNorm(embed_dim) #n\n","\n","    # the final output layer is tied to the token embeddings\n","    self.finalLinear = nn.Linear(embed_dim,tokenizer.n_vocab,bias=False)\n","    self.finalLinear.weight = nn.Parameter(self.embedding.weight) #n\n","\n","\n","\n","  def forward(self,tokx):\n","\n","    # create the token+position embedding\n","    token_embed = self.embedding(tokx)\n","    posit_embed = self.positions(torch.arange(tokx.shape[-1])) #n [numtokens, embedding_dims]\n","\n","    # their sum is the output of the embeddings (the addition will broadcast for batch>1)\n","    x = token_embed + posit_embed #n [batch, numtokens, embedding_dims]\n","\n","    # layernorm before linear layer\n","    x = self.layernorm(x) #n\n","\n","    # forward pass\n","    x = self.gelu(x) # note: full GPT models don't have gelu before the final unembeddings\n","    x = self.finalLinear(x) / np.sqrt(embed_dim)\n","    return x\n","\n","  def generate(self,tokx,temperature=1,n_new_tokens=50): #n\n","\n","    # tokx is batch X tokens\n","\n","    for _ in range(n_new_tokens):\n","\n","      # get predictions, but only from the past seq_len tokens\n","      x = self(tokx[:,-seq_len:]) #n [batch, seq_len, n_vocab]\n","\n","      # extract the final token to predict the next\n","      x = x[:,-1,:] # [batch, n_vocab]\n","\n","      # apply softmax to get probability values over all tokens in the vocab - with temperature\n","      probs = F.softmax(x/temperature,dim=-1) #n [batch, n_vocab]\n","\n","      # probabilistically sample from the distribution\n","      tokx_next = torch.multinomial(probs,num_samples=1) # [batch, 1]\n","\n","      # append\n","      tokx = torch.cat( (tokx,tokx_next),dim=1) # [batch, (tokens+1)]\n","    return tokx\n"],"metadata":{"id":"lpG1Af9RjQUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KU6o5jeIQVsc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create a model instance and test"],"metadata":{"id":"fCq05DDcbc-t"}},{"cell_type":"code","source":["m = Model()\n","X,y = token_dataset[4]\n","out = m(X)\n","\n","print(X.shape)\n","print(y.shape)\n","print(out.shape)"],"metadata":{"id":"SQoC6PTqbcRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Expected loss for random weights: {-np.log(1/tokenizer.n_vocab):.3f}')\n","print(f'Observed mean log-softmax output: {torch.mean(-F.log_softmax(out.detach(),dim=-1)):.3f}')\n","print(f'Cross-entropy loss from pytorch:  {F.cross_entropy(out.view(-1, out.shape[-1]), y.view(-1)):.3f}')"],"metadata":{"id":"iPGXa7y-BD4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kA3IOfKwzrBh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo of position+token embeddings"],"metadata":{"id":"ACHhCoEPyH1O"}},{"cell_type":"code","source":["# broadcasting the token+position embeddings in small matrices\n","T = torch.ones(2,3,4) # [batch, numtokens, embedding_dims]\n","P = torch.arange(4)\n","\n","print(f'Token embeddings matrix ({T.shape}):')\n","print(T)\n","\n","print(f'\\nPosition embeddings matrix ({P.shape}):')\n","print(P)\n","\n","print(f'\\nTheir sum: ({(T+P).shape}):')\n","print(T+P)"],"metadata":{"id":"C_e2V19ykwTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# again using the real weights from the model\n","P = m.positions(torch.arange(seq_len))\n","T = m.embedding(X)\n","\n","print(f'Token embeddings matrix ({T.shape})')\n","print(f'\\nPosition embeddings matrix ({P.shape})')\n","print(f'\\nTheir sum: ({(T+P).shape})')"],"metadata":{"id":"wQ1k_ACQyM7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tRKEZy_gz0mj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualizing the model output"],"metadata":{"id":"IhLfKsz_z0j8"}},{"cell_type":"code","source":["# visualize the softmax output\n","out = m(X)\n","final = out[-1,:].detach()\n","softmaxFinal = torch.exp(final) / torch.exp(final).sum()\n","\n","\n","# create a figure\n","_,axs = plt.subplots(1,3,figsize=(12,3.3))\n","\n","# show the logits (raw logit coloring throughout)\n","axs[0].scatter(range(len(final)),final,s=5,marker='o',c=final,alpha=.4)\n","axs[0].set(title='Raw model outputs',xlabel='Token index',ylabel='Value',xlim=[0,len(final)])\n","\n","# the softmaxified logits (probabilities)\n","axs[1].scatter(range(len(final)),softmaxFinal,s=5,marker='o',c=final,alpha=.4)\n","axs[1].set(title='Softmax probability',xlabel='Token index',ylabel='Probability',xlim=[0,len(final)])\n","\n","# their relation\n","axs[2].scatter(final,torch.log(softmaxFinal),s=10,marker='o',c=final,alpha=.4)\n","axs[2].set(xlabel='Raw outputs',ylabel='Log softmax probability')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"47Hf5f9ScoTl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"H9UP4E0mVBxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate text in batches"],"metadata":{"id":"i9tSvFLRVBz5"}},{"cell_type":"code","source":["# also need a dataloader\n","dataloader = DataLoader(\n","                token_dataset,\n","                batch_size = batch_size,\n","                shuffle    = True,\n","                drop_last  = True\n","            )\n","\n","# let's have a look at the indices\n","X,y = next(iter(dataloader))\n","print(f'Inputs ({batch_size} batches X {seq_len} tokens):')\n","print(X)"],"metadata":{"id":"ZncvA4AAVANZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get model outputs (logits)\n","out = m(X)\n","print(out.shape) # batch X tokens X vocab\n","print(out)"],"metadata":{"id":"FBfqUvbAWnJ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate some data\n","gen_tokens = m.generate(X,temperature=1.3,n_new_tokens=8)\n","print(gen_tokens.shape) # batch X (tokens+n_new_tokens)"],"metadata":{"id":"i5UpA0A3WiHR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat multiple times from the same starting point\n","for batchtok in gen_tokens:\n","  print('\\n--- NEXT SAMPLE: ---\\n')\n","  print(tokenizer.decode(batchtok.tolist()))"],"metadata":{"id":"XakghIo8WiEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XoyTa-GUWh-c"},"execution_count":null,"outputs":[]}]}