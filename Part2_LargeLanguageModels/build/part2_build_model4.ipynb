{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyN98JdYZ0G0OCoRqSqbfHrd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Model 4: Multiple Transformer blocks<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"IDW3AGijSPl9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# pytorch stuff\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","source":["#n load GPT2 tokenizer\n","from transformers import GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"4txp8aK9K_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B3ap39h0Jjdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"Gi-0zIzafVUo"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8 # aka context window\n","n_vocab = tokenizer.vocab_size\n","\n","# model hyperparameters\n","embed_dim = 128\n","nTransformerBlocks = 12\n","\n","\n","# training hyperparameters\n","batch_size = 5"],"metadata":{"id":"u0j1AmufJjg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LAp4k3HzfXCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# #n One attention head"],"metadata":{"id":"GjK4KWn9jQXO"}},{"cell_type":"code","source":["# create one attention head\n","class OneAttentionHead(nn.Module):\n","  def __init__(self,embed_dim):\n","    super().__init__()\n","\n","    # create the k/q/v matrices\n","    self.key   = nn.Linear(embed_dim,embed_dim,bias=False)\n","    self.query = nn.Linear(embed_dim,embed_dim,bias=False)\n","    self.value = nn.Linear(embed_dim,embed_dim,bias=False)\n","    self.W0    = nn.Linear(embed_dim,embed_dim,bias=False)\n","\n","  def forward(self,x):\n","\n","    # run the token embeddings vectors through attention\n","    k = self.key(x)\n","    q = self.query(x)\n","    v = self.value(x)\n","    y = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n","    y = self.W0(y) # linear weightings post-attention\n","\n","    return y"],"metadata":{"id":"o0Vq7bFElmwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6IKz5AQC6R7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# #n Transformer block"],"metadata":{"id":"E2-qzdxn8FiX"}},{"cell_type":"code","source":["#\n","class TransformerBlock(nn.Module):\n","  def __init__(self,embed_dim):\n","    super().__init__()\n","\n","    # attention sublayer\n","    self.layerNormAttn = nn.LayerNorm(embed_dim)\n","    self.attn = OneAttentionHead(embed_dim)\n","\n","    # feedforward (MLP) sublayer\n","    self.layerNormMLP  = nn.LayerNorm(embed_dim)\n","    self.W1   = nn.Linear(embed_dim,4*embed_dim) # 4x expansion\n","    self.gelu = nn.GELU()                        # nonlinearity\n","    self.W2   = nn.Linear(4*embed_dim,embed_dim) # 4x contraction\n","\n","\n","  def forward(self,x):\n","\n","    ## -------- attention sublayer -------- ##\n","    x = x + self.attn(self.layerNormAttn(x)) #\n","    ## ------------------------------------ ##\n","\n","    ## -------------------- MLP sublayer -------------------- ##\n","    y = x + self.W2(self.gelu(self.W1(self.layerNormMLP(x))))  #\n","    ## ------------------------------------------------------ ##\n","\n","    return y"],"metadata":{"id":"8c8TIZzvK51S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4ke3wJnG8Hww"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The full model"],"metadata":{"id":"m8CHi6rE8Ht-"}},{"cell_type":"code","source":["# the full model class, which calls the previously defined classes\n","class LanguageModel(nn.Module):\n","  def __init__(self,nTransformerBlocks,embed_dim):\n","    super().__init__()\n","\n","    # embedding matrices\n","    self.embedding = nn.Embedding(n_vocab,embed_dim)\n","    self.positions = nn.Embedding(seq_len,embed_dim)\n","\n","\n","    #n multiple Transformer blocks\n","    self.transformerBlocks = nn.Sequential(*[TransformerBlock(embed_dim) for _ in range(nTransformerBlocks)])\n","\n","\n","    # embedding to output (linear) layer\n","    self.finalLayerNorm = nn.LayerNorm(embed_dim) # final layernorm after transformers and before unembeddings\n","    self.finalLinear = nn.Linear(embed_dim,n_vocab,bias=False)\n","\n","    # the final output layer is tied to the token embedding\n","    self.finalLinear.weight = nn.Parameter(self.embedding.weight)\n","\n","\n","  def forward(self,tokx):\n","\n","    ## --------------------- embeddings --------------------- ##\n","    token_embed = self.embedding(tokx)\n","    posit_embed = self.positions(torch.arange(tokx.shape[-1])) # [seq_len x embedding_dim]\n","    x = token_embed + posit_embed # [batch, seq_len, embedding_dim]\n","    ## ------------------------------------------------------ ##\n","\n","\n","    #n\n","    ## --- transformer blocks --- ##\n","    x = self.transformerBlocks(x)\n","    ## -------------------------- ##\n","\n","\n","\n","    ## - finally, unembeddings - ##\n","    x = self.finalLayerNorm(x)\n","    x = self.finalLinear(x)\n","    ## ------------------------- ##\n","\n","    return x # not returning attention matrices like in model3\n","\n","\n","  def generate(self,tokx,temperature=1.,n_new_tokens=50):\n","\n","    for _ in range(n_new_tokens):\n","\n","      # get predictions, but only from the past seq_len tokens\n","      x = self(tokx[:,-seq_len:]) # [batch, seq_len, n_vocab]\n","\n","      # extract the final token to predict the next\n","      x = x[:,-1,:] # [batch, n_vocab]\n","\n","      # apply softmax to get probability values over all tokens in the vocab - with temperature\n","      probs = F.softmax(x/temperature,dim=-1) # [batch, n_vocab]\n","\n","      # probabilistically sample from the distribution\n","      tokx_next = torch.multinomial(probs,num_samples=1) # [batch, 1]\n","\n","      # append\n","      tokx = torch.cat( (tokx,tokx_next),dim=1) # [batch, (tokens+1)]\n","    return tokx\n"],"metadata":{"id":"lpG1Af9RjQUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZjB2dHKNhJFD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create a model instance and inspect"],"metadata":{"id":"fCq05DDcbc-t"}},{"cell_type":"code","source":["llm = LanguageModel(nTransformerBlocks,embed_dim)\n","llm"],"metadata":{"id":"SQoC6PTqbcRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm.transformerBlocks[4]"],"metadata":{"id":"dCGspdN3fRhT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm.transformerBlocks[4].attn"],"metadata":{"id":"s9mBdxMmfRef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm.transformerBlocks[4].attn.query.weight.detach().cpu().numpy()"],"metadata":{"id":"QgUUmHqif6h0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XoyTa-GUWh-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create data\n","tokens = tokenizer.encode('I prefer oat milk in my coffee.')\n","X = torch.tensor(tokens[:-1]).unsqueeze(0)\n","y = torch.tensor(tokens[1:]).unsqueeze(0)\n","\n","print(X.shape)\n","print(y.shape)"],"metadata":{"id":"D-WQgk4GhiXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = llm(X)\n","\n","print(out.shape)"],"metadata":{"id":"uIkkbfL9hiUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IQkqi7gRhiPN"},"execution_count":null,"outputs":[]}]}