{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1jmWytQcLQdHCqDzcEt7q1GICCdPBxZ3T","timestamp":1742377210164}],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPv47i1vKTPExCiV/7BQJZT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Pretrain LLMs<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Train model 1 with GPT2's embeddings<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"5dozf8utCbaf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import requests\n","import matplotlib.pyplot as plt\n","\n","# note the random_split\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","source":["# GPT-2's tokenizer\n","from transformers import GPT2Tokenizer,AutoModelForCausalLM\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"4txp8aK9K_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jKFC937nXs8V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cHj5D0oSDMoF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"Gi-0zIzafVUo"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8 # aka context length\n","stride = 2\n","\n","# model hyperparameters\n","embed_dim = 768\n","\n","# training hyperparameters\n","batch_size = 64"],"metadata":{"id":"u0j1AmufJjg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LAp4k3HzfXCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Data and model"],"metadata":{"id":"r0ntfbrPH9RF"}},{"cell_type":"code","source":["# import and tokenize the text\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","tmTokens = torch.tensor( tokenizer.encode(text) )"],"metadata":{"id":"4ROffDzOJqh7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a class for a dataset (note: batching is done by the DataLoader, not in the dataset)\n","class TokenDataset(Dataset):\n","  def __init__(self, tokens, seq_len=8, stride=4):\n","\n","    # initialize\n","    self.inputs  = []\n","    self.targets = []\n","\n","    # overlapping sequences of seq_len\n","    for i in range(0,len(tokens)-seq_len,stride):\n","\n","      # get c tokens and append to the lists\n","      self.inputs.append(\n","      self.targets.append(\n","\n","  def __len__(self):\n","    return len(self.inputs)\n","\n","  def __getitem__(self, idx):\n","    return self.inputs[idx], self.targets[idx]"],"metadata":{"id":"-d-Yg-XgJjYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance!\n","token_dataset = TokenDataset(tmTokens,seq_len,stride)\n","\n","# find the sizes\n","train_ratio = .9\n","train_size = int(train_ratio * len(token_dataset))\n","test_size  = len(token_dataset) - train_size\n","\n","# create train/test subsets\n","\n","\n","# create DataLoaders\n","\n","\n","print(f'Train set has {train_size:5} sequences.')\n","print(f'Test set has  {test_size:5} sequences.')"],"metadata":{"id":"OKCFDygz7XHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QNwmxlH40hGe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The model"],"metadata":{"id":"GjK4KWn9jQXO"}},{"cell_type":"code","source":["class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # embedding matrix\n","    self.embedding = nn.Embedding(,)\n","\n","    # embedding to output (linear) layer\n","    self.gelu =\n","    self.finalLinear = nn.Linear(,,bias=False)\n","\n","\n","\n","  def forward(self,tokx):\n","\n","    # forward pass\n","\n","    # return log-softmax\n","    return\n","\n","  def generate(self,tokx,n_new_tokens=30):\n","    for _ in range(n_new_tokens):\n","\n","    return tokx\n"],"metadata":{"id":"lpG1Af9RjQUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tshrRRYIZ-BP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Copy GPT2's embeddings onto Model 1"],"metadata":{"id":"vophfgNMZ__v"}},{"cell_type":"code","source":["# import the GPT2 small model\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')"],"metadata":{"id":"EUtazBQQXv8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance of our model\n","model ="],"metadata":{"id":"SQoC6PTqbcRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check that the sizes match\n","print(f'My model embedding:   {.shape}')\n","print(f'GPT2 model embedding: {.shape}')"],"metadata":{"id":"oc5c02g-aEL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and copy over\n"],"metadata":{"id":"JdY4VvbKcAMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check for equality\n"],"metadata":{"id":"E9FhirCXzr-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MZG5dtUQcp-m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Train the data with frozen embeddings"],"metadata":{"id":"PgsjpuaLcp75"}},{"cell_type":"code","source":["# toggle this ON if you're working on exercise 3 :P\n","this_is_exercise3 = True\n","\n","if this_is_exercise3:\n","\n","  # initial state\n","  print(model.embedding.weight.requires_grad)\n","\n","  # switch it off\n","\n","\n","  # confirm\n","  print("],"metadata":{"id":"w2wZeDfzctWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LmHK5ZJ2cEdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare for training"],"metadata":{"id":"Hc9NQl0Hzr7U"}},{"cell_type":"code","source":["# push the model to the GPU\n"],"metadata":{"id":"Ae0mZC83y7tu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the loss and optimizer functions\n","\n"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CJSN1JCuy7q6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now train the model!"],"metadata":{"id":"gICE2otK-s89"}},{"cell_type":"code","source":["num_epochs = 10\n","\n","# initialize losses\n","train_loss = []\n","test_loss = []\n","\n","for epoch in range(num_epochs):\n","\n","  # initialize\n","  epoch_loss = 0\n","\n","  # loop over batches in the data loader\n","  for X,y in train_dataloader:\n","\n","    # move data to GPU\n","\n","\n","    # clear previous gradients\n","\n","\n","    # forward pass\n","\n","\n","    # calculate the losses on the (reshaped) final target word\n","\n","\n","    # backprop\n","\n","\n","    # sum the per-epoch losses\n","\n","  #- loop over batches ends here\n","\n","\n","  # evaluate the model with the test set\n","  with torch.no_grad():\n","    testloss = 0 # reset the test loss\n","    for X,y in test_dataloader:\n","\n","\n","\n","\n","  # scale the train loss by the number of tokens in this dataloader\n","  train_loss.append(epoch_loss / len(train_dataloader))\n","  test_loss.append(testloss / len(test_dataloader))\n","\n","  # update our progress :)\n","  print(f'Epoch {epoch+1:2}, train loss: {epoch_loss / len(train_dataloader):.4f}, test loss: {testloss/len(test_dataloader):.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XoyTa-GUWh-c"},"execution_count":null,"outputs":[]}]}