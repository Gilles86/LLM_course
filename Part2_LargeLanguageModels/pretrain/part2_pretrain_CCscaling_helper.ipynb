{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMP5SAEm5XdQhPJeb0G8w8S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Pretrain LLMs<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Numerical scaling issues in DL models<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"LNAqDwgWCZs4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-a-SpjzK_vB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","\n","# vector figs\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"BuDsaLVaLA2I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Explore an example"],"metadata":{"id":"6Ho4Z0F_LAzS"}},{"cell_type":"code","source":["# create two normal-random matrices\n","q = np.random.randn(50,50)\n","k =\n","\n","# all pairwise dot products via matrix multiplication\n","dp ="],"metadata":{"id":"8ORlhQ87LAw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the standard deviations\n","std_q  =\n","std_k  =\n","std_dp =\n","\n","print(f'Standard deviation of q:  {std_q:.4f}')\n","print(f'Standard deviation of k:  {std_k:.4f}')\n","print(f'Standard deviation of dp: {std_dp:.4f}')\n","print(f'Square root of dimension: {}')"],"metadata":{"id":"7nAnuPE4LAuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,3))\n","\n","q_flat =\n","k_flat =\n","dp_flat =\n","n =\n","\n","plt.plot(,'ks',markerfacecolor=[.9,.7,.7],alpha=.4,markersize=9)\n","plt.plot(k\n","plt.plot(dp_\n","\n","plt.gca().set(yticks=[0,1,2],yticklabels=['$QK^T$','$K$','$Q$',],ylim=[-.75,2.75],\n","              xlabel='Value')\n","plt.show()"],"metadata":{"id":"FVNzDth7Id0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1kPt1I74LArd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: A parametric experiment"],"metadata":{"id":"DGfoch8iWvnW"}},{"cell_type":"code","source":["vector_lengths = np.arange(2,100)\n","\n","dp_stds = np.zeros(len(vector_lengths))\n","\n","for l in range(len(vector_lengths)):\n","\n","  # create two matrices\n","\n","\n","  # their dot products\n","\n","\n","  # their std\n","\n","\n","\n","plt.figure(figsize=(8,4))\n","\n","plt.legend()\n","plt.gca().set(xlabel='Vector size',ylabel='Standard deviation',title='Standard deviation of multiplied Gaussian noise')\n","plt.show()"],"metadata":{"id":"dW1TmBfILAo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uQTukRAcLAjo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Implications for softmax"],"metadata":{"id":"E32VHb0TLAgi"}},{"cell_type":"code","source":["\n","\n","_,axs = plt.subplots(2,2,figsize=(12,6))\n","for i in range(2):\n","\n","  # possible scaling\n","\n","  # calculate softmax and negative log llikelihood\n","  dps_flat =\n","  softmax =\n","  nll =\n","\n","  # and plot\n","  axs[i,0].plot(\n","  axs[i,0].set(xlabel='Data index',ylabel='Softmax prob.',title='Softmaxified logits ('+['unscaled)','scaled)'][i])\n","\n","  axs[i,1].plot(\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"NY5ovhN5LAdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k_vjGNv1LAay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Check GPT2's layernorm parameters"],"metadata":{"id":"X3WYgB_napnR"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gather all layernorm parameters into vectors\n","all_ln_weights = np.array([])\n","all_ln_biases = np.array([])\n","\n","for name,mat in gpt2.named_parameters():\n","  if 'ln' in name:\n",""],"metadata":{"id":"F_5g5PY-aphR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate their distributions\n","yW,xW = np.histogram()\n","yB,xB = np.histogram()\n","\n","# and plot\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","axs[0].plot(xW)\n","axs[0].set(xlabel='Stretching parameter value',ylabel='Count (log)',title='Stretch parameter learned in GPT2')\n","\n","axs[1].plot(\n","axs[1].set(xlabel='Shifting parameter value',ylabel='Count (log)',title='Shift parameter learned in GPT2')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t_R3Tq_dLAXs"},"execution_count":null,"outputs":[]}]}