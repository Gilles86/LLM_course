{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1jmWytQcLQdHCqDzcEt7q1GICCdPBxZ3T","timestamp":1742377210164}],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMr9uZ3kq20CjbR8gy+9/Pb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Pretrain LLMs<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Train a model to like \"X\"<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"aBqRYTG8CYPH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# pytorch stuff\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# for printing\n","import textwrap"]},{"cell_type":"code","source":["# GPT-2's tokenizer\n","from transformers import GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"4txp8aK9K_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use the GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"B3ap39h0Jjdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cHj5D0oSDMoF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: The model and its “x” preference"],"metadata":{"id":"bZQyNDLebsRK"}},{"cell_type":"markdown","source":["### Model 5 all in one cell"],"metadata":{"id":"9q0SBPfdxUmy"}},{"cell_type":"code","source":["# hyperparameters for GPT2-124M\n","n_vocab    = 50257     # GPT-2 vocab size\n","embed_dim  =   768     # embedding dimension\n","seq_len    =   256     # max sequence length\n","n_heads    =    12     # attention heads\n","n_blocks   =    12     # transformer blocks\n","batch_size =    16\n","\n","\n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # number of attention heads\n","    self.num_heads =\n","    self.head_dim  =\n","\n","    # the three Q,K,V weights matrices are initialized as one, and are split inside forward()\n","    self.QKV =\n","\n","    # linear mixing after attention\n","    self.W0 =\n","\n","\n","  def forward(self,x):\n","\n","    # sizes for later use\n","    B, T, E = x.shape # [batch, seq_len, embed_dim]\n","\n","    # push data through Q, K, and V in one concatenated matrix, then split into three\n","    qkv =\n","    q,k,v =\n","\n","    # reshape to [B, T, nHeads, head_dim]\n","    #  and then transpose to [B, nHeads, T, head_dim]\n","    q =\n","    k =\n","    v =\n","\n","    # Pytorch's dot-product attention function handles multi-head shapes\n","    out =\n","\n","    # recombine heads: (B, nHeads, T, head_dim) -> [B, T, E]\n","    out =\n","\n","    # finally, linearly mix the attention heads\n","    out =\n","\n","    return out\n","\n","\n","\n","\n","class TransformerBlock(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    ### attention subblock\n","    self.layernorm_1 = nn.LayerNorm\n","    self.attn = MultiHeadAttention()\n","\n","\n","    ### linear feedforward (MLP) subblock\n","    self.layernorm_2 =\n","    # 4x expansion, then back to embedding size\n","    self.mlp_1 = nn.Linear\n","    self.gelu  = nn.GELU()\n","    self.mlp_2 = nn.Linear\n","\n","  def forward(self, x):\n","\n","    # attention\n","\n","\n","    # MLP\n","\n","\n","    return x_ff\n","\n","\n","\n","class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # token + position embeddings\n","    self.wte =\n","    self.wpe =\n","\n","    # transformer blocks\n","    self.transformerBlocks = nn.Sequential(*[TransformerBlock() for _ in range(n_blocks)])\n","\n","    # final layernorm\n","    self.layernorm_final =\n","\n","    # lm head, with weights tied to token embedding\n","    self.final_head =\n","    self.final_head.weight =\n","\n","\n","  def forward(self, idx):\n","\n","    # token + position embeddings (note the device!)\n","\n","    x = token_emb + posit_emb # [B, T, E]\n","\n","    # pass through each transformer block\n","    x =\n","\n","    # final layernorm and unembeddings\n","    x =\n","    logits =\n","\n","    # scale and logsoftmax\n","    outputs = F.log_softmax(/,dim=)\n","\n","    return outputs\n","\n","\n","  def generate(self, idx, n_new_tokens=50):\n","\n","    return idx\n"],"metadata":{"id":"5uxH7fcuxUj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a new instance and put it on the GPU\n","model ="],"metadata":{"id":"SQoC6PTqbcRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how many generated tokens contain a target letter?\n","\n","# qualitative test: generate new tokens\n","X = torch.randint(,,).to(device)\n","Y = model.generate(,n_new_tokens=200)\n","print(textwrap.fill(tokenizer.decode(Y[0].tolist()), width=100))"],"metadata":{"id":"Q4eTGAJHuUTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# quantitative test: count the number of target-containing tokens\n","hasTarget = 0\n","for t in :\n","  if 'x' in :\n","    hasTarget +=\n","\n","print(f'{} of {} tokens have a target.')"],"metadata":{"id":"IW2OLaaOw9Z6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E9FhirCXzr-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Create a target token probability distribution"],"metadata":{"id":"dPJQBlIV486l"}},{"cell_type":"code","source":["# initialize\n","mask = torch.zeros(tokenizer.vocab_size)\n","\n","# loop over all tokens\n","for t in range(tokenizer.vocab_size):\n","\n","  # this token\n","  thistoken =\n","\n","  # if it has a target letter\n","  if\n","    mask[t] = 1\n","\n","print(f'{} out of {} ({}%) tokens have target letter \"x\"')\n","\n","# then normalize to probability dist\n","mask ="],"metadata":{"id":"U-9Dfpb_4_xm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","plt.plot(mask,'k.')\n","plt.gca().set(xlabel= ,ylabel=\n","plt.show()"],"metadata":{"id":"x10Z7_lr_24o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g2a9mAsMsFlo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Create a custom loss function"],"metadata":{"id":"FkFYgxTXpnEx"}},{"cell_type":"code","source":["class myLoss_x(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # mask: 1 if token contains a target, 0 otherwise\n","    self.mask = torch.zeros\n","\n","\n","    # normalize to pdist\n","    self.mask =\n","\n","  def forward(self, log_probs):\n","    return F.kl_div()"],"metadata":{"id":"33E370453tEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JpPGxgsJsn58"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Train the model"],"metadata":{"id":"gICE2otK-s89"}},{"cell_type":"code","source":["# create the optimizer function\n","optimizer = torch.optim.AdamW\n","\n","# and a loss function instance (on the GPU)\n","loss_function ="],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 200\n","\n","# initialize losses\n","total_loss = np.zeros(num_epochs)\n","\n","\n","for epoch in range(num_epochs):\n","\n","  # generate data and move data to GPU\n","\n","\n","  # clear previous gradients\n","\n","\n","  # forward pass\n","\n","\n","  # calculate the losses on the final token\n","\n","\n","  # backprop\n","\n","\n","  # get the loss\n","\n","\n","  # update our progress :)\n","  if epoch%25==0:\n","    print(f'Finished epoch {epoch:4} with loss {total_loss[epoch]:.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","\n","plt.gca().set(xlabel='Epoch',ylabel='Loss')\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6tuVdtLL8IA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and repeat the evals\n","\n","# qualitative eval by generating text\n","\n","print(textwrap.fill(tokenizer.decode(Y[0].tolist()), width=100))"],"metadata":{"id":"RCr8wYOsy7l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how many generated tokens contain a target letter?\n","\n","# quantitative eval by counting target appearances\n","\n","print(f'{hasTarget} of {len(Y[0][seq_len:])} tokens have a target.')"],"metadata":{"id":"XoyTa-GUWh-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eT3di2KynAWQ"},"execution_count":null,"outputs":[]}]}