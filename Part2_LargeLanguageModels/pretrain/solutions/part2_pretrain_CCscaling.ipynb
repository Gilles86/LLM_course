{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMXcxAv/tupHrRi95vZl4aj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Pretrain LLMs<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Numerical scaling issues in DL models<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"LNAqDwgWCZs4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-a-SpjzK_vB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","\n","# vector figs\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"BuDsaLVaLA2I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Explore an example"],"metadata":{"id":"6Ho4Z0F_LAzS"}},{"cell_type":"code","source":["# create two normal-random matrices\n","q = np.random.randn(50,50)\n","k = np.random.randn(50,50)\n","\n","# all pairwise dot products via matrix multiplication\n","dp = q @ k.T"],"metadata":{"id":"8ORlhQ87LAw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the standard deviations\n","std_q  = np.std(q)\n","std_k  = np.std(k)\n","std_dp = np.std(dp)\n","\n","print(f'Standard deviation of q:  {std_q:.4f}')\n","print(f'Standard deviation of k:  {std_k:.4f}')\n","print(f'Standard deviation of dp: {std_dp:.4f}')\n","print(f'Square root of dimension: {np.sqrt(q.shape[1]):.4f}')"],"metadata":{"id":"7nAnuPE4LAuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,3))\n","\n","q_flat = q.flatten()\n","k_flat = k.flatten()\n","dp_flat = dp.flatten()\n","n = len(q_flat)\n","\n","plt.plot(q_flat,np.random.normal(2,.02,n),'ks',markerfacecolor=[.9,.7,.7],alpha=.4,markersize=9)\n","plt.plot(k_flat,np.random.normal(1,.02,n),'ko',markerfacecolor=[.7,.9,.7],alpha=.4,markersize=9)\n","plt.plot(dp_flat,np.random.normal(0,.02,n),'k^',markerfacecolor=[.7,.7,.9],alpha=.4,markersize=9)\n","\n","plt.gca().set(yticks=[0,1,2],yticklabels=['$QK^T$','$K$','$Q$',],ylim=[-.75,2.75],\n","              xlabel='Value')\n","plt.show()"],"metadata":{"id":"FVNzDth7Id0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1kPt1I74LArd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: A parametric experiment"],"metadata":{"id":"DGfoch8iWvnW"}},{"cell_type":"code","source":["vector_lengths = np.arange(2,100)\n","\n","dp_stds = np.zeros(len(vector_lengths))\n","\n","for l in range(len(vector_lengths)):\n","\n","  # create two matrices\n","  q = np.random.randn(50,vector_lengths[l])\n","  k = np.random.randn(50,vector_lengths[l])\n","\n","  # their dot products\n","  dps = q@k.T\n","\n","  # their std\n","  dp_stds[l] = np.std(dps)\n","\n","\n","plt.figure(figsize=(8,4))\n","plt.plot(vector_lengths,np.sqrt(vector_lengths),'k',linewidth=2,label='Expectation')\n","plt.plot(vector_lengths,dp_stds,'ko',markersize=10,markerfacecolor=[.7,.7,.9],alpha=.5,label='Empirical')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Vector size',ylabel='Standard deviation',title='Standard deviation of multiplied Gaussian noise')\n","plt.show()"],"metadata":{"id":"dW1TmBfILAo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uQTukRAcLAjo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Implications for softmax"],"metadata":{"id":"E32VHb0TLAgi"}},{"cell_type":"code","source":["# create two vectors\n","q = np.random.randn(50,50)\n","k = np.random.randn(50,50)\n","\n","# their dot product\n","dps = q@k.T\n","\n","\n","_,axs = plt.subplots(2,2,figsize=(12,6))\n","for i in range(2):\n","\n","  # possible scaling\n","  if i==1:\n","    dps /= np.sqrt(q.shape[1])\n","\n","  # calculate softmax and nl\n","  dps_flat = dps.flatten()\n","  softmax = np.exp(dps_flat)/np.sum(np.exp(dps_flat))\n","  nll = -1/np.log(softmax)\n","\n","  # and plot\n","  axs[i,0].plot(softmax,'ko',markerfacecolor=[.9,.7,.7],alpha=.6)\n","  axs[i,0].set(xlabel='Data index',ylabel='Softmax prob.',title='Softmaxified logits ('+['unscaled)','scaled)'][i])\n","\n","  axs[i,1].plot(nll,'ko',markerfacecolor=[.7,.7,.9],alpha=.6)\n","  axs[i,1].set(xlabel='Data index',ylabel='-nll',title='Negative log-softmax ('+['unscaled)','scaled)'][i])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"NY5ovhN5LAdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k_vjGNv1LAay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Check GPT2's layernorm parameters"],"metadata":{"id":"X3WYgB_napnR"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gather all layernorm parameters into vectors\n","all_ln_weights = np.array([])\n","all_ln_biases = np.array([])\n","\n","for name,mat in gpt2.named_parameters():\n","  if 'ln' in name:\n","    if 'weight' in name:\n","      all_ln_weights = np.append(all_ln_weights,mat.data)\n","    elif 'bias' in name:\n","      all_ln_biases = np.append(all_ln_biases,mat.data)"],"metadata":{"id":"F_5g5PY-aphR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate their distributions\n","yW,xW = np.histogram(all_ln_weights,bins=np.linspace(0,3,100))\n","yB,xB = np.histogram(all_ln_biases,bins=np.linspace(-5,5,100))\n","\n","# and plot\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","axs[0].plot(xW[:-1],np.log(yW),'ks-',markerfacecolor=[.9,.7,.7])\n","axs[0].set(xlabel='Stretching parameter value',ylabel='Count (log)',title='Stretch parameter learned in GPT2')\n","\n","axs[1].plot(xB[:-1],np.log(yB),'ko-',markerfacecolor=[.7,.7,.9])\n","axs[1].set(xlabel='Shifting parameter value',ylabel='Count (log)',title='Shift parameter learned in GPT2')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t_R3Tq_dLAXs"},"execution_count":null,"outputs":[]}]}