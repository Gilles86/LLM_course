{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1jmWytQcLQdHCqDzcEt7q1GICCdPBxZ3T","timestamp":1742377210164}],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOSxQZ3sWj58Hwa/F7oORFO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Pretrain LLMs<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Train model 1 with GPT2's embeddings<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"5dozf8utCbaf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import requests\n","import matplotlib.pyplot as plt\n","\n","# note the random_split\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","source":["# GPT-2's tokenizer\n","from transformers import GPT2Tokenizer,AutoModelForCausalLM\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"4txp8aK9K_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jKFC937nXs8V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cHj5D0oSDMoF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"Gi-0zIzafVUo"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8 # aka context length\n","stride = 2\n","\n","# model hyperparameters\n","embed_dim = 768\n","\n","# training hyperparameters\n","batch_size = 64"],"metadata":{"id":"u0j1AmufJjg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LAp4k3HzfXCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Data and model"],"metadata":{"id":"r0ntfbrPH9RF"}},{"cell_type":"code","source":["# import and tokenize the text\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","tmTokens = torch.tensor( tokenizer.encode(text) )"],"metadata":{"id":"4ROffDzOJqh7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a class for a dataset (note: batching is done by the DataLoader, not in the dataset)\n","class TokenDataset(Dataset):\n","  def __init__(self, tokens, seq_len=8, stride=4):\n","\n","    # initialize\n","    self.inputs  = []\n","    self.targets = []\n","\n","    # overlapping sequences of seq_len\n","    for i in range(0,len(tokens)-seq_len,stride):\n","\n","      # get c tokens and append to the lists\n","      self.inputs.append( tokens[i   : i+seq_len])\n","      self.targets.append(tokens[i+1 : i+seq_len+1])\n","\n","  def __len__(self):\n","    return len(self.inputs)\n","\n","  def __getitem__(self, idx):\n","    return self.inputs[idx], self.targets[idx]"],"metadata":{"id":"-d-Yg-XgJjYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance!\n","token_dataset = TokenDataset(tmTokens,seq_len,stride)\n","\n","# find the sizes\n","train_ratio = .9\n","train_size = int(train_ratio * len(token_dataset))\n","test_size  = len(token_dataset) - train_size\n","\n","# create train/test subsets\n","train_dataset, test_dataset = random_split(\n","    token_dataset, [train_size, test_size]\n","   )\n","\n","# create DataLoaders\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataloader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n","\n","print(f'Train set has {train_size:5} sequences.')\n","print(f'Test set has  {test_size:5} sequences.')"],"metadata":{"id":"OKCFDygz7XHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QNwmxlH40hGe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The model"],"metadata":{"id":"GjK4KWn9jQXO"}},{"cell_type":"code","source":["class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # embedding matrix\n","    self.embedding = nn.Embedding(tokenizer.vocab_size,embed_dim)\n","\n","    # embedding to output (linear) layer\n","    self.gelu = nn.GELU()\n","    self.finalLinear = nn.Linear(embed_dim,tokenizer.vocab_size,bias=False)\n","\n","\n","\n","  def forward(self,tokx):\n","\n","    # forward pass\n","    x = self.embedding(tokx) # batch, token, embedding dimension\n","    x = self.gelu(x)\n","    x = self.finalLinear(x)\n","\n","    # return log-softmax\n","    return F.log_softmax(x,dim=-1)\n","\n","  def generate(self,tokx,n_new_tokens=30):\n","    for _ in range(n_new_tokens):\n","      x = self(tokx)\n","      x = x[:,-1,:]\n","      probs = torch.exp(x) # undo the log, keep the softmax\n","      tokx_next = torch.multinomial(probs,num_samples=1)\n","      tokx = torch.cat( (tokx,tokx_next),dim=1)\n","    return tokx\n"],"metadata":{"id":"lpG1Af9RjQUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tshrRRYIZ-BP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Copy GPT2's embeddings onto Model 1"],"metadata":{"id":"vophfgNMZ__v"}},{"cell_type":"code","source":["# import the GPT2 small model\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')"],"metadata":{"id":"EUtazBQQXv8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model()"],"metadata":{"id":"SQoC6PTqbcRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check that the sizes match\n","print(f'My model embedding:   {model.embedding.weight.shape}')\n","print(f'GPT2 model embedding: {gpt2.transformer.wte.weight.shape}')"],"metadata":{"id":"oc5c02g-aEL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and copy over\n","model.embedding.weight.data = gpt2.transformer.wte.weight.data"],"metadata":{"id":"JdY4VvbKcAMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check for equality\n","model.embedding.weight[0] - gpt2.transformer.wte.weight[0]"],"metadata":{"id":"E9FhirCXzr-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MZG5dtUQcp-m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Train the data with frozen embeddings"],"metadata":{"id":"PgsjpuaLcp75"}},{"cell_type":"code","source":["# toggle this ON if you're working on exercise 3 :P\n","this_is_exercise3 = True\n","\n","if this_is_exercise3:\n","\n","  # initial state\n","  print(model.embedding.weight.requires_grad)\n","\n","  # switch it off\n","  model.embedding.weight.requires_grad = False\n","\n","  # confirm\n","  print(model.embedding.weight.requires_grad)"],"metadata":{"id":"w2wZeDfzctWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LmHK5ZJ2cEdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare for training"],"metadata":{"id":"Hc9NQl0Hzr7U"}},{"cell_type":"code","source":["# push the model to the GPU\n","model = model.to(device)"],"metadata":{"id":"Ae0mZC83y7tu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the loss and optimizer functions\n","loss_function = nn.NLLLoss().to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=.001, weight_decay=.01)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CJSN1JCuy7q6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now train the model!"],"metadata":{"id":"gICE2otK-s89"}},{"cell_type":"code","source":["num_epochs = 10\n","\n","# initialize losses\n","train_loss = []\n","test_loss = []\n","\n","for epoch in range(num_epochs):\n","\n","  # initialize\n","  epoch_loss = 0\n","\n","  # loop over batches in the data loader\n","  for X,y in train_dataloader:\n","\n","    # move data to GPU\n","    X,y = X.to(device), y.to(device)\n","\n","    # clear previous gradients\n","    model.zero_grad()\n","\n","    # forward pass\n","    log_probs = model(X)\n","\n","    # calculate the losses on the (reshaped) final target word\n","    log_probs_flat = log_probs[:,:-1,:].reshape(-1,log_probs.shape[-1])\n","    y_flat = y[:,1:].reshape(-1)\n","    loss = loss_function(log_probs_flat, y_flat)\n","\n","    # backprop\n","    loss.backward()\n","    optimizer.step()\n","\n","    # sum the per-epoch losses\n","    epoch_loss += loss.item()\n","  #- loop over batches ends here\n","\n","\n","  # evaluate the model with the test set\n","  with torch.no_grad():\n","    testloss = 0 # reset the test loss\n","    for X,y in test_dataloader:\n","      X,y = X.to(device), y.to(device)  # push it to the GPU\n","      out = model(X)                    # forward pass\n","      out_flat = out[:,:-1,:].reshape(-1,out.shape[-1]) # reshape output\n","      y_flat = y[:,1:].reshape(-1)     # reshape targets\n","      thisloss = loss_function(out_flat, y_flat) # calculate loss\n","      testloss += thisloss.item()\n","\n","\n","\n","  # scale the train loss by the number of tokens in this dataloader\n","  train_loss.append(epoch_loss / len(train_dataloader))\n","  test_loss.append(testloss / len(test_dataloader))\n","\n","  # update our progress :)\n","  print(f'Epoch {epoch+1:2}, train loss: {epoch_loss / len(train_dataloader):.4f}, test loss: {testloss/len(test_dataloader):.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.plot(train_loss,'ks-',markerfacecolor='w',markersize=8,label='Train loss')\n","plt.plot(test_loss,'ro-',markerfacecolor='w',markersize=8,label='Test loss')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Epoch',ylabel='Loss',ylim=[3,9])\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XoyTa-GUWh-c"},"execution_count":null,"outputs":[]}]}