{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1kznMxzXR6EyrnNWL-19oUOOMZUoZnJEi","timestamp":1742398093808}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOd2IMC58DRZ1sQOJgP+H48"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Pretrain LLMs<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Train model 5 with weight inits<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"jS3lu9WhCYsy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggclU75JnzrI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"659KlDz3lpkt"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import requests\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"3ZXGg9FO9FfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameters for GPT2-124M\n","n_vocab    = 50257     # GPT-2 vocab size\n","embed_dim  =   768     # embedding dimension\n","seq_len    =   256     # max sequence length\n","n_heads    =    12     # attention heads\n","n_blocks   =    12     # transformer blocks\n","batch_size =    16\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"sGjyf07DlqdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lSirCpBeDDCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize the text\n","# Gulliver's travels :)\n","text = requests.get('https://www.gutenberg.org/cache/epub/829/pg829.txt').text\n","gtTokens = torch.tensor( tokenizer.encode(text),dtype=torch.long )\n","len(gtTokens)"],"metadata":{"id":"4ROffDzOJqh7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train/test split\n","\n","train_ratio = .9\n","\n","# index to split data\n","test_split_point = int(train_ratio*len(gtTokens))\n","\n","train_data = gtTokens[:test_split_point]\n","test_data  = gtTokens[test_split_point:]\n","\n","\n","# a function that returns a batch of data samples\n","def get_data_batch(training=True):\n","\n","  # pick the dataset to use\n","  if training:\n","    data = train_data\n","  else:\n","    data = test_data\n","\n","  # pick random indices to start\n","  ix = torch.randint(len(data)-seq_len,size=(batch_size,))\n","\n","  # get the data and targets (via broadcasting outer product)\n","  X = data[ix[:,None] + torch.arange(seq_len)]\n","  y = data[ix[:,None] + torch.arange(1,seq_len+1)]\n","  return X,y\n","\n","\n","# example\n","X,y = get_data_batch()\n","print(f'Input data (size {X.shape}):\\n',X)\n","print(f'\\n\\nTargets (size {y.shape}):\\n',y)"],"metadata":{"id":"saBD2yIXh9C4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xiRuBIVQDCvg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # number of attention heads\n","    self.num_heads = n_heads\n","    self.head_dim  = embed_dim // n_heads\n","\n","    # the three Q,K,V weights matrices are initialized as one, and are split inside forward()\n","    self.QKV = nn.Linear(embed_dim, 3*embed_dim, bias=True)\n","\n","    # linear mixing after attention\n","    self.W0 = nn.Linear(embed_dim, embed_dim, bias=True)\n","\n","\n","  def forward(self,x):\n","\n","    # sizes for later use\n","    B, T, E = x.shape # [batch, seq_len, embed_dim]\n","\n","    # push data through Q, K, and V in one concatenated matrix\n","    qkv = self.QKV(x) # [batch, sequence, 3*embed]\n","    q,k,v = torch.split(qkv,E,dim=2) # each matrix is [B, T, E]\n","\n","    # reshape to [B, T, nHeads, head_dim]\n","    #  and then transpose to [B, nHeads, T, head_dim]\n","    q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2) # [B, nHeads, T, head_dim]\n","    k = k.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n","    v = v.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n","\n","    # Pytorch's dot-product attention function handles multi-head shapes\n","    out = F.scaled_dot_product_attention(q, k, v, is_causal=True) # [B, nHeads, T, head_dim]\n","\n","    # recombine heads: (B, nHeads, T, head_dim) -> [B, T, E]\n","    out = out.transpose(1,2).view(B, T, E)\n","\n","    # finally, linearly mix the attention heads\n","    out = self.W0(out)\n","\n","    return out\n","\n","\n","\n","\n","class TransformerBlock(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    ### attention subblock\n","    self.layernorm_1 = nn.LayerNorm(embed_dim, eps=1e-5)\n","    self.attn = MultiHeadAttention()\n","\n","\n","    ### linear feedforward (MLP) subblock\n","    self.layernorm_2 = nn.LayerNorm(embed_dim, eps=1e-5)\n","    # 4x expansion, then back to embedding size\n","    self.mlp_1 = nn.Linear(embed_dim, 4*embed_dim, bias=True)\n","    self.gelu  = nn.GELU()\n","    self.mlp_2 = nn.Linear(4*embed_dim, embed_dim, bias=True)\n","\n","  def forward(self, x):\n","\n","    # attention\n","    x_att = self.layernorm_1(x) # pre-attention normalization\n","    x_att = x + self.attn(x_att) # run through attention, then add pre-attention activation (\"residual\")\n","\n","\n","    # MLP\n","    x_ff = self.layernorm_2(x_att) # pre-MLP normalization\n","    x_ff = x_att + self.mlp_2(self.gelu( self.mlp_1(x_ff) )) # adjustment from expansion-contraction\n","\n","    return x_ff"],"metadata":{"id":"hTwegsn4oQdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LanguageModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # token + position embeddings\n","    self.wte = nn.Embedding(n_vocab, embed_dim) # token embedding\n","    self.wpe = nn.Embedding(seq_len, embed_dim) # position embedding\n","\n","    # transformer blocks\n","    self.transformerBlocks = nn.Sequential(*[TransformerBlock() for _ in range(n_blocks)])\n","\n","    # final layernorm\n","    self.layernorm_final = nn.LayerNorm(embed_dim, eps=1e-5)\n","\n","    # lm head, with weights tied to token embedding\n","    self.final_head = nn.Linear(embed_dim, n_vocab, bias=False)\n","    self.final_head.weight = nn.Parameter(self.wte.weight)\n","\n","\n","\n","\n","    ### --- Exercise 1: weight initializations\n","    self.apply(self.weightInits)\n","\n","  def weightInits(self, module):\n","\n","    # initialize nn.linear to normal with std=.02\n","    if isinstance(module, nn.Linear):\n","      nn.init.normal_(module.weight,mean=0,std=.02)\n","\n","      # initialize bias terms to zero\n","      if module.bias is not None:\n","          nn.init.zeros_(module.bias)\n","\n","    # nn.Embeddings to Xavier\n","    if isinstance(module, nn.Embedding):\n","      nn.init.xavier_normal_(module.weight)\n","  ### ---\n","\n","\n","\n","\n","  def forward(self, idx):\n","\n","    # token + position embeddings (note the device!)\n","    token_emb = self.wte(idx) # [B, T, E]\n","    posit_emb = self.wpe(torch.arange(idx.shape[-1],device=device)) # [T, E]\n","    x = token_emb + posit_emb # [B, T, E]\n","\n","    # pass through each transformer block\n","    x = self.transformerBlocks(x)\n","\n","    # final layernorm and unembeddings\n","    x = self.layernorm_final(x)\n","    logits = self.final_head(x)  # [B, T, n_vocab]\n","\n","    # scale and logsoftmax\n","    outputs = F.log_softmax(logits/np.sqrt(embed_dim),dim=-1)\n","\n","    return outputs\n","\n","\n","  def generate(self, idx, max_new_tokens=50):\n","\n","    for _ in range(max_new_tokens):\n","\n","      # forward pass\n","      logits = self(idx[:,-seq_len:])  # [B, T, n_vocab]\n","      logits = logits[:,-1,:]  # last token's logits: [B, n_vocab]\n","\n","      # undo the log-softmax to get \"normal\" softmax (probability values)\n","      probs = torch.exp(logits) # [B, n_vocab]\n","\n","      # sample next token\n","      idx_next = torch.multinomial(probs, num_samples=1) # [B, 1]\n","\n","      # append\n","      idx = torch.cat((idx, idx_next), dim=1) # [B, T+1]\n","    return idx\n"],"metadata":{"id":"KOY0Sf_wtMQJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance and test with some data\n","model = LanguageModel().to(device)\n","\n","X,y = get_data_batch()\n","X,y = X.to(device), y.to(device)\n","out = model(X) # ~45s on cpu, <1s on gpu :D\n","print(f'Input size:  {X.shape}')\n","print(f'Output size: {out.shape}')"],"metadata":{"id":"VK63bdKWoQbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the weights distributions\n","print('mlp bias vector: ',model.transformerBlocks[1].mlp_1.bias.data)\n","print('std of mlp weights: ',torch.std(model.transformerBlocks[1].mlp_1.weight.data))\n","print('std of embeddings weights: ',torch.std(model.wte.weight.data))\n","print('std of positions weights: ',torch.std(model.wpe.weight.data))"],"metadata":{"id":"mc1MpnVU9Ozq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# where do the xavier std numbers come from?\n","np.sqrt(2/sum(list(model.wte.weight.shape)))"],"metadata":{"id":"Fq-LvnRjgkqD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# but it is an nn.Embedding, not nn.Linear...\n","isinstance(model.wte, nn.Embedding),isinstance(model.wte, nn.Linear)"],"metadata":{"id":"cTiNwdv_HbQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KaOHMOnPgl5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pre-train exploration..."],"metadata":{"id":"_-bnhT_7hKMU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = 'I went on holiday to Liliput and'\n","in2gpt = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n","\n","output = model.generate(in2gpt,max_new_tokens=100)\n","print(tokenizer.decode(output[0]))"],"metadata":{"id":"tqTElK80TMGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rrj3Y-njTNG1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare for training"],"metadata":{"id":"Ov7xjbKBTFow"}},{"cell_type":"code","source":["# create the loss and optimizer functions\n","loss_function = nn.NLLLoss().to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=.001, weight_decay=.01)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check loss function with sizes\n","X,y = get_data_batch()\n","X = X[0].unsqueeze(0)\n","y = y[0].unsqueeze(0)\n","log_probs = model(X.to(device))\n","\n","print(f'Model input is size:   {X.shape}')\n","print(f'Model output is size:  {log_probs.shape}')\n","print(f'Target tokens is size: {y.shape}')\n","\n","# flatten as before\n","log_probs_flat = log_probs.view(-1,log_probs.shape[-1])\n","\n","# compute the loss\n","loss = loss_function(log_probs_flat, y.view(-1).to(device))\n","print('\\nLoss:',loss)"],"metadata":{"id":"2DlmJtO18ceu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CJSN1JCuy7q6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### --- for Exercise 2: Finding the path to the attention weights\n","yh,xh = np.histogram(model.transformerBlocks[1].attn.QKV.weight.detach().cpu(),bins=np.linspace(-.1,.1,101))\n","plt.plot(xh[:-1],yh);"],"metadata":{"id":"I9gE3jtMauku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"54jxlpuaUAi6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now train the model!"],"metadata":{"id":"gICE2otK-s89"}},{"cell_type":"code","source":["num_samples = 501\n","\n","# initialize losses\n","train_loss = []\n","test_loss = []\n","attn_W_dists = np.zeros((num_samples,100))\n","attn_W_stds = np.zeros((num_samples,len(model.transformerBlocks)))\n","\n","\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  X,y = get_data_batch()\n","\n","  # move data to GPU\n","  X,y = X.to(device), y.to(device)\n","\n","  # clear previous gradients\n","  model.zero_grad(set_to_none=True)\n","\n","  # forward pass\n","  log_probs = model(X)\n","\n","  # calculate the losses on the (reshaped) targets\n","  loss = loss_function(log_probs.view(-1,log_probs.shape[-1]),y.view(-1))\n","\n","  # backprop\n","  loss.backward()\n","  optimizer.step()\n","\n","  # store the per-sample loss\n","  train_loss.append( loss.item() )\n","\n","\n","  # evaluate the model with the test set\n","  if sampli%50==0:\n","\n","\n","\n","    ### --- for Exercise 2\n","    # get the attention weights distributions\n","    hidx = 4 # just from one transformer block\n","    qkvWeights = model.transformerBlocks[hidx].attn.QKV.weight.detach().cpu()\n","    yh,xh = np.histogram(qkvWeights,bins=np.linspace(-.1,.1,101))\n","    attn_W_dists[sampli,:] = yh\n","    for hidx in range(len(model.transformerBlocks)):\n","      qkvWeights = model.transformerBlocks[hidx].attn.QKV.weight.detach().cpu()\n","      attn_W_stds[sampli,hidx] = torch.std(qkvWeights)\n","    # ----------\n","\n","\n","\n","    with torch.no_grad():\n","      X,y = get_data_batch(False)       # False -> testset data\n","      X,y = X.to(device), y.to(device)  # push it to the GPU\n","      out = model(X)                    # forward pass\n","      thisloss = loss_function(out.view(-1,out.shape[-1]),y.view(-1)) # calculate loss\n","      test_loss.append( thisloss.item() )\n","\n","      # update our progress :)\n","      print(f'Sample {sampli:4}, train loss: {train_loss[-1]:5.2f}, test loss: {test_loss[-1]:5.2f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.plot(train_loss,'k',label='Train loss')\n","plt.plot(range(0,num_samples,50),test_loss,'rs-',markerfacecolor='w',markersize=8,label='Test loss')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Epoch',ylabel='Loss')\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ygH3cjGN_SXH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vt1y7Lrt8GQA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Distributions of attention weights during training"],"metadata":{"id":"GSSfeNyScHLp"}},{"cell_type":"code","source":["# colors for each line\n","linecolors = plt.cm.plasma(np.linspace(0,1,num_samples))\n","\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","for i in range(0,num_samples,50):\n","  axs[0].plot(xh[:-1],attn_W_dists[i,:],color=linecolors[i],label=f'{i}')\n","\n","axs[0].legend()\n","axs[0].set(xlim=xh[[0,-1]],xlabel='Weight value',ylabel='Count',title='Attention weights distributions over learning')\n","\n","\n","# plot the standard deviations\n","linecolors = plt.cm.plasma(np.linspace(0,1,attn_W_stds.shape[1]))\n","for i in range(attn_W_stds.shape[1]):\n","  axs[1].plot(range(0,num_samples,50),attn_W_stds[::50,i],'ks',markerfacecolor=linecolors[i],markersize=9,\n","              label=f'Layer {i}')\n","\n","axs[1].set(xlabel='Training epoch',ylabel='Standard deviation')\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"zxVUoR1XcLaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2PKseRI-cHI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A fun little test :)"],"metadata":{"id":"1wZg_eplZBDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = 'I went on holiday to Liliput and'\n","in2gpt = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n","\n","output = model.generate(in2gpt,max_new_tokens=100)\n","print(tokenizer.decode(output[0]).replace('\\r','\\n'))"],"metadata":{"id":"4ql-aaqpZBAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GgkMTtuLTcov"},"execution_count":null,"outputs":[]}]}