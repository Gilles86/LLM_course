{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO180ExrkD3El0gLuzipsXG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Pretrain LLMs<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: SGD vs. Adam vs. AdamW<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"HNzKrc2qCZM1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSs7lMgaqMTv"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"Dzg4UhGuqNcv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pre-exercise example"],"metadata":{"id":"CPcnIlJWtc-d"}},{"cell_type":"code","source":["# initialize weight value\n","w = torch.tensor([0.], requires_grad=True)\n","\n","# target value\n","target = torch.tensor([torch.pi])\n","\n","# learning rate\n","lr = .01\n","\n","# create the optimizers\n","optimizer = torch.optim.SGD([w],lr=lr)\n","\n","# number of training iterations\n","numIters = 150\n","\n","# initialize results matrices\n","all_losses  = np.zeros(numIters)\n","all_weights = np.zeros(numIters+1)\n","all_weights[0] = w.item() # initial value\n","\n","\n","### training loop\n","for i in range(numIters):\n","\n","  # train the weight\n","  optimizer.zero_grad()\n","  loss = (w - target)**2\n","  loss.backward()\n","  optimizer.step()\n","\n","  # store the loss and update the weight value\n","  all_losses[i] = loss.item()\n","  all_weights[i+1] = w.item()\n","\n","\n","\n","#### visualization\n","_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","# plot the losses and weight values\n","axs[0].plot(range(1,numIters+1),all_losses,'k',linewidth=2)\n","axs[1].plot(range(0,numIters+1),all_weights,'k',linewidth=2)\n","\n","axs[0].set(title='Losses',ylabel='$L_2$ loss',xlabel='Training epoch')\n","axs[1].set(title='Weight',ylabel='Weight value',xlabel='Training epoch')\n","axs[1].axhline(target,linestyle='--',color=[.7,.7,.7],label='Target')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"NU4rA61HtfYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oukiNidUtc7Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: The optimizer competition :0"],"metadata":{"id":"pBqX64iDqNaG"}},{"cell_type":"code","source":["# initialize weight value\n","wSGD   = torch.tensor([2.], requires_grad=True)\n","wAdam  = torch.tensor([2.], requires_grad=True)\n","wAdamW = torch.tensor([2.], requires_grad=True)\n","\n","# target value\n","target = torch.tensor([5.])\n","\n","# learning rates (equal for exercises 1&3; changed in exercise 2)\n","learningrateSGD = .05\n","learningrateAdm = .05\n","\n","# create the optimizers\n","optimizerSGD   = torch.optim.SGD([wSGD],    lr=learningrateSGD)\n","optimizerAdam  = torch.optim.Adam([wAdam],  lr=learningrateAdm)\n","optimizerAdamW = torch.optim.AdamW([wAdamW],lr=learningrateAdm)\n","\n","# number of training iterations\n","numIters = 50\n","\n","# initialize results matrices\n","all_losses  = np.zeros((3,numIters))\n","all_weights = np.zeros((3,numIters+1))\n","all_weights[:,0] = wSGD.item()\n","\n","\n","# training loop\n","for i in range(numIters):\n","\n","  # train the SGD weight\n","  optimizerSGD.zero_grad()\n","  lossSGD = (wSGD - target)**2\n","  lossSGD.backward()\n","  optimizerSGD.step()\n","\n","  # train with Adam\n","  optimizerAdam.zero_grad()\n","  lossAdam = (wAdam - target)**2\n","  lossAdam.backward()\n","  optimizerAdam.step()\n","\n","  # train with AdamW\n","  optimizerAdamW.zero_grad()\n","  lossAdamW = (wAdamW - target)**2\n","  lossAdamW.backward()\n","  optimizerAdamW.step()\n","\n","  # store the losses and updated weight value\n","  all_losses[0,i] = lossSGD.item()\n","  all_losses[1,i] = lossAdam.item()\n","  all_losses[2,i] = lossAdamW.item()\n","\n","  all_weights[0,i+1] = wSGD.item()\n","  all_weights[1,i+1] = wAdam.item()\n","  all_weights[2,i+1] = wAdamW.item()\n",""],"metadata":{"id":"iD3jy9IrqNXL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","# labels and markers\n","optimlabels = [ 'SGD','Adam','AdamW' ]\n","markercols = [ [.7,.7,.9],[.7,.9,.7],[.9,.7,.7] ]\n","shapes = 'so^'\n","\n","\n","# loop over the optimizers and plot\n","for i in range(3):\n","\n","  # plot the losses\n","  axs[0].plot(range(1,numIters+1),all_losses[i,:],'k',marker=shapes[i],\n","              markerfacecolor=markercols[i],label=optimlabels[i])\n","\n","  # plot the weight\n","  axs[1].plot(range(0,numIters+1),all_weights[i,:],'k',marker=shapes[i],\n","              markerfacecolor=markercols[i],label=optimlabels[i])\n","\n","\n","axs[0].set(title='Losses',ylabel='$L_2$ loss',xlabel='Training epoch')\n","axs[1].set(title='Weight',ylabel='Weight value',xlabel='Training epoch')\n","axs[1].axhline(target,linestyle='--',color=[.7,.7,.7],label='Target')\n","for a in axs: a.legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"dg8gYZLGqNUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7q61vsObqNR1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Adjust the learning rates"],"metadata":{"id":"M36SBLwtqPJL"}},{"cell_type":"code","source":["# I used:\n","learningrateSGD = .04\n","learningrateAdm = .1"],"metadata":{"id":"Fa2firq9qPGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gf3_hZGw5nS4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Without resetting gradients"],"metadata":{"id":"23dDpdRqwIfV"}},{"cell_type":"code","source":["# add this line:\n","print(f'{wAdam.grad.item():7.3f} {wAdamW.grad.item():7.3f}')"],"metadata":{"id":"r9pwrG0MqNPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5njCvyTr7I0Z"},"execution_count":null,"outputs":[]}]}