{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1jmWytQcLQdHCqDzcEt7q1GICCdPBxZ3T","timestamp":1742377210164}],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNdZsSgStuiWf6iTH8NYvod"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Pretrain LLMs<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Train a model to like \"X\"<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"aBqRYTG8CYPH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# pytorch stuff\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# for printing\n","import textwrap"]},{"cell_type":"code","source":["# GPT-2's tokenizer\n","from transformers import GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"4txp8aK9K_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use the GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"B3ap39h0Jjdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cHj5D0oSDMoF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: The model and its “x” preference"],"metadata":{"id":"bZQyNDLebsRK"}},{"cell_type":"markdown","source":["### Model 5 all in one cell"],"metadata":{"id":"9q0SBPfdxUmy"}},{"cell_type":"code","source":["# hyperparameters for GPT2-124M\n","n_vocab    = 50257     # GPT-2 vocab size\n","embed_dim  =   768     # embedding dimension\n","seq_len    =   256     # max sequence length\n","n_heads    =    12     # attention heads\n","n_blocks   =    12     # transformer blocks\n","batch_size =    16\n","\n","\n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # number of attention heads\n","    self.num_heads = n_heads\n","    self.head_dim  = embed_dim // n_heads\n","\n","    # the three Q,K,V weights matrices are initialized as one, and are split inside forward()\n","    self.QKV = nn.Linear(embed_dim, 3*embed_dim, bias=True)\n","\n","    # linear mixing after attention\n","    self.W0 = nn.Linear(embed_dim, embed_dim, bias=True)\n","\n","\n","  def forward(self,x):\n","\n","    # sizes for later use\n","    B, T, E = x.shape # [batch, seq_len, embed_dim]\n","\n","    # push data through Q, K, and V in one concatenated matrix\n","    qkv = self.QKV(x) # [batch, sequence, 3*embed]\n","    q,k,v = torch.split(qkv,E,dim=2) # each matrix is [B, T, E]\n","\n","    # reshape to [B, T, nHeads, head_dim]\n","    #  and then transpose to [B, nHeads, T, head_dim]\n","    q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2) # [B, nHeads, T, head_dim]\n","    k = k.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n","    v = v.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n","\n","    # Pytorch's dot-product attention function handles multi-head shapes\n","    out = F.scaled_dot_product_attention(q, k, v, is_causal=True) # [B, nHeads, T, head_dim]\n","\n","    # recombine heads: (B, nHeads, T, head_dim) -> [B, T, E]\n","    out = out.transpose(1,2).view(B, T, E)\n","\n","    # finally, linearly mix the attention heads\n","    out = self.W0(out)\n","\n","    return out\n","\n","\n","\n","\n","class TransformerBlock(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    ### attention subblock\n","    self.layernorm_1 = nn.LayerNorm(embed_dim, eps=1e-5)\n","    self.attn = MultiHeadAttention()\n","\n","\n","    ### linear feedforward (MLP) subblock\n","    self.layernorm_2 = nn.LayerNorm(embed_dim, eps=1e-5)\n","    # 4x expansion, then back to embedding size\n","    self.mlp_1 = nn.Linear(embed_dim, 4*embed_dim, bias=True)\n","    self.gelu  = nn.GELU()\n","    self.mlp_2 = nn.Linear(4*embed_dim, embed_dim, bias=True)\n","\n","  def forward(self, x):\n","\n","    # attention\n","    x_att = self.layernorm_1(x) # pre-attention normalization\n","    x_att = x + self.attn(x_att) # run through attention, then add pre-attention activation (\"residual\")\n","\n","\n","    # MLP\n","    x_ff = self.layernorm_2(x_att) # pre-MLP normalization\n","    x_ff = x_att + self.mlp_2(self.gelu( self.mlp_1(x_ff) )) # adjustment from expansion-contraction\n","\n","    return x_ff\n","\n","\n","\n","class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # token + position embeddings\n","    self.wte = nn.Embedding(n_vocab, embed_dim) # token embedding\n","    self.wpe = nn.Embedding(seq_len, embed_dim) # position embedding\n","\n","    # transformer blocks\n","    self.transformerBlocks = nn.Sequential(*[TransformerBlock() for _ in range(n_blocks)])\n","\n","    # final layernorm\n","    self.layernorm_final = nn.LayerNorm(embed_dim, eps=1e-5)\n","\n","    # lm head, with weights tied to token embedding\n","    self.final_head = nn.Linear(embed_dim, n_vocab, bias=False)\n","    self.final_head.weight = nn.Parameter(self.wte.weight)\n","\n","\n","  def forward(self, idx):\n","\n","    # token + position embeddings (note the device!)\n","    token_emb = self.wte(idx) # [B, T, E]\n","    posit_emb = self.wpe(torch.arange(idx.shape[-1],device=device)) # [T, E]\n","    x = token_emb + posit_emb # [B, T, E]\n","\n","    # pass through each transformer block\n","    x = self.transformerBlocks(x)\n","\n","    # final layernorm and unembeddings\n","    x = self.layernorm_final(x)\n","    logits = self.final_head(x)  # [B, T, n_vocab]\n","\n","    # scale and logsoftmax\n","    outputs = F.log_softmax(logits/np.sqrt(embed_dim),dim=-1)\n","\n","    return outputs\n","\n","\n","  def generate(self, idx, n_new_tokens=50):\n","\n","    for _ in range(n_new_tokens):\n","\n","      # forward pass\n","      logits = self(idx[:,-seq_len:])  # [B, T, n_vocab]\n","      logits = logits[:,-1,:]  # last token's logits: [B, n_vocab]\n","\n","      # undo the log-softmax to get \"normal\" softmax (probability values)\n","      probs = torch.exp(logits) # [B, n_vocab]\n","\n","      # sample next token\n","      idx_next = torch.multinomial(probs, num_samples=1) # [B, 1]\n","\n","      # append\n","      idx = torch.cat((idx, idx_next), dim=1) # [B, T+1]\n","    return idx\n"],"metadata":{"id":"5uxH7fcuxUj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a new instance and put it on the GPU\n","model = Model().to(device)"],"metadata":{"id":"SQoC6PTqbcRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how many generated tokens contain a target letter?\n","\n","# qualitative test: generate new tokens\n","X = torch.randint(0,tokenizer.vocab_size,(1,seq_len)).to(device)\n","Y = model.generate(X,n_new_tokens=200)\n","print(textwrap.fill(tokenizer.decode(Y[0].tolist()), width=100))"],"metadata":{"id":"Q4eTGAJHuUTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# quantitative test: count the number of target-containing tokens\n","hasTarget = 0\n","for t in Y[0][seq_len:]:\n","  if 'x' in tokenizer.decode(t):\n","    hasTarget += 1\n","\n","print(f'{hasTarget} of {len(Y[0][seq_len:])} tokens have a target.')"],"metadata":{"id":"IW2OLaaOw9Z6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E9FhirCXzr-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Create a target token probability distribution"],"metadata":{"id":"dPJQBlIV486l"}},{"cell_type":"code","source":["# initialize\n","mask = torch.zeros(tokenizer.vocab_size)\n","\n","# loop over all tokens\n","for t in range(tokenizer.vocab_size):\n","\n","  # this token\n","  thistoken = tokenizer.decode([t])\n","\n","  # if it has a target letter\n","  if 'x' in thistoken:\n","    mask[t] = 1\n","\n","print(f'{int(sum(mask))} out of {len(mask):,} ({100*mask.mean():.2f}%) tokens have target letter \"x\"')\n","\n","# then normalize to probability dist\n","mask = mask/torch.sum(mask)"],"metadata":{"id":"U-9Dfpb_4_xm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","plt.plot(mask,'k.')\n","plt.gca().set(xlabel='Token index',ylabel='Probability')\n","plt.show()"],"metadata":{"id":"x10Z7_lr_24o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g2a9mAsMsFlo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Create a custom loss function"],"metadata":{"id":"FkFYgxTXpnEx"}},{"cell_type":"code","source":["class myLoss_x(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # mask: 1 if token contains a target, 0 otherwise\n","    self.mask = torch.zeros(tokenizer.vocab_size, device=device)\n","    for t in range(tokenizer.vocab_size):\n","      thistoken = tokenizer.decode([t])\n","      if 'x' in thistoken:\n","        self.mask[t] = 1\n","\n","    # normalize to pdist\n","    self.mask = self.mask/torch.sum(self.mask)\n","\n","  def forward(self, log_probs):\n","    return F.kl_div(log_probs, self.mask, reduction='batchmean')"],"metadata":{"id":"33E370453tEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JpPGxgsJsn58"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Train the model"],"metadata":{"id":"gICE2otK-s89"}},{"cell_type":"code","source":["# create the optimizer function\n","optimizer = torch.optim.AdamW(model.parameters(), lr=.001, weight_decay=.01)\n","\n","# and a loss function instance\n","loss_function = myLoss_x().to(device)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 200\n","\n","# initialize losses\n","total_loss = np.zeros(num_epochs)\n","\n","\n","for epoch in range(num_epochs):\n","\n","  # generate data and move data to GPU\n","  X = torch.randint(0,tokenizer.vocab_size,(batch_size,seq_len)).to(device)\n","\n","  # clear previous gradients\n","  optimizer.zero_grad()\n","\n","  # forward pass\n","  log_probs = model(X)\n","\n","  # calculate the losses on the final token\n","  loss = loss_function(log_probs[:,-1,:])\n","\n","  # backprop\n","  loss.backward()\n","  optimizer.step()\n","\n","  # get the loss\n","  total_loss[epoch] = loss.item()\n","\n","  # update our progress :)\n","  if epoch%25==0:\n","    print(f'Finished epoch {epoch:4} with loss {total_loss[epoch]:.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.plot(total_loss,'ks-',markerfacecolor='w',markersize=8)\n","plt.gca().set(xlabel='Epoch',ylabel='Loss')\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6tuVdtLL8IA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and repeat the evals\n","X = torch.randint(0,tokenizer.vocab_size,(1,seq_len)).to(device)\n","Y = model.generate(X,n_new_tokens=200)\n","print(textwrap.fill(tokenizer.decode(Y[0].tolist()), width=100))"],"metadata":{"id":"RCr8wYOsy7l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how many generated tokens contain a target letter?\n","hasTarget = 0\n","for t in Y[0][seq_len:]:\n","  if 'x' in tokenizer.decode(t):\n","    hasTarget += 1\n","\n","print(f'{hasTarget} of {len(Y[0][seq_len:])} tokens have a target.')"],"metadata":{"id":"XoyTa-GUWh-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eT3di2KynAWQ"},"execution_count":null,"outputs":[]}]}