{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1jmWytQcLQdHCqDzcEt7q1GICCdPBxZ3T","timestamp":1742377210164}],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPaVlsXt/zKsweH+0YBkBn8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Pretrain LLMs<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Train model 1<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"tq6zNUt5CVlU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import requests\n","import matplotlib.pyplot as plt\n","\n","# pytorch stuff\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# GPT-2's tokenizer\n","from transformers import GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"4txp8aK9K_dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use the GPU for speed\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"B3ap39h0Jjdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cHj5D0oSDMoF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"Gi-0zIzafVUo"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8 # context length\n","stride = 2\n","\n","# model hyperparameters\n","embed_dim = 2**6 # 64\n","\n","# training hyperparameters\n","batch_size = 64"],"metadata":{"id":"u0j1AmufJjg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LAp4k3HzfXCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get data"],"metadata":{"id":"frpSLHaPJpeq"}},{"cell_type":"code","source":["# tokenize the text and make it a tensor\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","tmTokens = torch.tensor( tokenizer.encode(text) )\n","len(tmTokens)"],"metadata":{"id":"4ROffDzOJqh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DataLoader"],"metadata":{"id":"8BDcTdUxJjbM"}},{"cell_type":"code","source":["# create a class for a dataset (note: batching is done by the DataLoader, not in the dataset)\n","class TokenDataset(Dataset):\n","  def __init__(self, tokens, seq_len=8, stride=4):\n","\n","    # initialize\n","    self.inputs  = []\n","    self.targets = []\n","\n","    # overlapping sequences of seq_len\n","    for i in range(0,len(tokens)-seq_len,stride):\n","\n","      # get c tokens and append to the lists\n","      self.inputs.append( tokens[i   : i+seq_len])\n","      self.targets.append(tokens[i+1 : i+seq_len+1])\n","\n","  def __len__(self):\n","    return len(self.inputs)\n","\n","  def __getitem__(self, idx):\n","    return self.inputs[idx], self.targets[idx]\n"],"metadata":{"id":"-d-Yg-XgJjYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance!\n","token_dataset = TokenDataset(tmTokens,seq_len,stride)\n","\n","token_dataloader = DataLoader(token_dataset, batch_size=batch_size, shuffle=True)\n","next(iter(token_dataloader))"],"metadata":{"id":"OKCFDygz7XHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QNwmxlH40hGe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The model"],"metadata":{"id":"GjK4KWn9jQXO"}},{"cell_type":"code","source":["class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # embedding matrix\n","    self.embedding = nn.Embedding(tokenizer.vocab_size,embed_dim)\n","\n","    # embedding to output (linear) layer\n","    self.gelu = nn.GELU()\n","    self.finalLinear = nn.Linear(embed_dim,tokenizer.vocab_size)\n","\n","\n","\n","  def forward(self,tokx):\n","\n","    # forward pass\n","    x = self.embedding(tokx) # [batch, token, embeddings]\n","    x = self.gelu(x)\n","    x = self.finalLinear(x) # [embeddings, vocab]\n","\n","    #n return log-softmax\n","    return F.log_softmax(x,dim=-1)\n","\n","  def generate(self,tokx,n_new_tokens=30):\n","    # mostly same as in DULM_buildGPT_model1 but without comments\n","    for _ in range(n_new_tokens):\n","      x = self(tokx)\n","      x = x[:,-1,:]\n","      probs = torch.exp(x) #n undo the log but keep the softmax\n","      nextToken = torch.multinomial(probs,num_samples=1)\n","      tokx = torch.cat( (tokx,nextToken),dim=1)\n","    return tokx\n"],"metadata":{"id":"lpG1Af9RjQUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1R630QmAEaxY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Check the model output"],"metadata":{"id":"fCq05DDcbc-t"}},{"cell_type":"code","source":["model = Model()\n","X,y = token_dataset[4]\n","out = model(X)\n","\n","print(X.shape)\n","print(y.shape)\n","print(out.shape) # confirm torch.sum(torch.exp(out))==1"],"metadata":{"id":"SQoC6PTqbcRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X)\n","print(y)\n","print(torch.argmax(out))"],"metadata":{"id":"ODWHQSD1binT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k4JIvM0qC3wv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E9FhirCXzr-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare for training"],"metadata":{"id":"Hc9NQl0Hzr7U"}},{"cell_type":"code","source":["# push the model to the GPU\n","model = model.to(device)"],"metadata":{"id":"Ae0mZC83y7tu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the loss and optimizer functions\n","loss_function = nn.NLLLoss().to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=.001, weight_decay=.01)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check loss function with sizes\n","X,y = next(iter(token_dataloader))\n","log_probs = model(X.to(device))\n","\n","print(f'Model input is size:   {X.shape}')\n","print(f'Model output is size:  {log_probs.shape}')\n","print(f'Target tokens is size: {y.shape}')\n","\n","# uh oh...\n","loss_function(log_probs[:,:-1,:],y[:,1:].to(device))"],"metadata":{"id":"2DlmJtO18ceu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# flatten to [batch*token, vocab]\n","log_probs_flat = log_probs[:,:-1,:].reshape(-1,log_probs.shape[-1])\n","\n","# flatten to a vector\n","y_flat = y[:,1:].reshape(-1)\n","\n","print(f'Model output is size:  {log_probs_flat.shape}')\n","print(f'Target tokens is size: {y_flat.shape}')\n","\n","# Now compute the loss\n","loss = loss_function(log_probs_flat, y_flat.to(device))\n","print('\\nLoss:',loss)"],"metadata":{"id":"6mwGMlBt9tJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CJSN1JCuy7q6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some model generated text (compare with post-training)\n","\n","# needs to be a tensor with one batch\n","startToks = torch.tensor(tokenizer.encode('I thought the Eloi would be smarter than')).unsqueeze(0)\n","\n","# text generation\n","Y = model.generate(startToks.to(device))\n","print(tokenizer.decode(Y[0].tolist()))"],"metadata":{"id":"qIEM8SMEC3qd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ufa6ou04DFMb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now train the model!"],"metadata":{"id":"gICE2otK-s89"}},{"cell_type":"code","source":["num_epochs = 25\n","\n","# initialize losses\n","total_loss = np.zeros(num_epochs)\n","\n","\n","\n","# training loop\n","for epoch in range(num_epochs):\n","\n","  # initialize batch losses to accumulate\n","  epoch_loss = 0\n","\n","  # loop over batches in the data loader\n","  for X,y in token_dataloader:\n","\n","    # move data to GPU\n","    X,y = X.to(device), y.to(device)\n","\n","    # clear previous gradients\n","    model.zero_grad()\n","\n","    # forward pass\n","    log_probs = model(X)\n","\n","    # calculate the losses on the (reshaped) final target word\n","    log_probs_flat = log_probs[:,:-1,:].reshape(-1,log_probs.shape[-1]) # tokens 0:N-1\n","    y_flat = y[:,1:].reshape(-1) # tokens 1:N\n","    loss = loss_function(log_probs_flat, y_flat)\n","\n","    # backprop\n","    loss.backward()\n","    optimizer.step()\n","\n","    # sum the batch loss\n","    epoch_loss += loss.item()\n","\n","  # scale by the number of tokens in this dataloader\n","  total_loss[epoch] = epoch_loss / len(token_dataloader)\n","\n","  # update our progress :)\n","  if epoch%2==0:\n","    print(f'Finished epoch {epoch+1:2} with loss {epoch_loss / len(token_dataloader):.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.plot(total_loss,'ks-',markerfacecolor='w',markersize=8)\n","plt.gca().set(xlabel='Epoch',ylabel='Loss')\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# theoretical loss for untrained weights\n","-np.log(1/tokenizer.vocab_size)"],"metadata":{"id":"gSts30tc2YxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6tuVdtLL8IA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the model output for a training sequence\n","print(tokenizer.decode(X[6].tolist()))\n","print(tokenizer.decode(y[6].tolist()))"],"metadata":{"id":"i0RptutR8H4P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and generate new topic-related data\n","\n","# needs to be a tensor with one batch\n","startToks = torch.tensor(tokenizer.encode('I thought the Eloi would be smarter than')).unsqueeze(0)\n","\n","# text generation\n","Y = model.generate(startToks.to(device),n_new_tokens=80)\n","print(tokenizer.decode(Y[0].tolist()))"],"metadata":{"id":"RCr8wYOsy7l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j7aFuHJuIjUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# when the initial text doesn't repeat:\n","startToks,Y"],"metadata":{"id":"XoyTa-GUWh-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use this:\n","print(tokenizer.decode(Y[0].tolist()).replace('\\r','\\n'))"],"metadata":{"id":"dYrZxRMEHRV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# b/c of the carriage return (go to the beginning of the line, not the next line)\n","tokenizer.decode([201])"],"metadata":{"id":"FWk5jE4CHk23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"npZ9KFQBJPu7"},"execution_count":null,"outputs":[]}]}