{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPLpK268H7l2b6fM5Xg0ABM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Fine-tune codeGen for calculus<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"7aBuD6f6Atsa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6_UeqNSZhCH"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","tokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen-350M-mono')\n","\n","model = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-350M-mono').to(device)"],"metadata":{"id":"Chy4Qa9mZhqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this kind of prompt is for instruction-tuned models\n","text = 'Import re and define a regular expression that matches an email address.'\n","\n","# pretrained GPTs are better at context-informed token generation\n","text = 'for i in range(10):'\n","input_ids = tokenizer(text, return_tensors='pt').input_ids.to(device)\n","\n","generated_ids = model.generate(input_ids, max_length=128, temperature=1.4, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n","print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"],"metadata":{"id":"VIriQOWyaRmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aX9hiRx2Zhni"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import git-calc files and tokenize"],"metadata":{"id":"31vBDPouZhkl"}},{"cell_type":"code","source":["import os\n","import subprocess\n","import nbformat\n","\n","# download the repo\n","repo_url = 'https://github.com/mikexcohen/Calculus_book.git'\n","repo_dir = 'Calculus_book'\n","subprocess.run(['git','clone',repo_url,repo_dir])\n","\n","# initialize a list for all tokens\n","all_tokens = []\n","\n","\n","## find all .ipynb files\n","for root, dirs, files in os.walk(repo_dir):\n","  for file in files:\n","    if file.endswith('.ipynb'):\n","\n","      # load and process notebooks\n","      # print(f'Processing notebook: {os.path.join(root, file)}')\n","      with open(os.path.join(root, file),'r',encoding='utf-8') as f:\n","\n","        # parse the notebook structure\n","        nb_data = nbformat.read(f,as_version=4)\n","\n","        # gather all code cells into a single string\n","        code_cells = []\n","        for cell in nb_data['cells']:\n","          if cell['cell_type'] == 'code': # this cell contains code\n","            code_cells.append(cell['source']) # the actual code in this cell\n","\n","        # join code cells\n","        all_code = '\\n'.join(code_cells)\n","\n","        # tokenize and add to the list of all tokens\n","        all_tokens += tokenizer(all_code)['input_ids']\n","\n","# check the final dataset size\n","print(f'\\n\\nTraining data contains {len(all_tokens):,} tokens, of which {len(set(all_tokens)):,} are unique.')"],"metadata":{"id":"RrhJNrHqZhh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# need the tokens to be a tensor type\n","all_tokens = torch.tensor(all_tokens,dtype=torch.long)"],"metadata":{"id":"fyndV1WUZhfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1Whs7YYSZhce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune the model"],"metadata":{"id":"wkXTHacS2HyC"}},{"cell_type":"code","source":["# optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","\n","# training parameters\n","seq_len     = 128 # max sequence length\n","batch_size  =  64\n","num_samples = 200"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### training loop\n","\n","# initialize losses\n","losses = np.zeros(num_samples)\n","\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  ix =\n","  X  = all_tokens[]\n","\n","  # forward pass and get loss\n","\n","\n","  # backprop and store loss\n","\n","  # update progress display\n",""],"metadata":{"id":"nS-Xv_J_ZhaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Fy_0YP2KqyYz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluate the model"],"metadata":{"id":"JVsF-UuknVNV"}},{"cell_type":"code","source":["text = 'for i in range(10):'\n"],"metadata":{"id":"nma-aNxZqyV9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5m7JOVSXqySv"},"execution_count":null,"outputs":[]}]}