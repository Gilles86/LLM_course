{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOQDnSguaVs5aLylNSr4i3n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Quantify the Alice/Edgar fine-tuning<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"UZBWw8OLApjL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import requests\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"ng-VWPU_eXPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Tokenize and find the most frequent 100 tokens"],"metadata":{"id":"qkJR84ju20PE"}},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","tokenizer.pad_token_id = tokenizer.encode(' ')[0]\n","\n","# load in two GPTneo's and push to GPU\n","modelAlice = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","modelEdgar = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# -> GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","modelAlice = modelAlice.to(device)\n","modelEdgar = modelEdgar.to(device)"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nKDP6zFpPuzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import texts"],"metadata":{"id":"bCqF2dybPuwr"}},{"cell_type":"code","source":["# Alice Adventures in Wonderland\n","text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n","aliceTokens = tokenizer.encode(text,return_tensors='pt')[0]\n","\n","# Edgar Allen Poe\n","text = requests.get('https://www.gutenberg.org/cache/epub/2148/pg2148.txt').text\n","edgarTokens = tokenizer.encode(text,return_tensors='pt')[0]\n","\n","# summary\n","print(f'Alice in Wonderland has  {len(aliceTokens):7,} tokens.')\n","print(f'Edgar Allen Poe text has {len(edgarTokens):7,} tokens.')"],"metadata":{"id":"SS8ySA1SHPhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5YByOXZw20R_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Find the most frequent tokens"],"metadata":{"id":"nGGFI4haDMSO"}},{"cell_type":"code","source":["# create a filtered token vector initialized to zeros\n","aliceTokensFilt = np.full(len(aliceTokens),-1,dtype=int)\n","\n","# copy over the token only if it has >2 characters\n","for t in range(len(aliceTokens)):\n","  if len(tokenizer.decode(aliceTokens[t]))>2:\n","    aliceTokensFilt[t] = aliceTokens[t]\n","\n","\n","# repeat for edgar\n","edgarTokensFilt = np.full(len(edgarTokens),-1,dtype=int)\n","\n","# copy over the token only if it has >2 characters\n","for t in range(len(edgarTokens)):\n","  if len(tokenizer.decode(edgarTokens[t]))>2:\n","    edgarTokensFilt[t] = edgarTokens[t]"],"metadata":{"id":"c-eAC4D5LHDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# report\n","print('EDGAR:')\n","print(f'  {(edgarTokensFilt==-1).sum():,}/{len(edgarTokensFilt):,} ({100*(edgarTokensFilt==-1).sum()/len(edgarTokensFilt):.2f}%) tokens have <3 characters.')\n","\n","print('\\n\\nALICE:')\n","print(f'  {(aliceTokensFilt==-1).sum():,}/{len(aliceTokensFilt):,} ({100*(aliceTokensFilt==-1).sum()/len(aliceTokensFilt):.2f}%) tokens have <3 characters.')"],"metadata":{"id":"jfwB-oH3LHAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i8ttisGmLG-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for Alice\n","uniq,counts = np.unique(aliceTokensFilt,return_counts=True)\n","freqidx = np.argsort(counts)[::-1]\n","top100Alice = uniq[freqidx[1:101]]\n","\n","# for Edgar\n","uniq,counts = np.unique(edgarTokensFilt,return_counts=True)\n","freqidx = np.argsort(counts)[::-1]\n","top100Edgar = uniq[freqidx[1:101]]\n","\n","for t in top100Alice:\n","  print(f'Token {t:5} appears {np.sum(aliceTokensFilt==t):4} times and is \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"dxaG9S93Xed4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for t in top100Edgar:\n","  print(f'Token {t:5} appears {np.sum(edgarTokensFilt==t):4} times and is \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"H1wLToK1Xea3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YmP87KDzXeSu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Quantify common token usage pretraining"],"metadata":{"id":"frtsy6QmHPj1"}},{"cell_type":"code","source":["numreps =  10 # number of random repetitions\n","numtoks = 100 # output length\n","\n","# initialize\n","tokenUsageAlice = np.zeros((2,2)) # [ pre/post , Alice/Edgar ]\n","tokenUsageEdgar = np.zeros((2,2)) # [ pre/post , Alice/Edgar ]\n","\n","# random starting tokens\n","randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n","\n","\n","# ALICE: generate and store tokens\n","outAlice = modelAlice.generate(randstarts,\n","      min_length  = numtoks+1,\n","      max_length  = numtoks+1,\n","      do_sample   = True,\n","      pad_token_id = tokenizer.pad_token_id ).cpu()\n","genTokensAlice = outAlice[:,1:].reshape(-1)\n","\n","\n","# EDGAR: same as above but compressed :)\n","outEdgar = modelEdgar.generate(randstarts, min_length=numtoks+1, max_length=numtoks+1,\n","      do_sample=True, pad_token_id=tokenizer.pad_token_id ).cpu()\n","genTokensEdgar = outEdgar[:,1:].reshape(-1)\n","\n","# calculate the percentage\n","tokenUsageAlice[0,0] = np.mean(100*np.isin(genTokensAlice,top100Alice)) # ALICE model, ALICE tokens\n","tokenUsageAlice[0,1] = np.mean(100*np.isin(genTokensEdgar,top100Alice)) # EDGAR model, ALICE tokens\n","\n","tokenUsageEdgar[0,0] = np.mean(100*np.isin(genTokensAlice,top100Edgar)) # ALICE model, EDGAR tokens\n","tokenUsageEdgar[0,1] = np.mean(100*np.isin(genTokensEdgar,top100Edgar)) # EDGAR model, EDGAR tokens\n","\n","tokenUsageEdgar # have a look"],"metadata":{"id":"9U2YftTpZ-fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r_v2gN2_Ch-w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Fine-tune the models"],"metadata":{"id":"syOer_DkHPmY"}},{"cell_type":"code","source":["# ALICE optimizer\n","optimizerAlice = torch.optim.AdamW(modelAlice.parameters(), lr=5e-5)\n","\n","# EDGAR optimizer\n","optimizerEdgar = torch.optim.AdamW(modelEdgar.parameters(), lr=5e-5)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training parameters\n","seq_len     = 256 # max sequence length\n","batch_size  =  32\n","num_samples = 476"],"metadata":{"id":"CwOb8RCqT92V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize losses\n","lossAlice = np.zeros(num_samples)\n","lossEdgar = np.zeros(num_samples)\n","\n","for sampli in range(num_samples):\n","\n","\n","  ### --- ALICE fine-tuning\n","  # get a batch of data\n","  ix = torch.randint(len(aliceTokens)-seq_len,size=(batch_size,))\n","  X  = aliceTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # forward pass and get loss\n","  modelAlice.zero_grad()\n","  outputs = modelAlice(X,labels=X)\n","\n","  # backprop and store loss\n","  outputs.loss.backward()\n","  optimizerAlice.step()\n","  lossAlice[sampli] = outputs.loss.item()\n","  ### ---------------------\n","\n","\n","  ### --- EDGAR fine-tuning\n","  # get a batch of data\n","  ix = torch.randint(len(edgarTokens)-seq_len,size=(batch_size,))\n","  X  = edgarTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # forward pass and get loss\n","  modelEdgar.zero_grad()\n","  outputs = modelEdgar(X,labels=X)\n","\n","  # backprop and store loss\n","  outputs.loss.backward()\n","  optimizerEdgar.step()\n","  lossEdgar[sampli] = outputs.loss.item()\n","  ### ---------------------\n","\n","  # update progress display\n","  if sampli%77==0:\n","    print(f'Sample {sampli:4}/{num_samples}, losses (Alice/Edgar): {lossAlice[sampli]:.2f}/{lossEdgar[sampli]:.2f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oMKA4BKnky1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.figure(figsize=(10,3))\n","plt.plot(lossAlice,'k',markersize=8,label='ALICE loss')\n","plt.plot(lossEdgar,'b',markersize=8,label='EDGAR loss')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Data sample',ylabel='Loss',xlim=[0,num_samples])\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1wZg_eplZBDV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Evaluation the fine-tuning"],"metadata":{"id":"4ghbBiEBD5Gy"}},{"cell_type":"code","source":["# random starting tokens\n","randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n","\n","\n","# ALICE: generate and store tokens\n","outAlice = modelAlice.generate(randstarts,\n","      min_length  = numtoks+1,\n","      max_length  = numtoks+1,\n","      do_sample   = True,\n","      pad_token_id = tokenizer.pad_token_id ).cpu()\n","genTokensAlice = outAlice[:,1:].reshape(-1)\n","\n","\n","# EDGAR: same as above :)\n","outEdgar = modelEdgar.generate(randstarts, min_length=numtoks+1, max_length=numtoks+1,\n","      do_sample=True, pad_token_id=tokenizer.pad_token_id ).cpu()\n","genTokensEdgar = outEdgar[:,1:].reshape(-1)\n","\n","\n","# calculate the percentage\n","tokenUsageAlice[1,0] = np.mean(100*np.isin(genTokensAlice,top100Alice)) # ALICE model, ALICE tokens\n","tokenUsageAlice[1,1] = np.mean(100*np.isin(genTokensEdgar,top100Alice)) # EDGAR model, ALICE tokens\n","\n","tokenUsageEdgar[1,0] = np.mean(100*np.isin(genTokensAlice,top100Edgar)) # ALICE model, EDGAR tokens\n","tokenUsageEdgar[1,1] = np.mean(100*np.isin(genTokensEdgar,top100Edgar)) # EDGAR model, EDGAR tokens"],"metadata":{"id":"ibJxB9VgHPrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize the results!\n","\n","_,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","\n","# show the pre-train token usage\n","axs[0].bar([.8,1.8],tokenUsageAlice[0,:],width=.4,label='ALICE tokens')\n","axs[0].bar([1.2,2.2],tokenUsageEdgar[0,:],width=.4,label='EDGAR tokens')\n","minmaxY = np.sort(np.concatenate((tokenUsageAlice[0,:],tokenUsageEdgar[0,:])))[[0,-1]]\n","axs[0].set(ylim=[minmaxY[0]-2,minmaxY[1]+2],xticks=[1,2],xticklabels=['ALICE model','EDGAR model'],\n","           ylabel='Percent generated (%)',title='BEFORE training')\n","axs[0].legend()\n","\n","# show the post-train token usage\n","axs[1].bar([.8,1.8],tokenUsageAlice[1,:],width=.4,label='ALICE tokens')\n","axs[1].bar([1.2,2.2],tokenUsageEdgar[1,:],width=.4,label='EDGAR tokens')\n","minmaxY = np.sort(np.concatenate((tokenUsageAlice[1,:],tokenUsageEdgar[1,:])))[[0,-1]]\n","axs[1].set(ylim=[minmaxY[0]-2,minmaxY[1]+2],xticks=[1,2],xticklabels=['ALICE model','EDGAR model'],\n","           ylabel='Percent generated (%)',title='AFTER training')\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"rS7XsMjhHPw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"q971n8R3EJqJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Qualitative assessment"],"metadata":{"id":"P0MJbceLEJmY"}},{"cell_type":"code","source":["# input\n","x = tokenizer.encode('What did the Red Queen say to Alice?', return_tensors='pt').to(device)\n","\n","# get the output\n","outAlice = modelAlice.generate(x,max_new_tokens=120,do_sample=True,pad_token_id=50256)\n","outEdgar = modelEdgar.generate(x,max_new_tokens=120,do_sample=True,pad_token_id=50256)\n","\n","# print both models' outputs\n","print('** Alice model says:')\n","print(tokenizer.decode(outAlice[0].cpu()))\n","\n","print('\\n\\n** Edgar model says:')\n","print(tokenizer.decode(outEdgar[0].cpu()))"],"metadata":{"id":"4ql-aaqpZBAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P9YC0HXyHPzb"},"execution_count":null,"outputs":[]}]}