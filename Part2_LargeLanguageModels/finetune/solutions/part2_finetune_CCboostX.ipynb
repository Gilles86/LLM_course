{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1BrHjTCfZM4KWS9AjjmjZeVRIQ4vVnmn3","timestamp":1748858732323},{"file_id":"1jmWytQcLQdHCqDzcEt7q1GICCdPBxZ3T","timestamp":1742377210164}],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMLrBGp2xQMSimhoutfZDJK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Maximize the \"X\" factor<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"aBqRYTG8CYPH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAnIB2kIp_A"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# pytorch stuff\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","\n","# for text printing\n","import textwrap"]},{"cell_type":"code","source":[],"metadata":{"id":"cHj5D0oSDMoF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import and inspect GPT2-medium"],"metadata":{"id":"bZQyNDLebsRK"}},{"cell_type":"code","source":["# use the GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# load pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2-medium').to(device)\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token # set pad token"],"metadata":{"id":"B3ap39h0Jjdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt2"],"metadata":{"id":"5uxH7fcuxUj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt2.config"],"metadata":{"id":"m6-Sh094H5_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'There are {len(gpt2.transformer.h)} transformer blocks.')"],"metadata":{"id":"JbwMR8JNHeME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E9FhirCXzr-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Create and test the loss function"],"metadata":{"id":"Js9ri6zbIHzI"}},{"cell_type":"code","source":["class myLoss_x(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # mask: 1 if token contains a target, 0 otherwise\n","    self.mask = torch.zeros(tokenizer.vocab_size, device=device)\n","    for t in range(tokenizer.vocab_size):\n","      thistoken = tokenizer.decode([t])\n","      if 'x' in thistoken:\n","        self.mask[t] = 1\n","\n","    # normalize to pdist\n","    self.mask = self.mask/torch.sum(self.mask)\n","\n","  def forward(self, log_probs):\n","    # assumes log-softmax-prob input!\n","    return F.kl_div(log_probs, self.mask, reduction='batchmean')"],"metadata":{"id":"33E370453tEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a loss function instance\n","loss_function = myLoss_x().to(device)"],"metadata":{"id":"9O89dfXGItRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 4\n","seq_len = 64\n","\n","# generate data and move data to GPU\n","X = torch.randint(0,tokenizer.vocab_size,(batch_size,seq_len)).to(device)\n","\n","# forward pass (disable gradient-associated calculations)\n","with torch.no_grad():\n","  out = gpt2(X)\n","\n","print(f'Model input has size: {X.shape}')\n","# print(f'Model output has size: {out.shape}')\n","print(f'Model output has size: {out[0].shape}')"],"metadata":{"id":"JpPGxgsJsn58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# is this a probability distribution, a log-probdist, or neither?\n","print(f'Sum of outputs for one token: {out[0][0,0,:].sum()}')\n","print(f'Sum of exp(outputs) for one token: {torch.exp(out[0][0,0,:]).sum()}')"],"metadata":{"id":"F4z7xeOUJ44A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reshape the output and transform to log-softmax\n","logprobs = F.log_softmax(out[0],dim=-1)\n","logprobs_reshape = logprobs.view(-1,tokenizer.vocab_size)\n","\n","print('Shape of logprob(logits):   ',logprobs.shape)\n","print('Shape of reshaped logprobs: ',logprobs_reshape.shape)\n","print('Shape of loss function mask:',loss_function.mask.shape)"],"metadata":{"id":"qtUS50gEJT43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate KL losses\n","loss_function(logprobs_reshape)"],"metadata":{"id":"kdB08oquIzyq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GnDpDV4-L_X3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Train the model"],"metadata":{"id":"FkFYgxTXpnEx"}},{"cell_type":"code","source":["# pre-fine-tuning evals\n","X = tokenizer.encode('Why did the chicken cross the road?',return_tensors='pt').to(device)\n","Y = gpt2.generate(X,do_sample=True,max_length=200)\n","print(textwrap.fill(tokenizer.decode(Y[0].tolist()), width=100))\n","\n","\n","# how many generated tokens contain a target letter?\n","hasTarget = 0\n","for t in Y[0][len(X[0]):]:\n","  if 'x' in tokenizer.decode(t):\n","    hasTarget += 1\n","\n","print('\\n\\n')\n","print(f'{hasTarget} of {len(Y[0][len(X[0]):])} tokens have a target.')"],"metadata":{"id":"kKXySTroQlWS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Mb3SeX5QeLuh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the optimizer function (lower learning than before!)\n","optimizer = torch.optim.AdamW(gpt2.parameters(), lr=1e-6, weight_decay=.01)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 300\n","\n","# initialize losses\n","total_loss = np.zeros(num_epochs)\n","\n","\n","for epoch in range(num_epochs):\n","\n","  # generate data and move data to GPU\n","  X = torch.randint(0,tokenizer.vocab_size,(batch_size,seq_len)).to(device)\n","\n","  # forward pass\n","  optimizer.zero_grad()\n","  logits = gpt2(X)[0]\n","\n","  # calculate the losses\n","  logits_reshape = logits.view(-1,tokenizer.vocab_size)\n","  logprobs_reshape = F.log_softmax(logits_reshape,dim=-1)\n","  loss = loss_function(logprobs_reshape)\n","\n","  # backprop\n","  loss.backward()\n","  optimizer.step()\n","\n","  # get the loss\n","  total_loss[epoch] = loss.item()\n","\n","  # update our progress :)\n","  if epoch%37==0:\n","    print(f'Finished epoch {epoch:4} with loss {total_loss[epoch]:.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.figure(figsize=(8,3))\n","plt.plot(total_loss,'k')\n","plt.gca().set(xlabel='Epoch',ylabel='Loss')\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6tuVdtLL8IA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pre-fine-tuning evals\n","X = tokenizer.encode('Why did the chicken cross the road?',return_tensors='pt').to(device)\n","Y = gpt2.generate(X,do_sample=True,max_length=200)\n","print(textwrap.fill(tokenizer.decode(Y[0].tolist()), width=100))\n","\n","\n","# how many generated tokens contain a target letter?\n","hasTarget = 0\n","for t in Y[0][len(X[0]):]:\n","  if 'x' in tokenizer.decode(t):\n","    hasTarget += 1\n","\n","print('\\n\\n')\n","print(f'{hasTarget} of {len(Y[0][len(X[0]):])} tokens have a target.')"],"metadata":{"id":"RCr8wYOsy7l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eT3di2KynAWQ"},"execution_count":null,"outputs":[]}]}