{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOz2wnff9CBZlvWC8zpZERi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Impact of freezing neo on fine-tuning<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"6nQjZWUP_hiM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","import time\n","import requests\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"ng-VWPU_eXPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","tokenizer.pad_token_id = tokenizer.encode(' ')[0]\n","\n","# -> GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# load in two GPTneo's and push to GPU\n","modelFreeze = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m').to(device)\n","modelTrain  = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m').to(device)"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JQ1jAf314fxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Find the most frequent 100 tokens"],"metadata":{"id":"qkJR84ju20PE"}},{"cell_type":"code","source":["# Moby Dick\n","text = requests.get('https://www.gutenberg.org/cache/epub/2701/pg2701.txt').text\n","tokens = tokenizer.encode(text,return_tensors='pt')[0]\n","\n","# summary\n","print(f'Moby Dick has {len(tokens):,} tokens, of which {len(torch.unique(tokens)):,} are unique.')"],"metadata":{"id":"SS8ySA1SHPhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["uniq,counts = np.unique(tokens,return_counts=True)\n","freqidx = np.argsort(counts)[::-1]\n","top100 = uniq[freqidx[:100]]\n","\n","for t in top100:\n","  print(f'Token {t:5} appears {torch.sum(tokens==t):4} times and is \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"dxaG9S93Xed4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WUVeZyhvA7M5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numreps =  10 # number of random repetitions\n","numtoks = 100 # output length\n","\n","# initialize\n","tokenUsage = np.zeros((2,2)) # [ pre/post , Freeze/Train ]\n","\n","# random starting tokens\n","randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n","\n","\n","# FREEZE: generate and store tokens\n","outFreeze = modelFreeze.generate(randstarts,\n","      min_length  = numtoks+1,\n","      max_length  = numtoks+1,\n","      do_sample   = True, pad_token_id = tokenizer.pad_token_id ).cpu()\n","genTokensFreeze = outFreeze[:,1:].reshape(-1)\n","\n","\n","# TRAIN: same as above :)\n","outTrain = modelTrain.generate(randstarts, min_length=numtoks+1, max_length=numtoks+1,\n","      do_sample=True, pad_token_id=tokenizer.pad_token_id ).cpu()\n","genTokensTrain = outTrain[:,1:].reshape(-1)\n","\n","\n","# calculate the percentage\n","tokenUsage[0,0] = np.mean(100*np.isin(genTokensFreeze,top100))\n","tokenUsage[0,1] = np.mean(100*np.isin(genTokensTrain ,top100))"],"metadata":{"id":"yIr2iS2sA7Jy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenUsage"],"metadata":{"id":"ZRjHqpxAA7G3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"J9TdsZHtA7EB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Targeted training"],"metadata":{"id":"W36VFhVsVc_Z"}},{"cell_type":"code","source":["for name,param in modelFreeze.named_parameters():\n","  splitstr = name.split('.')\n","  print(splitstr)"],"metadata":{"id":"xxPwuHc7z6Nc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TEST: identify QVK weights in layers >5\n","for name,param in modelFreeze.named_parameters():\n","  splitstr = name.split('.')\n","  if (len(splitstr)>5) and (splitstr[3]=='attn'):\n","    if (int(splitstr[2])>5) and (splitstr[5][0] in 'qvk'):\n","      print(name)"],"metadata":{"id":"2hM9ArOazuw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for name,param in modelFreeze.named_parameters():\n","\n","  # split the name by .\n","  splitstr = name.split('.')\n","\n","  # see if this fits our filter\n","  if (len(splitstr)>5) and (splitstr[3]=='attn'):\n","    if (int(splitstr[2])>5) and (splitstr[5][0] in 'qvk'):\n","      param.requires_grad = True\n","      print(f'+++ Layer {name} is trainable (.requires_grad = {param.requires_grad}).')\n","\n","  # otherwise, freeze the layer\n","  else:\n","    param.requires_grad = False\n","    print(f'--- Layer {name} is frozen (.requires_grad = {param.requires_grad}).')"],"metadata":{"id":"8AxhjmrvVg5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aQXP2zWkB1j2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Fine-tune the models"],"metadata":{"id":"wkXTHacS2HyC"}},{"cell_type":"code","source":["# FREEZE optimizer\n","optimizerFreeze = torch.optim.AdamW(modelFreeze.parameters(), lr=.0005)\n","\n","# TRAIN optimizer\n","optimizerTrain = torch.optim.AdamW(modelTrain.parameters(), lr=.0005)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training parameters\n","seq_len     = 256 # max sequence length\n","batch_size  =  16\n","num_samples = 474"],"metadata":{"id":"CwOb8RCqT92V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize losses\n","losses = np.zeros((num_samples,2))\n","delta_norm_em = np.zeros((num_samples,2))\n","\n","# and computation times\n","timeTrain = 0\n","timeFreeze = 0\n","\n","\n","# grab the initial MLP weights for comparison\n","prev_emFreeze = modelFreeze.transformer.h[6].attn.attention.k_proj.weight.detach() + 0\n","prev_emTrain = modelTrain.transformer.h[6].attn.attention.k_proj.weight.detach() + 0\n","\n","\n","\n","# and run the training!\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  ix = torch.randint(len(tokens)-seq_len,size=(batch_size,))\n","  X  = tokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","\n","  ### --- FREEZE fine-tuning\n","  # forward pass and get loss\n","  start_time = time.time() # start the timer\n","  modelFreeze.zero_grad()\n","  outputs = modelFreeze(X,labels=X)\n","\n","  # backprop and store loss\n","  outputs.loss.backward()\n","  optimizerFreeze.step()\n","  losses[sampli,0] = outputs.loss.item()\n","  timeFreeze += time.time()-start_time # end the timer and add\n","  ### ---------------------\n","\n","\n","  ### --- TRAIN fine-tuning\n","  # forward pass and get loss\n","  start_time = time.time() # start the timer\n","  modelTrain.zero_grad()\n","  outputs = modelTrain(X,labels=X)\n","\n","  # backprop and store loss\n","  outputs.loss.backward()\n","  optimizerTrain.step()\n","  losses[sampli,1] = outputs.loss.item()\n","  timeTrain += time.time()-start_time # end the timer and add\n","  ### ---------------------\n","\n","\n","\n","  ### --- matrix norm to assess change in MLP layer\n","  delta_norm_em[sampli,0] = torch.norm(modelFreeze.transformer.h[6].attn.attention.k_proj.weight.detach() - prev_emFreeze)\n","  prev_emFreeze = modelFreeze.transformer.h[6].attn.attention.k_proj.weight.detach() + 0\n","\n","  delta_norm_em[sampli,1] = torch.norm(modelTrain.transformer.h[6].attn.attention.k_proj.weight.detach() - prev_emTrain)\n","  prev_emTrain = modelTrain.transformer.h[6].attn.attention.k_proj.weight.detach() + 0\n","\n","\n","\n","\n","  # update progress display\n","  if sampli%37==0:\n","    print(f'Sample {sampli:4}/{num_samples}, losses (Freeze/Train): {losses[sampli,0]:.2f}/{losses[sampli,1]:.2f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Zxmfn70YB1eI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Visualize the results"],"metadata":{"id":"WCeG4Md65HId"}},{"cell_type":"code","source":["# plot the losses\n","_,axs = plt.subplots(1,2,figsize=(10,4))\n","axs[0].plot(losses[:,0],'k',markersize=8,label='FREEZE loss')\n","axs[0].plot(losses[:,1],'b',markersize=8,label='TRAIN loss')\n","axs[0].legend()\n","axs[0].set(ylim=[0,5],xlabel='Data sample',ylabel='Loss',xlim=[0,num_samples],title='Losses over training')\n","\n","axs[1].plot(losses[:,0],losses[:,1],'ko',markerfacecolor=[.9,.7,.7],alpha=.4,label='Data')\n","xylim = [np.min(losses)-.1,np.max(losses)+.1]\n","axs[1].plot(xylim,xylim,'k--',zorder=-3,label='Unity')\n","axs[1].set(xlabel='FREEZE model',ylabel='TRAIN model',title='Losses',xlim=xylim,ylim=xylim)\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# random starting tokens\n","randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n","\n","\n","# FREEZE: generate and store tokens\n","outFreeze = modelFreeze.generate(randstarts,\n","      min_length  = numtoks+1,\n","      max_length  = numtoks+1,\n","      do_sample   = True, pad_token_id = tokenizer.pad_token_id ).cpu()\n","genTokensFreeze = outFreeze[:,1:].reshape(-1)\n","\n","\n","# TRAIN: same as above :)\n","outTrain = modelTrain.generate(randstarts, min_length=numtoks+1, max_length=numtoks+1,\n","      do_sample=True, pad_token_id=tokenizer.pad_token_id ).cpu()\n","genTokensTrain = outTrain[:,1:].reshape(-1)\n","\n","\n","# calculate the percentage\n","tokenUsage[1,0] = np.mean(100*np.isin(genTokensFreeze,top100))\n","tokenUsage[1,1] = np.mean(100*np.isin(genTokensTrain ,top100))"],"metadata":{"id":"mJIX3fdAB1bg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode(genTokensTrain))"],"metadata":{"id":"UMnLmZw7Q5Xc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4yHFImklQ5US"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize the results!\n","\n","_,axs = plt.subplots(1,2,figsize=(10,3))\n","\n","# show the pre-train token usage\n","axs[0].bar([.8,1.8],tokenUsage[0,:],width=.4,label='BEFORE')\n","axs[0].bar([1.2,2.2],tokenUsage[1,:],width=.4,label='AFTER')\n","axs[0].set(ylim=[min(tokenUsage.flatten())-2,max(tokenUsage.flatten())+2],xticks=[1,2],xlim=[.3,2.6],\n","           xticklabels=['FREEZE model','TRAIN model'],ylabel='Percent generated (%)',title='Common Moby Dick tokens generated')\n","axs[0].legend()\n","\n","axs[1].bar([1,2],np.diff(tokenUsage,axis=0)[0])\n","axs[1].set(xticks=[1,2],xlim=[.3,2.6],xticklabels=['FREEZE model','TRAIN model'],\n","           ylabel='Change in generated tokens (%)',title='Post- minus pre-training')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"G6yCCy3OELTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pfiB2AVGELPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how did the embeddings weights change?\n","plt.figure(figsize=(8,3))\n","\n","plt.plot(delta_norm_em[:,0],linewidth=2,label='FREEZE')\n","plt.plot(delta_norm_em[:,1],linewidth=2,label='TRAIN')\n","\n","plt.legend()\n","plt.gca().set(xlim=[0,num_samples],xlabel='Training sample',ylabel='Matrix difference norm')\n","plt.show()"],"metadata":{"id":"j7ztvZzwELJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sgtdgDaWKiIg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computation time\n","plt.bar([1,2],[timeFreeze,timeTrain])\n","plt.gca().set(xticks=[1,2],xticklabels=['FREEZE','TRAIN'],ylabel='Computation time (s)',\n","              ylim=[min(timeFreeze,timeTrain)*.8,max(timeFreeze,timeTrain)*1.2],\n","              title=f'Computation time across {num_samples} training samples')\n","plt.show()"],"metadata":{"id":"ta3y7rchpbys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oNjaoy6qpZ_P"},"execution_count":null,"outputs":[]}]}