{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNmAJqo+BIScHS4jzyHbyuY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Clip, freeze, and schedule BERT<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"qyGsqU_wT0qV"}},{"cell_type":"code","source":["# run this code, then restart the python session (and then comment it out)\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"wyNnAMu7Sdn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# typical python libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","# pytorch libraries\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","# huggingface libraries\n","from transformers import BertModel, BertTokenizer\n","from transformers import get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\n","from datasets import load_dataset, DatasetDict"],"metadata":{"id":"-mhNmp9CT17A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"zGSG7ILmT13L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DglXfXWcb36U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: The data and the model"],"metadata":{"id":"KSEbXYTweK5M"}},{"cell_type":"code","source":["# load the IMDB dataset (from HF)\n","dataset = load_dataset('imdb')\n","\n","# reduce the size (overwriting the variable!)\n","dataset = DatasetDict({split:dataset[split].select(range(5_000,20_000)) for split in ['train','test']})"],"metadata":{"id":"wLWXY0dnb4AA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","plt.plot(dataset['train']['label'] + np.random.randn(len(dataset['train']))/20,'m.',markersize=1,alpha=.2)\n","\n","plt.gca().set(xlabel='Review index',ylabel='Label',yticks=[0,1],yticklabels=['Negative','Positive'],\n","              xlim=[0,len(dataset['train'])],ylim=[-.5,1.5])\n","plt.show()"],"metadata":{"id":"vA77mq5YeVhR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define a tokenization function that processes each data sample\n","def tokenize_function(one_sample):\n","  return tokenizer(\n","    one_sample['text'],\n","    max_length = 512,         # max sequence length\n","    padding    = 'max_length',# pad to max length\n","    truncation = True)        # truncate if longer than max_length\n","\n","\n","# apply the tokenization function to the dataset (batched for efficiency)\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","\n","# remove text pair\n","tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n","\n","# change format to pytorch tensors\n","tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n","\n","# create DataLoaders for training and testing\n","train_dataloader = DataLoader(tokenized_dataset['train'], shuffle=True, batch_size=64)\n","test_dataloader  = DataLoader(tokenized_dataset['test'], batch_size=64)"],"metadata":{"id":"kJh6cuaYePG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(train_dataloader))"],"metadata":{"id":"wFui0MzSErj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rGN-O0kbTyT_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create an LLM model using pretrained BERT with a new head"],"metadata":{"id":"mQ0JGbZ3T1pm"}},{"cell_type":"code","source":["class BertForBinaryClassification(nn.Module):\n","  def __init__(self, num_labels=2):\n","    super(BertForBinaryClassification, self).__init__()\n","\n","    # load the pre-trained BERT model\n","    self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","    # classification head that converts the 768-d pooled output into 2 final outputs\n","    self.classifier = nn.Linear(768,2)\n","    self.dropout = nn.Dropout(self.bert.embeddings.dropout.p) # 10%\n","\n","    # initialize the weights and biases\n","    nn.init.xavier_uniform_(self.classifier.weight)\n","    nn.init.zeros_(self.classifier.bias)\n","\n","\n","  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","\n","    # forward pass through the downloaded (pretrained) BERT\n","    outputs = self.bert(\n","      input_ids      = input_ids,\n","      attention_mask = attention_mask,\n","      token_type_ids = token_type_ids)\n","\n","    # extract the pooled output and apply dropout\n","    pooled_output = self.dropout( outputs.pooler_output )\n","\n","    # final push through the classification layer.\n","    logits = self.classifier(pooled_output)\n","    return logits"],"metadata":{"id":"05j-M4MRb4Iy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance of the model and test it:\n","model = BertForBinaryClassification().to(device)"],"metadata":{"id":"SLb0HwWLciY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.bert"],"metadata":{"id":"eyQcm-eJajWs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"axWMEPr-UFz4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Freeze the attention weights"],"metadata":{"id":"SWTS7RozUFxa"}},{"cell_type":"code","source":["# parameter counts\n","trainParamsCount = 0\n","frozenParamsCount = 0\n","\n","for name,param in model.named_parameters():\n","  if ('attention' in name) or ('embeddings' in name):\n","    param.requires_grad = False\n","    frozenParamsCount += torch.numel(param)\n","    print(f'--- Layer {name} is frozen (.requires_grad = {param.requires_grad}).')\n","\n","  else:\n","    param.requires_grad = True # just in case :P\n","    trainParamsCount += torch.numel(param)\n","    print(f'+++ Layer {name} is trainable (.requires_grad = {param.requires_grad}).')\n","\n","print(f'\\n\\nThere are {frozenParamsCount:,} ({frozenParamsCount*100/(frozenParamsCount+trainParamsCount):.2f}%) frozen weights,')\n","print(f'      and {trainParamsCount:,} ({trainParamsCount*100/(frozenParamsCount+trainParamsCount):.2f}%) trainable weights.')"],"metadata":{"id":"VRcn-ubxOZPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RRwIIXP8T8sN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Setup a learning rate scheduler"],"metadata":{"id":"Ftzlsxq_b33g"}},{"cell_type":"code","source":["# training steps\n","num_samples = 300\n","\n","# create an optimizer with a \"model\"\n","optimizer = torch.optim.AdamW(nn.Linear(10,10).parameters(),lr=3e-5)\n","\n","# learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = int(num_samples*.05),  # gentle 5% warmup\n","    num_training_steps = int(num_samples*1.5) # steps set to 150% so the lr stays >0\n",")\n","\n","# quick test to see the learning rates\n","lrs = np.zeros(num_samples)\n","for i in range(num_samples):\n","  optimizer.step() # update the optimizer\n","  scheduler.step() # run the scheduler\n","  lrs[i] = scheduler.get_last_lr()[0] # get the actual learning rate\n","\n","# plot!\n","plt.figure(figsize=(10,3))\n","plt.plot(lrs,'ko',markersize=5,markerfacecolor=[.7,.7,.9],alpha=.3)\n","\n","plt.gca().set(xlabel='Training epoch',ylabel='Learning rate')\n","plt.show()"],"metadata":{"id":"zeATs9lCVGQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now for the real optimizer and loss function\n","optimizer = torch.optim.AdamW(model.parameters(),lr=1e-5)\n","loss_fun = nn.CrossEntropyLoss() # (cross-entropy loss for multi-class classification)\n","\n","# IMPORTANT: redefine the scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = int(num_samples*.05),  # gentle 5% warmup\n","    num_training_steps = int(num_samples*1.5) # steps set to 150% so the lr stays >0\n",")"],"metadata":{"id":"LrR83zjvoy-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WzQDZ1pPVGOi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Fine-tune with adjustments"],"metadata":{"id":"ZnZHBtlrVGMI"}},{"cell_type":"code","source":["# initialize performance metrices\n","train_losses = np.zeros(num_samples)\n","train_accuracy = np.zeros(num_samples)\n","test_losses = np.zeros(num_samples)\n","test_accuracy = np.zeros(num_samples)\n","norms = np.zeros((num_samples,2))\n","\n","\n","## loop over data samples\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  batch = next(iter(train_dataloader))\n","\n","  # and move it to the GPU\n","  tokenz  = batch['input_ids'].to(device)\n","  att_msk = batch['attention_mask'].to(device)\n","  labels  = batch['label'].to(device)\n","\n","  # clear the previous gradients\n","  optimizer.zero_grad()\n","\n","  # forward pass and get model predictions\n","  logits = model(tokenz, attention_mask=att_msk)\n","  predLabels = torch.argmax(logits, dim=1)\n","\n","  # calculate and store loss + average accuracy\n","  loss = loss_fun(logits, labels)\n","  train_losses[sampli] = loss.item()\n","  train_accuracy[sampli] = (predLabels == labels).sum().item()/train_dataloader.batch_size\n","\n","  # backward pass\n","  loss.backward()\n","\n","  # get two gradient norms\n","  norms[sampli,0] = torch.norm(model.bert.encoder.layer[7].output.dense.weight.grad).item()\n","  norms[sampli,1] = torch.norm(model.classifier.weight.grad).item()\n","\n","  # gradient clipping to prevent exploding gradients\n","  nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)\n","\n","  # update the weights and the learning rate\n","  optimizer.step()\n","  scheduler.step()\n","\n","  # test the model and report losses every k samples\n","  if sampli%10 == 0:\n","\n","    # evaluation using the test set\n","    model.eval()\n","    with torch.no_grad():\n","\n","      # get a batch of data and move it to the GPU\n","      batch   = next(iter(test_dataloader))\n","      tokenz  = batch['input_ids'].to(device)\n","      att_msk = batch['attention_mask'].to(device)\n","      labels  = batch['label'].to(device)\n","\n","      # forward pass and get model predictions\n","      logits = model(tokenz, attention_mask=att_msk)\n","      predLabels = torch.argmax(logits, dim=1)\n","\n","      # calculate and store loss + accuracy\n","      loss = loss_fun(logits, labels)\n","      test_losses[sampli] = loss.item()\n","      test_accuracy[sampli] = (predLabels == labels).sum().item()/train_dataloader.batch_size\n","\n","      # report the results\n","      print(f'Sample {sampli:4}/{num_samples}, losses (train/test): {train_losses[sampli]:.2f}/{test_losses[sampli]:.2f}, accuracy: {train_accuracy[sampli]:.2f}/{test_accuracy[sampli]:.2f}')\n","\n","      # put the model back into train mode\n","      model.train()"],"metadata":{"id":"v5m3eWnynMeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","# plot the losses\n","axs[0].plot(train_losses,'ko',markerfacecolor=[.9,.7,.7],alpha=.5,label='Train')\n","axs[0].plot(range(0,num_samples,10),test_losses[::10],'ks-',\n","            markerfacecolor=[.7,.9,.7],markersize=8,alpha=.8,linewidth=3,label='Test')\n","axs[0].set(xlabel='Training sample',ylabel='Loss',title='Cross-entropy loss',xlim=[0,num_samples])\n","axs[0].legend()\n","\n","# plot the prediction accuracy\n","axs[1].plot(100*train_accuracy,'ko',markerfacecolor=[.7,.7,.9],alpha=.5,label='Train')\n","axs[1].plot(range(0,num_samples,10),100*test_accuracy[::10],'ks-',\n","            markerfacecolor=[.7,.9,.7],markersize=8,alpha=.8,linewidth=3,label='Test')\n","axs[1].set(xlabel='Training sample',ylabel='Percent correct (%)',title='Prediction accuracy',xlim=[0,num_samples])\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"dKn24FCtnMbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B9VzgEzjnMYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3.6))\n","\n","# plot the losses\n","axs[0].plot(norms[:,0],'ko',markerfacecolor=[.9,.7,.7],alpha=.7)\n","axs[0].set(xlabel='Training sample',ylabel='Norm',title='MLP: Pre-clip gradient norm',xlim=[0,num_samples])\n","\n","axs[1].plot(norms[:,1],'ks',markerfacecolor=[.7,.9,.7],alpha=.7)\n","axs[1].set(xlabel='Training sample',ylabel='Norm',title='Classifier: Pre-clip gradient norm',xlim=[0,num_samples])\n","\n","for a in axs: a.axhline(1,linestyle='--',color='k')\n","\n","plt.show()"],"metadata":{"id":"BJ15c00KZieS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1ViIXBoOZiY0"},"execution_count":null,"outputs":[]}]}