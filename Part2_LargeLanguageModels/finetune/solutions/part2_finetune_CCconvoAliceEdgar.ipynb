{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMZr2xXi/yRiEj2vOqgRAvW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: A chat between Alice and Edgar<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"6-t6Rf39AuPZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import requests"]},{"cell_type":"code","source":[],"metadata":{"id":"ng-VWPU_eXPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Copy and condense from the previous codechallenge"],"metadata":{"id":"YuvBoUevUxSr"}},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","tokenizer.pad_token_id = tokenizer.encode(' ')[0]\n","\n","# load in two GPTneo's and push to GPU\n","modelAlice = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","modelEdgar = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# -> GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","modelAlice = modelAlice.to(device)\n","modelEdgar = modelEdgar.to(device)"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qH0so0epSGWu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Alice Adventures in Wonderland\n","text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n","aliceTokens = tokenizer.encode(text,return_tensors='pt')[0]\n","\n","# Edgar Allen Poe\n","text = requests.get('https://www.gutenberg.org/cache/epub/2148/pg2148.txt').text\n","edgarTokens = tokenizer.encode(text,return_tensors='pt')[0]"],"metadata":{"id":"SS8ySA1SHPhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-JhncV9lT9tS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune the model"],"metadata":{"id":"wkXTHacS2HyC"}},{"cell_type":"code","source":["# ALICE optimizer\n","optimizerAlice = torch.optim.AdamW(modelAlice.parameters(), lr=5e-5, weight_decay=.01)\n","\n","# EDGAR optimizer\n","optimizerEdgar = torch.optim.AdamW(modelEdgar.parameters(), lr=5e-5, weight_decay=.01)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training parameters\n","seq_len    = 256 # max sequence length\n","batch_size =  16\n","num_samples = 476"],"metadata":{"id":"CwOb8RCqT92V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize losses\n","lossAlice = np.zeros(num_samples)\n","lossEdgar = np.zeros(num_samples)\n","\n","for sampli in range(num_samples):\n","\n","\n","  ### --- ALICE fine-tuning\n","  # get a batch of data\n","  ix = torch.randint(len(aliceTokens)-seq_len,size=(batch_size,))\n","  X  = aliceTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # forward pass and get loss\n","  modelAlice.zero_grad()\n","  outputs = modelAlice(X,labels=X)\n","\n","  # backprop and store loss\n","  outputs.loss.backward()\n","  optimizerAlice.step()\n","  lossAlice[sampli] = outputs.loss.item()\n","  ### ---------------------\n","\n","\n","  ### --- EDGAR fine-tuning\n","  # get a batch of data\n","  ix = torch.randint(len(edgarTokens)-seq_len,size=(batch_size,))\n","  X  = edgarTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # forward pass and get loss\n","  modelEdgar.zero_grad()\n","  outputs = modelEdgar(X,labels=X)\n","\n","  # backprop and store loss\n","  outputs.loss.backward()\n","  optimizerEdgar.step()\n","  lossEdgar[sampli] = outputs.loss.item()\n","  ### ---------------------\n","\n","  # update progress display\n","  if sampli%77==0:\n","    print(f'Sample {sampli:4}/{num_samples}, losses (Alice/Edgar): {lossAlice[sampli]:.2f}/{lossEdgar[sampli]:.2f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oMKA4BKnky1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.figure(figsize=(10,3))\n","plt.plot(lossAlice,'k',markersize=8,label='ALICE loss')\n","plt.plot(lossEdgar,'b',markersize=8,label='EDGAR loss')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Data sample',ylabel='Loss',xlim=[0,num_samples])\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ui67c2XjWcTO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Have the models chat with each other"],"metadata":{"id":"xoo3ezKl6PlU"}},{"cell_type":"code","source":["# kick-off the convo\n","outAlice = tokenizer.encode('Hello, my name is Alice.', return_tensors='pt').to(device)\n","print('\\n\\n** Alice says:\\n',tokenizer.decode(outAlice[0].cpu()))\n","\n","for _ in range(5):\n","\n","  # Edgar's turn\n","  outEdgar = modelEdgar.generate(outAlice,max_new_tokens=50,do_sample=True,pad_token_id=50256)\n","  print(f'\\n\\n** Edgar says (total token count: {len(outEdgar[0])}):\\n',\n","        tokenizer.decode(outEdgar[0][len(outAlice[0]):].cpu()))\n","\n","  # Alice's turn\n","  outAlice = modelAlice.generate(outEdgar,max_new_tokens=50,do_sample=True,pad_token_id=50256)\n","  print(f'\\n\\n** Alice says (total token count: {len(outAlice[0])}):\\n',\n","        tokenizer.decode(outAlice[0][len(outEdgar[0]):].cpu()))"],"metadata":{"id":"BbEGXhbXaB3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q0DI6Bkejdve"},"execution_count":null,"outputs":[]}]}