{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyN9Y79T2Gl29EDUR89QSalE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Fine-tune a pretrained GPT2<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"wzuj7gIJkgWr"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","import requests\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameters\n","seq_len    = 256 # max sequence length\n","batch_size =  16\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"7AInxn0_kzOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize the text\n","# Gulliver's travels :)\n","text = requests.get('https://www.gutenberg.org/cache/epub/829/pg829.txt').text\n","\n","# the old way:\n","gtTokens = torch.tensor( tokenizer.encode(text),dtype=torch.long )\n","print(gtTokens.shape)\n","\n","# a better way ;)\n","gtTokens = tokenizer.encode(text,return_tensors='pt')\n","print(gtTokens.shape)\n","\n","# but the rest of the code is setup for dimensionless tensors\n","gtTokens = gtTokens[0]\n","print(gtTokens.shape)"],"metadata":{"id":"lP_txTZrkzLn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the most frequent 100 tokens\n","uniq,counts = np.unique(gtTokens,return_counts=True)\n","freqidx = np.argsort(counts)[::-1]\n","top100 = uniq[freqidx[:100]]\n","\n","for t in top100:\n","  print(f'Token {t:5} appears {torch.sum(gtTokens==t):4} times and is \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"RxWzr6U4ky9x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SFVusrX5qM1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# move the model to the GPU\n","gpt2 = gpt2.to(device)"],"metadata":{"id":"QlWxZTAaky6q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check out some text\n","prompt = 'I cannot believe that'\n","in2gpt = tokenizer.encode(prompt,return_tensors='pt').to(device)\n","\n","output = gpt2.generate(in2gpt,max_length=100,pad_token_id=50256,do_sample=True).cpu()\n","print(tokenizer.decode(output[0]))"],"metadata":{"id":"y1c4MlmemixA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hek7DkLW1ngZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantify the frequency of GT frequent tokens in GPT's output"],"metadata":{"id":"Pnw74CpE1nV-"}},{"cell_type":"code","source":["numreps =  10 # number of random repetitions\n","numtoks = 100 # output length\n","\n","# random starting tokens\n","randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n","\n","# generate some data\n","out = gpt2.generate(\n","  randstarts,\n","  max_length = numtoks+1,\n","  min_length = numtoks+1,\n","  do_sample  = True,\n","  bad_words_ids = [tokenizer.encode(tokenizer.eos_token)],\n","  pad_token_id = tokenizer.encode(tokenizer.eos_token)[0]\n",").cpu()\n","\n","print(out,'\\n')\n","\n","for o in out:\n","  print('\\n*** Next batch of output:')\n","  print(tokenizer.decode(o))"],"metadata":{"id":"0H6PJrd92LEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate and report the percentage\n","percentFreqTokens_pre = np.mean(100*np.isin(out[:,1:],top100).flatten())\n","print(f\"Gulliver's travels common tokens appeared in {percentFreqTokens_pre}% of new tokens.\")"],"metadata":{"id":"EbExPkfLV_fN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BLpOjjndmitp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune the model"],"metadata":{"id":"wkXTHacS2HyC"}},{"cell_type":"code","source":["# create the optimizer functions (note the small learning rate)\n","optimizer = torch.optim.AdamW(gpt2.parameters(), lr=5e-5, weight_decay=.01)\n","\n","# Note: don't need the loss function here, because it's calculated internally in the model (thanks HF :D )"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_samples = 1234\n","\n","# initialize losses\n","train_loss = np.zeros(num_samples)\n","\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  ix = torch.randint(len(gtTokens)-seq_len,size=(batch_size,))\n","  X  = gtTokens[ix[:,None] + torch.arange(seq_len)]\n","\n","  # move data to GPU\n","  X = X.to(device)\n","\n","  # clear previous gradients\n","  gpt2.zero_grad()\n","\n","  # forward pass (Hugging Face shifts X internally to get y)\n","  outputs = gpt2(X,labels=X)\n","  loss = outputs.loss\n","\n","  # backprop\n","  loss.backward()\n","  optimizer.step()\n","\n","  # store the per-sample loss\n","  train_loss[sampli] = loss.item()\n","\n","  # update progress display\n","  if sampli%77==0:\n","    print(f'Sample {sampli:4}/{num_samples}, train loss: {train_loss[sampli]:.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oMKA4BKnky1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.figure(figsize=(8,4))\n","plt.plot(train_loss,'k',markersize=8)\n","\n","plt.gca().set(xlabel='Data sample',ylabel='Train loss',xlim=[-1,num_samples])\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1wZg_eplZBDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Qualtative assessment\n","prompt = 'I cannot believe that'\n","in2gpt = tokenizer.encode(prompt,return_tensors='pt').to(device)\n","\n","output = gpt2.generate(in2gpt,max_length=100,pad_token_id=50256)\n","print(tokenizer.decode(output[0]))"],"metadata":{"id":"4ql-aaqpZBAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7eA3Ak8LZA82"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Calculate percentage of GT tokens generated"],"metadata":{"id":"IY3ph6roofPI"}},{"cell_type":"code","source":["# random starting tokens\n","randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n","\n","# generate some data\n","out = gpt2.generate(\n","  randstarts,\n","  max_length = numtoks+1,\n","  min_length = numtoks+1,\n","  do_sample  = True,\n","  bad_words_ids = [tokenizer.encode(tokenizer.eos_token)],\n","  pad_token_id = tokenizer.encode(tokenizer.eos_token)[0]\n",").cpu()\n","\n","\n","for o in out:\n","  print('\\n*** Next batch of output:')\n","  print(tokenizer.decode(o))"],"metadata":{"id":"fVcT-MNE4OHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate and report the percentage\n","percentFreqTokens_pst = np.mean(100*np.isin(out[:,1:],top100).flatten())\n","\n","print(f'Common GT tokens usage went from {percentFreqTokens_pre:.2f}% to {percentFreqTokens_pst:.2f}% after fine-tuning.')"],"metadata":{"id":"6tMVZm72WjEu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vkbGVHeVofFw"},"execution_count":null,"outputs":[]}]}