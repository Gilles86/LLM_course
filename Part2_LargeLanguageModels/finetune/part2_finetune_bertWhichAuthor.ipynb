{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyM5uYoZ/UDpPFICgHeJRK+A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>BERT decides: Alice or Edgar?<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"qyGsqU_wT0qV"}},{"cell_type":"code","source":["# typical python libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import requests\n","\n","# pytorch libraries\n","import torch\n","import torch.nn as nn\n","\n","# huggingface libraries\n","from transformers import BertModel, BertTokenizer"],"metadata":{"id":"-mhNmp9CT17A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import the BERT model and tokenizer\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"zGSG7ILmT13L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dMKtr8CWeK72"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import the two datasets"],"metadata":{"id":"KSEbXYTweK5M"}},{"cell_type":"code","source":["# Alice in Wonderland\n","text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n","aliceTokens = torch.tensor( tokenizer.encode(text),dtype=torch.long )\n","\n","# Edgar Allen Poe\n","text = requests.get('https://www.gutenberg.org/cache/epub/2148/pg2148.txt').text\n","edgarTokens = torch.tensor( tokenizer.encode(text),dtype=torch.long )"],"metadata":{"id":"wLWXY0dnb4AA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DglXfXWcb36U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create an LLM model using pretrained BERT with a new head"],"metadata":{"id":"mQ0JGbZ3T1pm"}},{"cell_type":"code","source":["class BertForBinaryClassification(nn.Module):\n","  def __init__(self, num_labels=2):\n","    super(BertForBinaryClassification, self).__init__()\n","\n","    # Load the pre-trained BERT model.\n","    self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","    # classification head that converts the 768-d pooled output into 2 final outputs.\n","    self.classifier = nn.Linear(768,2)\n","    self.dropout = nn.Dropout(.1) # hard-coded dropout at 10%\n","\n","    # initialize the weights and biases\n","    nn.init.xavier_uniform_(self.classifier.weight)\n","    nn.init.zeros_(self.classifier.bias)\n","\n","\n","  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","\n","    # forward pass through the downloaded (pretrained) BERT\n","    outputs = self.bert(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask,\n","      token_type_ids=token_type_ids)\n","\n","    # extract the pooled output and apply dropout\n","    pooled_output = self.dropout( outputs.pooler_output )\n","\n","    # final push through the classification layer.\n","    logits = self.classifier(pooled_output)\n","    return logits"],"metadata":{"id":"05j-M4MRb4Iy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance of the model and test it:\n","model = BertForBinaryClassification().to(device)\n","model"],"metadata":{"id":"SLb0HwWLciY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1pvNPedCPB2I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare to fine-tune the model"],"metadata":{"id":"Ftzlsxq_b33g"}},{"cell_type":"code","source":["# training hyperparameters\n","num_training = 150\n","batch_size = 64\n","seq_len = 256\n","\n","# optimizer and loss function\n","optimizer = torch.optim.AdamW(model.parameters(),lr=5e-7)\n","loss_fun = nn.CrossEntropyLoss()"],"metadata":{"id":"LrR83zjvoy-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZaKLzZEpCyJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a batch of data\n","ixA = torch.randint(len(aliceTokens)-seq_len,size=(batch_size//2,))\n","ixE = torch.randint(len(edgarTokens)-seq_len,size=(batch_size//2,))\n","X   = torch.concatenate(\n","    (aliceTokens[ixA[:,None] + torch.arange(seq_len)],\n","     edgarTokens[ixE[:,None] + torch.arange(seq_len)]), axis=0).to(device)\n","\n","# and the labels (same for every batch)\n","labels = torch.concatenate((\n","            torch.zeros(batch_size//2,dtype=torch.long),\n","            torch.ones(batch_size//2,dtype=torch.long)),\n","                        axis=0).to(device)\n","\n","print(f'Data batch shape: {X.shape}')\n","print(f'Labels batch shape: {labels.shape}')\n","\n","\n","# forward pass, get model predictions, and report the loss+accuracy\n","logits = model(X)\n","predLabels = torch.argmax(logits, dim=1)\n","loss = loss_fun(logits,labels).item()\n","\n","print('\\nPredicted labels:\\n',predLabels)\n","print('Actual labels:\\n',labels)\n","\n","print(f'\\nLoss: {loss:.4f}')\n","print(f'\\nAccuracy: {(predLabels == labels).sum().item()/batch_size}')"],"metadata":{"id":"D5ttZStGLSfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fGCILEs4QA3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train the model"],"metadata":{"id":"_iFZPYNUQA00"}},{"cell_type":"code","source":["# initialize performance metrices\n","losses = np.zeros(num_training)\n","accuracy = np.zeros(num_training)\n","\n","\n","## loop over data samples\n","for sampli in range(num_training):\n","\n","  # create a batch of data ('labels' created in previous cell)\n","  ixA = torch.randint(len(aliceTokens)-seq_len,size=(batch_size//2,))\n","  ixE = torch.randint(len(edgarTokens)-seq_len,size=(batch_size//2,))\n","  X   = torch.concatenate(\n","           (aliceTokens[ixA[:,None] + torch.arange(seq_len)],\n","            edgarTokens[ixE[:,None] + torch.arange(seq_len)]), axis=0).to(device)\n","\n","  # clear the previous gradients\n","  optimizer.zero_grad()\n","\n","  # forward pass and get model predictions\n","  logits = model(X)\n","  predLabels = torch.argmax(logits, dim=1)\n","\n","  # calculate and store loss + average accuracy\n","  loss = loss_fun(logits,labels)\n","  losses[sampli] = loss.item()\n","  accuracy[sampli] = (predLabels == labels).sum().item()/batch_size\n","\n","  # backward pass\n","  loss.backward()\n","\n","  # update the weights and the learning rate\n","  optimizer.step()\n","\n","  # test the model and report losses every k samples\n","  if sampli%7 == 0:\n","    # report the results\n","    print(f'Sample {sampli:4}/{num_training}, losses: {losses[sampli]:.2f}, accuracy: {accuracy[sampli]:.2f}')"],"metadata":{"id":"v5m3eWnynMeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mean-smoothing function\n","def meansmooth(x,k=3):\n","  y = x+0 # copy of the data\n","  w = (k-1)//2 # number of elements to average on either side\n","\n","  # loop over samples\n","  for i in range(w,len(x)-w):\n","    y[i] = x[i-w:i+w].mean() # centered mean\n","\n","  return y\n","\n","# demo\n","x = np.array([1,5,3,0,4,1,2,-2,-1])\n","y = meansmooth(x)\n","plt.plot(x,'s-',label='Original')\n","plt.plot(y,'o-',label='Smoothed')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"xIniaN2iR1Rq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,ax = plt.subplots(1,figsize=(8,4))\n","\n","# plot the losses\n","ax.plot(losses,'C0',linewidth=.5)\n","ax.plot(meansmooth(losses,5),'C0')\n","ax.set_ylabel('Loss',color='C0')\n","ax.tick_params(axis='y',color='C0',labelcolor='C0')\n","\n","axr = ax.twinx()\n","axr.plot(accuracy,'C1',linewidth=.5)\n","axr.plot(meansmooth(accuracy,5),'C1')\n","axr.set_ylabel('Accuracy',color='C1')\n","axr.tick_params(axis='y',color='C1',labelcolor='C1')\n","\n","ax.set(xlabel='Training sample',xlim=[-1,num_training])\n","plt.show()"],"metadata":{"id":"gR23cLZfOoCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B9VzgEzjnMYZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save the model"],"metadata":{"id":"BSrv0k26ThNF"}},{"cell_type":"code","source":["torch.save(model.state_dict(),'bert_classifier_AliceVsEdgar.pt')\n","# don't forget to download it :D"],"metadata":{"id":"NvCsnsw9ThFC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LFe1ZCRzTqKM"},"execution_count":null,"outputs":[]}]}