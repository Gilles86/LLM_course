{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPAD1uJX2q8UEashDSkUmC4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Clip, freeze, and schedule BERT<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"qyGsqU_wT0qV"}},{"cell_type":"code","source":["# run this code, then restart the python session (and then comment it out)\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"wyNnAMu7Sdn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# typical python libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","# pytorch libraries\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","# huggingface libraries\n","from transformers import BertModel, BertTokenizer\n","from transformers import get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\n","from datasets import load_dataset, DatasetDict"],"metadata":{"id":"-mhNmp9CT17A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"zGSG7ILmT13L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DglXfXWcb36U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: The data and the model"],"metadata":{"id":"KSEbXYTweK5M"}},{"cell_type":"code","source":["# load the IMDB dataset (from HF)\n","dataset = load_dataset('imdb')\n","\n","# reduce the size (overwriting the variable!)\n","dataset = DatasetDict({split:dataset[split].select(range(5_000,20_000)) for split in ['train','test']})"],"metadata":{"id":"wLWXY0dnb4AA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","plt.plot(dataset ,'m.',markersize=1,alpha=.2)\n","\n","plt.show()"],"metadata":{"id":"vA77mq5YeVhR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define a tokenization function that processes each data sample\n","def tokenize_function(one_sample):\n","  return tokenizer(\n","    one_sample['text'],\n","    max_length = 512,         # max sequence length\n","    padding    = 'max_length',# pad to max length\n","    truncation = True)        # truncate if longer than max_length\n","\n","\n","# apply the tokenization function to the dataset (batched for efficiency)\n","tokenized_dataset = dataset.map(, batched=True)\n","\n","# remove text pair\n","tokenized_dataset = tokenized_dataset.remove_columns\n","\n","# change format to pytorch tensors\n","tokenized_dataset.set_format('something', columns=['input_ids', 'attention_mask', 'label'])\n","\n","# create DataLoaders for training and testing\n","train_dataloader = DataLoader(\n","test_dataloader  = DataLoader("],"metadata":{"id":"kJh6cuaYePG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(train_dataloader))"],"metadata":{"id":"wFui0MzSErj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rGN-O0kbTyT_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create an LLM model using pretrained BERT with a new head"],"metadata":{"id":"mQ0JGbZ3T1pm"}},{"cell_type":"code","source":["class BertForBinaryClassification(nn.Module):\n","  def __init__(self, num_labels=2):\n","    super(BertForBinaryClassification, self).__init__()\n","\n","    # load the pre-trained BERT model\n","    self.bert =\n","\n","    # classification head that converts the 768-d pooled output into 2 final outputs\n","    self.classifier = nn.Linear\n","    self.dropout = nn.Dropout() # 10%\n","\n","    # initialize the weights and biases\n","    nn.init.xavier_uniform_(self.classifier.weight)\n","    nn.init.zeros_(\n","\n","\n","  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","\n","    # forward pass through the downloaded (pretrained) BERT\n","    outputs = self.bert(\n","      input_ids      =\n","      attention_mask =\n","      token_type_ids =\n","\n","    # extract the pooled output and apply dropout\n","    pooled_output = self.dropout(\n","\n","    # final push through the classification layer.\n","    logits =\n","    return"],"metadata":{"id":"05j-M4MRb4Iy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance of the model and test it:\n","model = BertForBinaryClassification().to(device)"],"metadata":{"id":"SLb0HwWLciY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.bert"],"metadata":{"id":"eyQcm-eJajWs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"axWMEPr-UFz4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Freeze the attention weights"],"metadata":{"id":"SWTS7RozUFxa"}},{"cell_type":"code","source":["# parameter counts\n","trainParamsCount\n","frozenParamsCount\n","\n","for name,param in model.named_parameters():\n","  if ('attention' in name) or ('embeddings' in name):\n","\n","    print(f'--- Layer {name} is frozen (.requires_grad = {param.requires_grad}).')\n","\n","  else:\n","\n","    print(f'+++ Layer {name} is trainable (.requires_grad = {param.requires_grad}).')\n","\n","print(f'\\n\\nThere are {:,} ({:.2f}%) frozen weights,')\n","print(f'      and {:,} ({:.2f}%) trainable weights.')"],"metadata":{"id":"VRcn-ubxOZPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RRwIIXP8T8sN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Setup a learning rate scheduler"],"metadata":{"id":"Ftzlsxq_b33g"}},{"cell_type":"code","source":["# training steps\n","num_samples = 300\n","\n","# create an optimizer with a \"model\"\n","optimizer = torch.optim.AdamW(,lr=3e-5)\n","\n","# learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    ,\n","    num_warmup_steps = ,  # gentle 5% warmup\n","    num_training_steps =  # steps set to 150% so the lr stays >0\n",")\n","\n","# quick test to see the learning rates\n","lrs = np.zeros(num_samples)\n","for i in range(num_samples):\n","   # update the optimizer\n","   # run the scheduler\n","  lrs[i] = scheduler. # get the actual learning rate\n","\n","# plot!\n","plt.figure(figsize=(10,3))\n","plt.plot(lrs,'ko',markersize=5,markerfacecolor=[.7,.7,.9],alpha=.3)\n","\n","plt.gca().set(xlabel='Training epoch',ylabel='Learning rate')\n","plt.show()"],"metadata":{"id":"zeATs9lCVGQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now for the real optimizer and loss function\n","optimizer = torch.optim.AdamW(,lr=1e-5)\n","loss_fun =  # (cross-entropy loss for multi-class classification)\n","\n","# IMPORTANT: redefine the scheduler\n","scheduler ="],"metadata":{"id":"LrR83zjvoy-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WzQDZ1pPVGOi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Fine-tune with adjustments"],"metadata":{"id":"ZnZHBtlrVGMI"}},{"cell_type":"code","source":["# initialize performance metrices\n","train_losses = np.zeros(num_samples)\n","train_accuracy = np.zeros(num_samples)\n","test_losses = np.zeros(num_samples)\n","test_accuracy = np.zeros(num_samples)\n","norms = np.zeros((num_samples,2))\n","\n","\n","## loop over data samples\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  batch =\n","\n","  # and move it to the GPU\n","  tokenz  = batch['input_ids']\n","  att_msk =\n","  labels  =\n","\n","  # clear the previous gradients\n","\n","\n","  # forward pass and get model predictions\n","  logits = model()\n","  predLabels = torch.argmax\n","\n","  # calculate and store loss + average accuracy\n","  loss = loss_fun\n","  train_losses[sampli] =\n","  train_accuracy[sampli] =\n","\n","  # backward pass\n","\n","\n","  # get two gradient norms\n","  norms[sampli,0] = torch.norm().item()\n","  norms[sampli,1] = torch.norm().item()\n","\n","  # gradient clipping to prevent exploding gradients\n","  nn.utils.c\n","\n","  # update the weights and the learning rate\n","\n","\n","  # test the model and report losses every k samples\n","  if sampli%10 == 0:\n","\n","    # evaluation using the test set\n","    model.eval()\n","    with torch.no_grad():\n","\n","      # get a batch of data and move it to the GPU\n","\n","      # forward pass and get model predictions\n","\n","      # calculate and store loss + accuracy\n","\n","      # report the results\n","      print(f'Sample {sampli:4}/{num_samples}, losses (train/test): {train_losses[sampli]:.2f}/{test_losses[sampli]:.2f}, accuracy: {train_accuracy[sampli]:.2f}/{test_accuracy[sampli]:.2f}')\n","\n","      # put the model back into train mode\n","      model.train()"],"metadata":{"id":"v5m3eWnynMeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","# plot the losses\n","axs[0].plot(train_losses,'ko',markerfacecolor=[.9,.7,.7],alpha=.5,label='Train')\n","axs[0].plot(range(0,num_samples,10),test_losses[::10],'ks-',\n","            markerfacecolor=[.7,.9,.7],markersize=8,alpha=.8,linewidth=3,label='Test')\n","axs[0].set(xlabel='Training sample',ylabel='Loss',title='Cross-entropy loss',xlim=[0,num_samples])\n","axs[0].legend()\n","\n","# plot the prediction accuracy\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"dKn24FCtnMbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B9VzgEzjnMYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3.6))\n","\n","# plot the losses\n","axs[0].plot(norms[:,0],'ko',markerfacecolor=[.9,.7,.7],alpha=.7)\n","axs[0].set(xlabel='Training sample',ylabel='Norm',title='MLP: Pre-clip gradient norm',xlim=[0,num_samples])\n","\n","axs[1].plot(norms[:,1],'ks',markerfacecolor=[.7,.9,.7],alpha=.7)\n","axs[1].set(xlabel='Training sample',ylabel='Norm',title='Classifier: Pre-clip gradient norm',xlim=[0,num_samples])\n","\n","\n","plt.show()"],"metadata":{"id":"BJ15c00KZieS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1ViIXBoOZiY0"},"execution_count":null,"outputs":[]}]}