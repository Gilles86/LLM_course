{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyN/OjWyGIBWJkkD96gieygy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Impact of freezing neo on fine-tuning<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"6nQjZWUP_hiM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","import time\n","import requests\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"ng-VWPU_eXPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","tokenizer.pad_token_id = tokenizer.encode(' ')[0]\n","\n","# -> GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# load in two GPTneo's and push to GPU\n","modelFreeze =\n","modelTrain  ="],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JQ1jAf314fxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Find the most frequent 100 tokens"],"metadata":{"id":"qkJR84ju20PE"}},{"cell_type":"code","source":["# Moby Dick\n","text = requests.get('https://www.gutenberg.org/cache/epub/2701/pg2701.txt\n","\n","# summary\n","print"],"metadata":{"id":"SS8ySA1SHPhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the most common tokens and print the top-100\n"],"metadata":{"id":"dxaG9S93Xed4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WUVeZyhvA7M5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numreps =  10 # number of random repetitions\n","numtoks = 100 # output length\n","\n","# initialize\n","tokenUsage = np.zeros() # [ pre/post , Freeze/Train ]\n","\n","# random starting tokens\n","randstarts = torch.\n","\n","\n","# FREEZE: generate and store tokens\n","outFreeze = modelFreeze.generate\n","genTokensFreeze =\n","\n","\n","# TRAIN: same as above :)\n","\n","genTokensTrain =\n","\n","\n","# calculate the percentage\n","tokenUsage[0,0] =\n","tokenUsage[0,1] ="],"metadata":{"id":"yIr2iS2sA7Jy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenUsage"],"metadata":{"id":"ZRjHqpxAA7G3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"J9TdsZHtA7EB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Targeted training"],"metadata":{"id":"W36VFhVsVc_Z"}},{"cell_type":"code","source":["for name,param in modelFreeze.named_parameters():\n","  splitstr = name.split('.')\n","  print(splitstr)"],"metadata":{"id":"xxPwuHc7z6Nc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TEST: identify QVK weights in layers >5\n","for name,param in modelFreeze.named_parameters():\n","  splitstr = name.split('.')\n","  if (len(splitstr)>5) and (splitstr[3]=='attn'):\n","    if (int(splitstr[2])>5) and (splitstr[5][0] in 'qvk'):\n","      print(name)"],"metadata":{"id":"2hM9ArOazuw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for name,param in modelFreeze.named_parameters():\n","\n","  # split the name by .\n","\n","\n","  # see if this fits our filter\n","  if\n","      print(f'+++ Layer {name} is trainable (.requires_grad = {param.requires_grad}).')\n","\n","  # otherwise, freeze the layer\n","  else:\n","\n","    print(f'--- Layer {name} is frozen (.requires_grad = {param.requires_grad}).')"],"metadata":{"id":"8AxhjmrvVg5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aQXP2zWkB1j2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Fine-tune the models"],"metadata":{"id":"wkXTHacS2HyC"}},{"cell_type":"code","source":["# FREEZE optimizer\n","optimizerFreeze = (modelFreeze.parameters(), lr=.0005)\n","\n","# TRAIN optimizer\n","optimizerTrain = torch.optim."],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training parameters\n","seq_len     = 256 # max sequence length\n","batch_size  =  16\n","num_samples = 474"],"metadata":{"id":"CwOb8RCqT92V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize losses\n","losses = np.zeros((num_samples,2))\n","delta_norm_em = np.zeros((num_samples,2))\n","\n","# and computation times\n","timeTrain = 0\n","timeFreeze = 0\n","\n","\n","# grab the initial MLP weights for comparison\n","prev_emFreeze =\n","prev_emTrain = modelTrain.transformer.h\n","\n","\n","\n","# and run the training!\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  ix = torch.randint(len(tokens)-seq_len,size=(batch_size,))\n","  X  = tokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","\n","  ### --- FREEZE fine-tuning\n","  # forward pass and get loss\n","  start_time = time.time() # start the timer\n","  timeFreeze += -start_time # end the timer and add\n","  ### ---------------------\n","\n","\n","  ### --- TRAIN fine-tuning\n","  # forward pass and get loss\n","  ### ---------------------\n","\n","\n","\n","  ### --- matrix norm to assess change in MLP layer\n","  delta_norm_em[sampli,0] = torch.norm( - )\n","  prev_emFreeze = # the current matrix becomes the \"previous\" for the next iteration\n","\n","  delta_norm_em[sampli,1] = # same for the train model\n","  prev_emTrain =\n","\n","\n","\n","\n","  # update progress display\n","  if sampli%37==0:\n","    print(f'Sample {sampli:4}/{num_samples}, losses (Freeze/Train): {losses[sampli,0]:.2f}/{losses[sampli,1]:.2f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Zxmfn70YB1eI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Visualize the results"],"metadata":{"id":"WCeG4Md65HId"}},{"cell_type":"code","source":["# plot the losses\n","_,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# random starting tokens\n","randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n","\n","\n","# FREEZE: generate and store tokens\n","\n","\n","\n","# TRAIN: same as above :)\n","\n","\n","# calculate the percentage\n","tokenUsage[1,0] = np.mean(100*np.isin(genTokensFreeze,top100))\n","tokenUsage[1,1] = np.mean(100*np.isin(genTokensTrain ,top100))"],"metadata":{"id":"mJIX3fdAB1bg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode(genTokensTrain))"],"metadata":{"id":"UMnLmZw7Q5Xc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4yHFImklQ5US"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize the results!\n","\n","_,axs = plt.subplots(1,2,figsize=(10,3))\n","\n","# show the pre-train token usage\n","axs[0].bar([.8,1.8],,width=.4,label='BEFORE')\n","axs[0].bar([1.2,2.2],,width=.4,label='AFTER')\n","axs[0].set(ylim=[min(tokenUsage.flatten())-2,max(tokenUsage.flatten())+2],xticks=[1,2],xlim=[.3,2.6],\n","           xticklabels=['FREEZE model','TRAIN model'],ylabel='Percent generated (%)',title='Common Moby Dick tokens generated')\n","axs[0].legend()\n","\n","axs[1].bar([1,2],\n","axs[1].set(xticks=[1,2],xlim=[.3,2.6],xticklabels=['FREEZE model','TRAIN model'],\n","           ylabel='Change in generated tokens (%)',title='Post- minus pre-training')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"G6yCCy3OELTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pfiB2AVGELPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how did the embeddings weights change?\n","plt.figure(figsize=(8,3))\n","\n","plt.plot(delta_norm_em[:,0],linewidth=2,label='FREEZE')\n","plt.plot(delta_norm_em[:,1],linewidth=2,label='TRAIN')\n","\n","plt.legend()\n","plt.gca().set(xlim=[0,num_samples],xlabel='Training sample',ylabel='Matrix difference norm')\n","plt.show()"],"metadata":{"id":"j7ztvZzwELJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sgtdgDaWKiIg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computation time\n","plt.bar\n","plt.gca().set(xticks=[1,2],xticklabels=['FREEZE','TRAIN'],ylabel='Computation time (s)',\n","              ylim=[min(timeFreeze,timeTrain)*.8,max(timeFreeze,timeTrain)*1.2],\n","              title=f'Computation time across {num_samples} training samples')\n","plt.show()"],"metadata":{"id":"ta3y7rchpbys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oNjaoy6qpZ_P"},"execution_count":null,"outputs":[]}]}