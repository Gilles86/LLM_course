{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMhF+WXo8K/3T2TgBV5MNs6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Alice in Wonderland and Edgar Allen Poe (with GPT-neo)<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"vjwYlttCAj7X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","!pip install torchinfo\n","from torchinfo import summary\n","\n","import requests"]},{"cell_type":"code","source":[],"metadata":{"id":"ng-VWPU_eXPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","tokenizer.pad_token_id = tokenizer.encode(' ')[0]\n","\n","# load in two GPTneo's and push to GPU\n","modelAlice = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","modelEdgar = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# -> GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","modelAlice = modelAlice.to(device)\n","modelEdgar = modelEdgar.to(device)"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qH0so0epSGWu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inspect the model"],"metadata":{"id":"gyfc5TFiSGHm"}},{"cell_type":"code","source":["# inspect the model\n","modelAlice"],"metadata":{"id":"AuFrjIoXR8z-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# accessing a particular weights matrix\n","modelAlice.transformer.h[3].attn.attention.k_proj.weight.shape"],"metadata":{"id":"6YjYUSXtSfwb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model summary\n","x = tokenizer.encode('What did the Red Queen say to Alice?', return_tensors='pt').to(device)\n","summary(modelAlice, input_data=x, col_names=['input_size','output_size','num_params'])"],"metadata":{"id":"63L-w1CaSJHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# are the embeddings and unembeddings tied?\n","print('** Embedding:\\n',modelAlice.transformer.wte.weight.detach())\n","print('\\n** Unembedding:\\n',modelAlice.lm_head.weight.detach())"],"metadata":{"id":"YYlgH1-93F67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DZl1HbpCSJE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Explore the tokenizer"],"metadata":{"id":"7WIjOKWDR7_Q"}},{"cell_type":"code","source":["# A bit about their tokenizer\n","print(f'Tokenizer has {tokenizer.vocab_size:,} tokens.\\nA few random tokens:\\n')\n","\n","for i in range(30):\n","  # generate a random token\n","  randtok = torch.randint(tokenizer.vocab_size,(1,))\n","  print(f'Token {randtok[0]:5} is \"{tokenizer.decode(randtok)}\"')"],"metadata":{"id":"mWre9N6hPf2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the raw output shape\n","into = tokenizer.encode('What did the Red Queen say to Alice?', return_tensors='pt').to(device)\n","\n","out = modelAlice(into)\n","print(out.logits.shape) # [batch, tokens, embedding]"],"metadata":{"id":"e7FPvccFALDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# text generation\n","out = modelAlice.generate(into,max_new_tokens=120,do_sample=True,pad_token_id=50256)\n","print(tokenizer.decode(out[0].cpu()))"],"metadata":{"id":"xaEg6sE3HPeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nKDP6zFpPuzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# is it different from GPT's tokenizer?\n","\n","from transformers import GPT2Tokenizer\n","gptTokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"K6KmbOwo4Oz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(30):\n","  # generate a random token\n","  randtok = torch.randint(tokenizer.vocab_size,(1,))\n","\n","  # get the token text for both tokenizers\n","  e = tokenizer.decode(randtok)\n","  g = gptTokenizer.decode(randtok)\n","  print(f'Token {randtok[0]:5} is \"{e}\" and \"{g}\"')"],"metadata":{"id":"0LlInJMl4OxM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yblWbj_O4Ooq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import and process texts"],"metadata":{"id":"bCqF2dybPuwr"}},{"cell_type":"code","source":["# Through the Looking Glass (aka Alice in Wonderland)\n","text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n","aliceTokens = torch.tensor( tokenizer.encode(text),dtype=torch.long )\n","\n","# Edgar Allen Poe\n","text = requests.get('https://www.gutenberg.org/cache/epub/2148/pg2148.txt').text\n","edgarTokens = torch.tensor( tokenizer.encode(text),dtype=torch.long )\n","\n","# summary\n","print(f'Alice in Wonderland has  {len(aliceTokens):7,} tokens.')\n","print(f'Edgar Allen Poe text has {len(edgarTokens):7,} tokens.')"],"metadata":{"id":"SS8ySA1SHPhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Wq47AnZ2Z57t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare for fine-tuning"],"metadata":{"id":"syOer_DkHPmY"}},{"cell_type":"code","source":["# ALICE optimizer\n","optimizerAlice = torch.optim.AdamW(modelAlice.parameters(), lr=5e-5, weight_decay=.01)\n","\n","# EDGAR optimizer\n","optimizerEdgar = torch.optim.AdamW(modelEdgar.parameters(), lr=5e-5, weight_decay=.01)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training parameters\n","seq_len     = 256 # max sequence length\n","batch_size  =  16\n","num_samples = 476"],"metadata":{"id":"CwOb8RCqT92V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-JhncV9lT9tS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune the model"],"metadata":{"id":"wkXTHacS2HyC"}},{"cell_type":"code","source":["# initialize losses\n","lossAlice = np.zeros(num_samples)\n","lossEdgar = np.zeros(num_samples)\n","\n","\n","# loop over training\n","for sampli in range(num_samples):\n","\n","\n","  ### --- ALICE fine-tuning\n","  # get a batch of data\n","  ix = torch.randint(len(aliceTokens)-seq_len,size=(batch_size,))\n","  X  = aliceTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # forward pass and get loss\n","  modelAlice.zero_grad()\n","  outputs = modelAlice(X,labels=X)\n","\n","  # backprop and store loss\n","  outputs.loss.backward()\n","  optimizerAlice.step()\n","  lossAlice[sampli] = outputs.loss.item()\n","  ### ---------------------\n","\n","\n","  ### --- EDGAR fine-tuning\n","  # get a batch of data\n","  ix = torch.randint(len(edgarTokens)-seq_len,size=(batch_size,))\n","  X  = edgarTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # forward pass and get loss\n","  modelEdgar.zero_grad()\n","  outputs = modelEdgar(X,labels=X)\n","\n","  # backprop and store loss\n","  outputs.loss.backward()\n","  optimizerEdgar.step()\n","  lossEdgar[sampli] = outputs.loss.item()\n","  ### ---------------------\n","\n","  # update progress display\n","  if sampli%77==0:\n","    print(f'Sample {sampli:4}/{num_samples}, losses (Alice/Edgar): {lossAlice[sampli]:.2f}/{lossEdgar[sampli]:.2f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oMKA4BKnky1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.figure(figsize=(8,4))\n","plt.plot(lossAlice,'k',markersize=8,label='ALICE loss')\n","plt.plot(lossEdgar,'b',markersize=8,label='EDGAR loss')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Data sample',ylabel='Loss',xlim=[0,num_samples])\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1wZg_eplZBDV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Qualtative assessment"],"metadata":{"id":"4ql-aaqpZBAN"}},{"cell_type":"code","source":["# input\n","x = tokenizer.encode('What did the Red Queen say to Alice?', return_tensors='pt').to(device)\n","\n","# get the output\n","outAlice = modelAlice.generate(x,max_new_tokens=120,do_sample=True,pad_token_id=50256)\n","outEdgar = modelEdgar.generate(x,max_new_tokens=120,do_sample=True,pad_token_id=50256)\n","\n","# print both models' outputs\n","print('** Alice model says:')\n","print(tokenizer.decode(outAlice[0].cpu()))\n","\n","print('\\n\\n** Edgar model says:')\n","print(tokenizer.decode(outEdgar[0].cpu()))"],"metadata":{"id":"y6YwLjWQ0-Vo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P9YC0HXyHPzb"},"execution_count":null,"outputs":[]}]}