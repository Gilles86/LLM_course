{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNgZeRFQmSblUufZIiP4VZp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Evolution of Alice and Edgar<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"9Wzzry7kWwgR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","from transformers import BertModel, BertTokenizer\n","\n","import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import requests"]},{"cell_type":"code","source":["# -> GPU (note: best to use the A100, or whatever has the highest memory)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"Do-U2b3WW0Av"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UmDamKsAjLhi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Preparations"],"metadata":{"id":"BoO5QS1oWz7u"}},{"cell_type":"code","source":["class BertForBinaryClassification(nn.Module):\n","  def __init__(self, num_labels=2):\n","    super(BertForBinaryClassification, self).__init__()\n","\n","    # load the pre-trained BERT model\n","    self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","    # classification head that converts the 768-d pooled output into 2 final outputs\n","    self.classifier = nn.Linear\n","    self.dropout = nn.Dropout() # hard-coded dropout at 10%\n","\n","    # initialize the weights and biases\n","    nn.init.xavier_uniform_(\n","    nn.init.zeros_(\n","\n","\n","  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","\n","    # forward pass through the downloaded (pretrained) BERT\n","    outputs = self.bert()\n","\n","    # extract the pooled output and apply dropout\n","    pooled_output = self.dropout(\n","\n","    # final push through the classification layer.\n","    logits =\n","    return"],"metadata":{"id":"vmUvS_MsWz4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now upload the trained model (.pt file). Note that it takes a few mins to upload..."],"metadata":{"id":"thmkj95tclvH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a model instance, then replace the params with those from the trained model\n","bert = BertForBinaryClassification().to(device)\n","bert.load_state_dict(torch.load('./bert_classifier_AliceVsEdgar.pt'))\n","\n","# halve the memory of the classifier by converting to float16 from float32\n","bert.half()\n","\n","# and toggle on eval (no training)\n","\n","\n","# and need the BERT tokenizer\n","bertTokenizer ="],"metadata":{"id":"-0LO_QRuWz1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zTYbzE6tWzyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eleuther's tokenizer\n","eleuTokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# load in two GPTneo's and push to GPU\n","modelAlice = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m').to(device)\n","modelEdgar ="],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Alice in Wonderland\n","text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n","aliceTokens = eleuTokenizer.encode(text,return_tensors='pt')[0]\n","\n","# Edgar Allen Poe\n","text = requests.get('https://www.gutenberg.org/cache/epub/2148/pg2148.txt').text\n","edgarTokens = eleuTokenizer.encode(text,return_tensors='pt')[0]"],"metadata":{"id":"SS8ySA1SHPhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"u6__qIG2cywL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Translate text between Eleuther to BERT"],"metadata":{"id":"lvlXr72BaHG_"}},{"cell_type":"code","source":["# issue is that they have different tokenizers, so needs to be translated into text and re-tokenized\n","startingtext = 'Hello, my name is Mike and I like purple.'\n","\n","# eleuther's tokens:\n","eleuToks = eleuTokenizer(\n","\n","# bert's tokens\n","bertToks = bertTokenizer(\n","\n","print(f'Starting text:\\n{startingtext}')\n","print(f'\\n\\nEleuther tokens:\\n{eleuToks}')\n","print(f\"\\nDecoded using Eleuther:\\n{eleuTokenizer.decode}\")\n","print(f\"\\nDecoded using BERT:\\n{bertTokenizer.decode}\")\n","\n","print(f'\\n\\nBERT tokens:\\n{bertToks}')\n","print(f\"\\nDecoded using BERT:\\n{}\")\n","print(f\"\\nDecoded using Eleuther:\\n{}\")"],"metadata":{"id":"MVAkfwC6aMFW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# text -> Eleuther tokens -> text -> BERT tokens\n","\n","# 1) to Eleuther tokens\n","startingtext = 'Hello, my name is Mike and I like purple.'\n","eleuToks =\n","\n","# 2) back to text\n","eleuReconText =\n","\n","# 3) then to bert tokens\n","bertToks =\n","\n","# 4) show the reconstruction\n","bertTokenizer.decode"],"metadata":{"id":"NuScEgeVay4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# translation functions\n","def bert2eleu(bertToks):\n","  b =\n","  e =\n","  return\n","\n","def eleu2bert(eleuToks):\n","  e =\n","  b =\n","  return\n","\n","\n","# test\n","b2e = bert2eleu(bertToks['input_ids'])\n","e2b = eleu2bert(b2e)\n","\n","print(eleuTokenizer.decode(b2e))\n","print(bertTokenizer.decode(e2b))"],"metadata":{"id":"PjXLN6TTwcQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E9RXStDEej1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Have BERT classify Alice/Edgar model outputs"],"metadata":{"id":"omTVUwq-ejsx"}},{"cell_type":"code","source":["seq_len    = 128 # max sequence length\n","batch_size =  64"],"metadata":{"id":"m3wmn8p5nug0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Write a function that generates tokens for BERT to classify"],"metadata":{"id":"DXg-sKw1_B-A"}},{"cell_type":"code","source":["# note: this function creates the batch+labels directly on the GPU\n","\n","# exclude tokens that BERT will ignore\n","tokens_to_exclude = [\n","    eleuTokenizer.encode('\\n'),\n","    eleuTokenizer.encode('\\n\\n'),\n","    eleuTokenizer.encode('\\r'),\n","    eleuTokenizer.encode('\\t'),\n","    eleuTokenizer.encode(' '),\n","    [eleuTokenizer.eos_token_id],\n","]\n","\n","def batch_for_bert():\n","\n","  # batch_size should be even\n","  half_batch =\n","\n","  # initialize batch tensor and labels\n","  batch2classify = torch.zeros\n","  labels = torch.zeros(\n","\n","  # create random starting tokens\n","  randstarts = torch.randint(\n","\n","  # generate tokens in batch\n","  outs_alice = modelAlice.generate(\n","      randstarts,\n","      min_length  = 4*seq_len,\n","      max_length  = 4*seq_len,\n","      do_sample   = True,\n","      early_stopping = False,\n","      eos_token_id = None,\n","      pad_token_id = eleuTokenizer.pad_token_id,\n","      bad_words_ids = tokens_to_exclude,\n","      repetition_penalty = 1.3\n","      )\n","\n","  outs_edgar = modelEdgar.generate(\n","      randstarts,\n","      min_length  = 4*seq_len,\n","      max_length  = 4*seq_len,\n","      do_sample   = True,\n","      early_stopping = False,\n","      eos_token_id = None,\n","      pad_token_id = eleuTokenizer.pad_token_id,\n","      bad_words_ids = tokens_to_exclude,\n","      repetition_penalty = 1.3\n","      )\n","\n","  # fill the batch tensor\n","  for i in range(half_batch):\n","\n","    # first 1/2 is Alice-generated tokens\n","    outA = eleu2bert(outs_alice[i,1:])\n","    batch2classify[i,:] =\n","    labels[i] = 0  # Label \"0\" for Alice (was used in training this BERT model)\n","\n","    # second 1/2 is Edgar-generated tokens\n","\n","\n","  # remove the large matrices from memory\n","  del outs_alice,outs_edgar\n","\n","  return batch2classify, labels"],"metadata":{"id":"amoQs292_B7W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test\n","batch2classify,labels = batch_for_bert()\n","\n","print(batch2classify.shape)\n","print(labels.shape)"],"metadata":{"id":"0FMi-pf_ayyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SW6jQlt0VqHz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT loss function\n","bert_loss_fun = nn.CrossEntropyLoss()\n","\n","# forward pass, get model predictions, and report the loss+accuracy\n","logits = bert\n","predLabels = torch.argmax\n","loss = bert_loss_fun\n","\n","print('\\nPredicted labels:\\n',predLabels)\n","print('Actual labels:\\n',labels)\n","\n","print(f'\\nLoss: {loss:.4f}')\n","print(f'\\nAccuracy: {}')"],"metadata":{"id":"eHQyog4Yv9rU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aSAdiJHtcbB3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Fine-tune the models with BERT classification"],"metadata":{"id":"bKAGJKEDX9DX"}},{"cell_type":"code","source":["# create optimizers and set training params\n","\n","# ALICE optimizer\n","optimizerAlice = torch.optim.AdamW(modelAlice.parameters(), lr=1e-5)\n","\n","# EDGAR optimizer\n","optimizerEdgar = torch.\n","\n","\n","# training parameters\n","num_samples = 121"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize losses\n","lossAlice    = np.zeros(num_samples)\n","lossEdgar    = np.zeros(num_samples)\n","bertAccuracy = np.zeros(num_samples)\n","\n","for sampli in range(num_samples):\n","\n","\n","  ### --- ALICE fine-tuning\n","  # get a batch of data\n","  ix = torch.randint(len(aliceTokens)-seq_len,size=(batch_size,))\n","  X  = aliceTokens[ix[:,None] + torch.arange(seq_len)]\n","\n","  # zero-out gradients, forward pass, get loss\n","\n","  outputs = modelAlice\n","\n","  # backprop and store loss\n","  .backward()\n","  .step()\n","  lossAlice[sampli] =\n","  ### ---------------------\n","\n","\n","  ### --- EDGAR fine-tuning\n","\n","\n","\n","  ### ---------------------\n","\n","  # update progress display\n","  if sampli%10==0:\n","\n","    # have the models generate some text\n","    batch2classify,labels =\n","\n","    # have BERT classify\n","    with torch.no_grad(): # turn off gradient calculations\n","      logits =  # forward pass\n","    predLabels =  # get model predictions\n","    bertAccuracy[sampli] =\n","\n","    print(f"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oMKA4BKnky1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","_,axs = plt.subplots(1,2,figsize=(12,3.3))\n","axs[0].plot(,'k',markersize=8,label='ALICE loss')\n","axs[0].plot(,'b',markersize=8,label='EDGAR loss')\n","axs[0].legend()\n","axs[0].set(xlabel='Training batch',ylabel='Loss',xlim=[-1,num_samples],title='GEN model loss')\n","\n","# plot BERT's accuracy\n","\n","axs[1].plot(x4bert,,'ks-',markerfacecolor=[.7,.7,.9],markersize=8)\n","axs[1].set(ylim=[.2,1.02],xlim=[-5,num_samples+4],\n","           xlabel='Training batch',ylabel='Classification accuracy',title='BERT classifying gen models')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uq7gw9O-Wzg6"},"execution_count":null,"outputs":[]}]}