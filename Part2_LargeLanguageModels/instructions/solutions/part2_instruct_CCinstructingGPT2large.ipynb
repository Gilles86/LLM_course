{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1PGZSoDPtsUjh6fAGjGVCn38SmdAjYy6Y","timestamp":1743082802943}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyN6q9h1rnB3o4SFZsx/W26t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Instruction tuning<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Instruction tuning GPT2-large<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"wzuj7gIJkgWr"}},{"cell_type":"code","source":["# run this code, then restart the python session (and then comment it out)\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"-YukX3EQREEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","from datasets import load_dataset\n","\n","import textwrap\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"0ZksKfHmlNoa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"caPj3XkZlNly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Lengths of questions and answers"],"metadata":{"id":"S5uJPbJ0TYTJ"}},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt2"],"metadata":{"id":"fn8NtF6llKY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UicjeoxbLF8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://huggingface.co/datasets/THUDM/webglm-qa\n","dataset = load_dataset('THUDM/webglm-qa')\n","dataset"],"metadata":{"id":"lP_txTZrkzLn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sampleLengths = np.zeros((len(dataset['train']),2))\n","\n","for i in range(sampleLengths.shape[0]):\n","\n","  sampleLengths[i,0] = len(tokenizer.encode(dataset['train'][i]['question']))\n","  sampleLengths[i,1] = len(tokenizer.encode(dataset['train'][i]['answer']))"],"metadata":{"id":"1NO0ItDhTcJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["binedges = np.linspace(0,400,71)\n","yQ,xQ = np.histogram(sampleLengths[:,0],bins=binedges,density=True)\n","yA,xA = np.histogram(sampleLengths[:,1],bins=binedges,density=True)\n","\n","plt.figure(figsize=(8,4))\n","plt.bar(xA[:-1],yA,width=xA[1]-xA[0],edgecolor='k',facecolor=[.9,.7,.7],alpha=.8,label='Answers')\n","plt.bar(xQ[:-1],yQ,width=xQ[1]-xQ[0],edgecolor='k',facecolor=[.7,.7,.9],alpha=.8,label='Questions')\n","\n","plt.gca().set(xlim=[-5,300],xlabel='Number of tokens',ylabel='Density (pdf estimate)',title='Distribution of Q&A token lengths')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"cPMclKOYUSws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jbEzAFoQTYPh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Create question-starting batches"],"metadata":{"id":"0gM6xCb0V247"}},{"cell_type":"code","source":["# max sequence length\n","seq_len = 256"],"metadata":{"id":"7AInxn0_kzOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initializing (just using the first 10k data samples)\n","trainTokens = torch.full((10000,seq_len),tokenizer.pad_token_id)\n","testTokens  = torch.full((1000,seq_len),tokenizer.pad_token_id)\n","\n","# loop over tokens\n","for idx in range(trainTokens.shape[0]):\n","\n","  # construct the token sequence\n","  txt = f\"QUESTION: {dataset['train'][idx]['question']} ANSWER: {dataset['train'][idx]['answer']}.\"\n","  tokz = tokenizer.encode(txt,add_special_tokens=True)\n","\n","  # insert this sequence into the data matrix, truncating when necessary\n","  endOfSeq = min(seq_len,len(tokz))\n","  trainTokens[idx,:endOfSeq] = torch.tensor(tokz[:endOfSeq])\n","\n","\n","### repeat for test tokens\n","for idx in range(testTokens.shape[0]):\n","  txt = f\"QUESTION: {dataset['validation'][idx]['question']} ANSWER: {dataset['validation'][idx]['answer']}.\"\n","  tokz = tokenizer.encode(txt,add_special_tokens=True)\n","  endOfSeq = min(seq_len,len(tokz))\n","  testTokens[idx,:endOfSeq] = torch.tensor(tokz[:endOfSeq])"],"metadata":{"id":"pzVqTufmWK_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# attention mask\n","attn_mask = (trainTokens[0] != tokenizer.pad_token_id).long()\n","\n","print(f'Training tokens:\\n{trainTokens[0]}\\n')\n","print(f'Attention mask:\\n{attn_mask}')"],"metadata":{"id":"JBpHOZGtV2yk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check a random batch\n","ix = np.random.randint(0,trainTokens[0].shape,8)\n","X  = trainTokens[ix]\n","attn_mask = (X != tokenizer.pad_token_id).long()\n","\n","\n","print(f'Size of batch: {X.shape}')\n","print(f'Size of attention mask: {attn_mask.shape}\\n')\n","\n","print('Some examples:')\n","for t in range(5):\n","  print(f'*** Example: \\n',textwrap.fill(tokenizer.decode(X[t]),123),'\\n')"],"metadata":{"id":"CQp87qEqMrdN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aveAM = (trainTokens == tokenizer.pad_token_id).sum()/torch.numel(trainTokens)\n","print(f'{aveAM*100:5.2f}% of TRAIN token positions are EOS.')\n","\n","aveAM = (testTokens == tokenizer.pad_token_id).sum()/torch.numel(testTokens)\n","print(f'{aveAM*100:5.2f}% of TEST token positions are EOS.')"],"metadata":{"id":"1gxgaoN-qFQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gs46PMeRLP-s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Fine-tune the model"],"metadata":{"id":"wkXTHacS2HyC"}},{"cell_type":"code","source":["# move the model to the GPU\n","gpt2 = gpt2.to(device)\n","\n","# create the optimizer functions\n","optimizer = torch.optim.AdamW(gpt2.parameters(), lr=1e-4, weight_decay=.01)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size  =   8\n","num_samples = 123\n","\n","# initialize losses\n","train_loss = np.zeros(num_samples)\n","test_loss  = np.zeros(num_samples)\n","\n","\n","for sampli in range(num_samples):\n","\n","  # get a batch of data and create a mask\n","  ix = np.random.randint(0,trainTokens[0].shape,batch_size)\n","  X  = trainTokens[ix]\n","  attn_mask = (X != tokenizer.pad_token_id).long()\n","\n","  # move data to GPU\n","  attn_mask = attn_mask.to(device)\n","  X = X.to(device)\n","\n","  # forward pass (Hugging Face shifts X internally to get y)\n","  gpt2.zero_grad()\n","  outputs = gpt2(X,labels=X,attention_mask=attn_mask)\n","  loss = outputs.loss\n","\n","  # backprop\n","  loss.backward()\n","  optimizer.step()\n","\n","  # store the per-sample loss\n","  train_loss[sampli] = loss.item()\n","\n","  # test and update progress display\n","  if sampli%5==0:\n","\n","    # get a batch of data and create a mask\n","    ix = np.random.randint(0,testTokens[0].shape,batch_size)\n","    X  = testTokens[ix]\n","    attn_mask = (X != tokenizer.pad_token_id).long()\n","\n","    # move data to GPU\n","    attn_mask, X = attn_mask.to(device), X.to(device)\n","\n","    # forward pass and get loss\n","    with torch.no_grad():\n","      gpt2.eval()\n","      outputs = gpt2(X,labels=X,attention_mask=attn_mask)\n","      test_loss[sampli] = outputs.loss.item()\n","    gpt2.train()\n","\n","    # report progress\n","    print(f'Sample {sampli:4}/{num_samples}, train/test loss: {train_loss[sampli]:.4f}/{test_loss[sampli]:.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oMKA4BKnky1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.figure(figsize=(10,3.5))\n","plt.plot(train_loss,'k',label='Train loss')\n","x4test = np.where(test_loss)[0]\n","plt.plot(x4test,test_loss[x4test],'r',label='Test loss')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Data sample',ylabel='Loss',xlim=[-1,num_samples])\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1wZg_eplZBDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Qualtative assessment\n","prompt = 'QUESTION: Where does the word \"butterfly\" come from?'\n","# prompt = 'QUESTION: Would it be strange to have a pet rock and feed it styrofoam?'\n","in2gpt = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n","\n","output = gpt2.generate(in2gpt,do_sample=True,max_length=200,pad_token_id=50256)\n","print(textwrap.fill(tokenizer.decode(output[0]),80))"],"metadata":{"id":"4ql-aaqpZBAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vkbGVHeVofFw"},"execution_count":null,"outputs":[]}]}