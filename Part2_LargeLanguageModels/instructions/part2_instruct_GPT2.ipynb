{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1PGZSoDPtsUjh6fAGjGVCn38SmdAjYy6Y","timestamp":1743082802943}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNRP52a+zSdIRPKqawnjv8S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Instruction tuning<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Instruction training with GPT2<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202509\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202509</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"wzuj7gIJkgWr"}},{"cell_type":"code","source":["# run this code, then restart the python session (and then comment it out)\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"hlv9peNAock5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","\n","import textwrap\n","\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","from datasets import load_dataset"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameters\n","seq_len    = 256 # max sequence length\n","batch_size =  32\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"7AInxn0_kzOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hWG9ow-M6obT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import text data"],"metadata":{"id":"Cl1UKIaWLGJu"}},{"cell_type":"code","source":["# https://huggingface.co/datasets/THUDM/webglm-qa\n","dataset = load_dataset('THUDM/webglm-qa')\n","dataset"],"metadata":{"id":"lP_txTZrkzLn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'][3]"],"metadata":{"id":"CJbUNzo9IPT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# my minor modification:\n","txt = 'QUESTION: ' + dataset['train'][0]['question'] + ' ANSWER: ' + dataset['train'][0]['answer']\n","txt"],"metadata":{"id":"odJAM2qV1WxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize the data\n","trainTokens = []\n","\n","# just the first 5k samples (~800k tokens) for speed and simplicity\n","for idx in range(5000):#len(dataset['train'])):\n","\n","  # get text from this example\n","  txt = f\"QUESTION: {dataset['train'][idx]['question']} ANSWER: {dataset['train'][idx]['answer']}.\"\n","\n","  # tokenize and concatenate it\n","  trainTokens += tokenizer.encode(txt)\n","\n","# needs to be a torch tensor\n","trainTokens = torch.tensor(trainTokens)\n","trainTokens.shape"],"metadata":{"id":"vnulOWlTLQHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check a random batch\n","ix = torch.randint(len(trainTokens)-seq_len,size=(batch_size,))\n","X  = trainTokens[ix[:,None] + torch.arange(seq_len)]\n","print(X)"],"metadata":{"id":"CQp87qEqMrdN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for t in X[:5]:\n","  print(f'*** Example: \\n',textwrap.fill(tokenizer.decode(t),75),'\\n')"],"metadata":{"id":"5hcoxRdRMvTC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gs46PMeRLP-s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune the model"],"metadata":{"id":"wkXTHacS2HyC"}},{"cell_type":"code","source":["# move the model to the GPU\n","gpt2 = gpt2.to(device)\n","\n","# create the optimizer functions\n","optimizer = torch.optim.AdamW(gpt2.parameters(), lr=5e-5, weight_decay=.01)"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (training takes ~8 mins on A100)\n","num_samples = 1234\n","\n","# initialize losses\n","train_loss = np.zeros(num_samples)\n","\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  ix = torch.randint(len(trainTokens)-seq_len,size=(batch_size,))\n","  X  = trainTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # forward pass\n","  gpt2.zero_grad()\n","  outputs = gpt2(X,labels=X)\n","  loss = outputs.loss\n","\n","  # backprop\n","  loss.backward()\n","  optimizer.step()\n","\n","  # store the per-sample loss\n","  train_loss[sampli] = loss.item()\n","\n","  # update progress display\n","  if sampli%77==0:\n","    print(f'Sample {sampli:4}/{num_samples}, train loss: {train_loss[sampli]:.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oMKA4BKnky1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.figure(figsize=(8,3))\n","plt.plot(train_loss,'k')\n","\n","plt.gca().set(xlim=[0,num_samples+1],xlabel='Data sample',ylabel='Loss',title='Train loss')\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1wZg_eplZBDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Qualtative assessment\n","prompt = 'QUESTION: Would it be strange to have a pet rock and feed it styrofoam?'\n","# prompt = 'QUESTION: Where did the word \"butterfly\" come from?'\n","in2gpt = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n","\n","output = gpt2.generate(in2gpt,max_length=100,pad_token_id=50256,do_sample=True)\n","print(tokenizer.decode(in2gpt[0].cpu()),'\\n')\n","print(textwrap.fill(tokenizer.decode(output[0][len(in2gpt[0]):]),60))"],"metadata":{"id":"4ql-aaqpZBAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vkbGVHeVofFw"},"execution_count":null,"outputs":[]}]}